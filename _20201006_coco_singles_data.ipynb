{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "\n",
    "\n",
    "### Paths ###\n",
    "path         = Path('/home/rory/data/coco2017')\n",
    "train_json   = 'annotations/instances_train2017.json'\n",
    "valid_json   = 'annotations/instances_val2017.json'\n",
    "train_im_dir = 'train2017'\n",
    "valid_im_dir = 'val2017'\n",
    "\n",
    "\n",
    "### Get files and annos ###\n",
    "def get_annos(path, anno_file, im_folder):\n",
    "    xs, ys = get_annotations(path/anno_file)\n",
    "    return L(xs).map(lambda x: path/im_folder/x), ys\n",
    "train_files, train_annos = get_annos(path, train_json, train_im_dir)\n",
    "valid_files, valid_annos = get_annos(path, valid_json, valid_im_dir)\n",
    "files  = train_files + valid_files\n",
    "annos  = train_annos + valid_annos\n",
    "bboxes = [a[0] for a in annos]\n",
    "lbls   = [a[1] for a in annos]\n",
    "\n",
    "\n",
    "### Get largest anno ###\n",
    "def transpose(anno): return list(zip(*anno)) # tensor.t()\n",
    "def bbox_area(transposed_anno):\n",
    "    b = transposed_anno[0]\n",
    "    return((b[2]-b[0])*(b[3]-b[1])) # b-t * l-r\n",
    "def sort_annos(o): return sorted(transpose(o), key=bbox_area, reverse=True)\n",
    "sorted_annos = L(sort_annos(i) for i in annos)\n",
    "largest_anno = L(i[0] for i in sorted_annos)\n",
    "largest_bbox = L(i[0] for i in largest_anno)\n",
    "largest_lbl  = L(i[1] for i in largest_anno)\n",
    "# get_xyz helpers (used in following sections)\n",
    "files2lbl  = {f:l for f,l in zip(files,largest_lbl)}\n",
    "def get_lbl(f):  return files2lbl[f]\n",
    "files2bbox = {f:b for f,b in zip(files,largest_bbox)}\n",
    "def get_bbox(f): return files2bbox[f]\n",
    "\n",
    "\n",
    "### Get singles ###\n",
    "# identify singles\n",
    "lbls_per_im = L(len(l) for l in lbls)\n",
    "tuples = L(zip(files, largest_lbl, largest_bbox))\n",
    "singles = tuples[lbls_per_im.map(lambda n:n==1)]\n",
    "singles_tp = transpose(singles)\n",
    "# identify lbls with at least 500 singles\n",
    "lbl2paths = {l:[p for p in singles_tp[0] if get_lbl(p) == l] \n",
    "             for l in set(singles_tp[1])}\n",
    "lbl_subset=[]\n",
    "for lbl in lbl2paths:\n",
    "    l = len(lbl2paths[lbl])\n",
    "    if l > 500: lbl_subset += [lbl]\n",
    "# create subset of ims in lbl_subset\n",
    "subset = L(s for s in singles if s[1] in lbl_subset)\n",
    "files_subset = L(i[0] for i in subset)\n",
    "\n",
    "\n",
    "### Store outputs in a pickle ###\n",
    "df = pd.DataFrame({\n",
    "    \"im\":  files_subset,\n",
    "    \"lbl\": files_subset.map(get_lbl),\n",
    "    \"bbox\":files_subset.map(get_bbox),\n",
    "    })\n",
    "df.to_pickle(path/'singles.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

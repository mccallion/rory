{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Single-label-classifier\" data-toc-modified-id=\"Single-label-classifier-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Single label classifier</a></span><ul class=\"toc-item\"><li><span><a href=\"#Train-from-scratch\" data-toc-modified-id=\"Train-from-scratch-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Train from scratch</a></span></li><li><span><a href=\"#Pretrained-model\" data-toc-modified-id=\"Pretrained-model-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Pretrained model</a></span></li></ul></li><li><span><a href=\"#Single-label-classifier-using-singles-subset\" data-toc-modified-id=\"Single-label-classifier-using-singles-subset-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Single label classifier using singles subset</a></span><ul class=\"toc-item\"><li><span><a href=\"#Train-from-scratch\" data-toc-modified-id=\"Train-from-scratch-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Train from scratch</a></span></li><li><span><a href=\"#Pretrained-model\" data-toc-modified-id=\"Pretrained-model-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Pretrained model</a></span></li></ul></li><li><span><a href=\"#Multi-label-classifier\" data-toc-modified-id=\"Multi-label-classifier-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Multi label classifier</a></span><ul class=\"toc-item\"><li><span><a href=\"#debugging\" data-toc-modified-id=\"debugging-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>debugging</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single label classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "\n",
    "\n",
    "### Get files and annos ###\n",
    "path = Path('/home/rory/data/coco2017')\n",
    "def get_annos(path, anno_file, im_folder):\n",
    "    xs, ys = get_annotations(path/anno_file)\n",
    "    return L(xs).map(lambda x: path/im_folder/x), ys\n",
    "train_files, train_annos = get_annos(path,'annotations/instances_train2017.json', 'train2017')\n",
    "valid_files, valid_annos = get_annos(path,'annotations/instances_val2017.json',   'val2017')\n",
    "files  = train_files + valid_files\n",
    "annos  = train_annos + valid_annos\n",
    "bboxes = [a[0] for a in annos]\n",
    "lbls   = [a[1] for a in annos]\n",
    "\n",
    "\n",
    "### Process annos ###\n",
    "def transpose(anno): return list(zip(*anno))\n",
    "def bbox_area(transposed_anno):\n",
    "    b = transposed_anno[0]\n",
    "    return((b[2]-b[0])*(b[3]-b[1])) # b-t * l-r\n",
    "def sort_annos(o): return sorted(transpose(o), key=bbox_area, reverse=True)\n",
    "sorted_annos = L(sort_annos(i) for i in annos)\n",
    "largest_anno = L(i[0] for i in sorted_annos)\n",
    "largest_bbox = L(i[0] for i in largest_anno)\n",
    "largest_lbl  = L(i[1] for i in largest_anno)\n",
    "\n",
    "\n",
    "### Datasets ###\n",
    "files2lbls = {f:l for f,l in zip(files,largest_lbl)}\n",
    "def get_lbl(x): return files2lbls[x]\n",
    "splits = (L(range(len(train_files))),\n",
    "          L(range(len(valid_files))).map(lambda x: x + len(train_files)))\n",
    "dss_tfms = [[PILImage.create],[get_lbl, Categorize]]\n",
    "dss = Datasets(files, tfms=dss_tfms, splits=splits)\n",
    "\n",
    "\n",
    "### DataLoaders ###\n",
    "cpu_tfms = [Resize(224, method=ResizeMethod.Squish), ToTensor()]\n",
    "gpu_tfms = [IntToFloatTensor(), Normalize()]\n",
    "dls = dss.dataloaders(bs=64, after_item=cpu_tfms, after_batch=gpu_tfms)\n",
    "\n",
    "\n",
    "### Model ###\n",
    "def convlayr(i, o): return ConvLayer(i, o, stride=2)\n",
    "class ResBlock(Module):\n",
    "    def __init__(self, nf):\n",
    "        self.conv1 = ConvLayer(nf, nf, stride=1)\n",
    "        self.conv2 = ConvLayer(nf, nf, stride=1)\n",
    "    def forward(self, x): return x + self.conv2(self.conv1(x))\n",
    "def conv_res(i,o): return nn.Sequential(convlayr(i,o), ResBlock(o))\n",
    "p = 20\n",
    "mod = nn.Sequential(\n",
    "    conv_res(3  ,p*1),\n",
    "    conv_res(p*1,p*2),\n",
    "    conv_res(p*2,p*4),\n",
    "    conv_res(p*4,p*2),\n",
    "    convlayr(p*2,dls.c),\n",
    "    Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train ###\n",
    "learn = Learner(dls, mod, metrics=accuracy)\n",
    "lr_min, lr_steep = learn.lr_find()\n",
    "learn.fit_one_cycle(10, slice(lr_min/1000,lr_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model & Train ###\n",
    "learn = cnn_learner(dls, resnet34, metrics=accuracy)\n",
    "lr_min, lr_steep = learn.lr_find()\n",
    "learn.fit_one_cycle(2, lr=lr_min) # trained in 5min 43s; acc = .752"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single label classifier using singles subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "\n",
    "\n",
    "### Get files and annos ###\n",
    "path = Path('/home/rory/data/coco2017')\n",
    "def get_annos(path, anno_file, im_folder):\n",
    "    xs, ys = get_annotations(path/anno_file)\n",
    "    return L(xs).map(lambda x: path/im_folder/x), ys\n",
    "train_files, train_annos = get_annos(path,'annotations/instances_train2017.json', 'train2017')\n",
    "valid_files, valid_annos = get_annos(path,'annotations/instances_val2017.json',   'val2017')\n",
    "files  = train_files + valid_files\n",
    "annos  = train_annos + valid_annos\n",
    "bboxes = [a[0] for a in annos]\n",
    "lbls   = [a[1] for a in annos]\n",
    "# mappings\n",
    "files2lbls = {f:l for f,l in zip(files,largest_lbl)}\n",
    "def get_lbl(x): return files2lbls[x]\n",
    "\n",
    "\n",
    "### Process annos ###\n",
    "def transpose(anno): return list(zip(*anno))\n",
    "def bbox_area(transposed_anno):\n",
    "    b = transposed_anno[0]\n",
    "    return((b[2]-b[0])*(b[3]-b[1])) # b-t * l-r\n",
    "def sort_annos(o): return sorted(transpose(o), key=bbox_area, reverse=True)\n",
    "sorted_annos = L(sort_annos(i) for i in annos)\n",
    "largest_anno = L(i[0] for i in sorted_annos)\n",
    "largest_bbox = L(i[0] for i in largest_anno)\n",
    "largest_lbl  = L(i[1] for i in largest_anno)\n",
    "\n",
    "\n",
    "### Get singles ###\n",
    "# identify singles\n",
    "zipped = L(zip(files, largest_lbl))\n",
    "lbls_per_im = L(len(l) for l in lbls)\n",
    "singles = zipped[lbls_per_im.map(lambda n:n==1)]\n",
    "\n",
    "# identify lbls with at least 500 singles\n",
    "singles2lbl = {p:l for p,l in singles}\n",
    "transposed = list(zip(*singles))\n",
    "lbl2paths = {l:[p for p in transposed[0] if get_lbl(p) == l] \n",
    "             for l in set(transposed[1])}\n",
    "lbl_subset = []\n",
    "for lbl in lbl2paths:\n",
    "    l = len(lbl2paths[lbl])\n",
    "    if l > 500: lbl_subset += [lbl]\n",
    "        \n",
    "# create subset of ims in lbl_subset\n",
    "zipped_subset = L(s for s in singles if s[1] in lbl_subset)\n",
    "files_subset = L(i[0] for i in zipped_subset)\n",
    "\n",
    "\n",
    "### Datasets ###\n",
    "subset2lbl = {f:l for f,l in zipped_subset}\n",
    "def get_subset_lbl(x): return subset2lbl[x]\n",
    "splits = RandomSplitter(.1)(files_subset)\n",
    "dss_tfms = [[PILImage.create],[get_subset_lbl, Categorize]]\n",
    "dss = Datasets(files_subset, tfms=dss_tfms, splits=splits)\n",
    "\n",
    "\n",
    "### DataLoaders ###\n",
    "cpu_tfms = [Resize(224, method=ResizeMethod.Squish), ToTensor()]\n",
    "gpu_tfms = [IntToFloatTensor(), Normalize()]\n",
    "dls = dss.dataloaders(bs=64, after_item=cpu_tfms, after_batch=gpu_tfms)\n",
    "\n",
    "\n",
    "### Model ###\n",
    "def convlayr(i, o): return ConvLayer(i, o, stride=2)\n",
    "class ResBlock(Module):\n",
    "    def __init__(self, nf):\n",
    "        self.conv1 = ConvLayer(nf, nf, stride=1)\n",
    "        self.conv2 = ConvLayer(nf, nf, stride=1)\n",
    "    def forward(self, x): return x + self.conv2(self.conv1(x))\n",
    "def conv_res(i,o): return nn.Sequential(convlayr(i,o), ResBlock(o))\n",
    "p = 20\n",
    "mod = nn.Sequential(\n",
    "    conv_res(3  ,p*1),\n",
    "    conv_res(p*1,p*2),\n",
    "    conv_res(p*2,p*4),\n",
    "    conv_res(p*4,p*2),\n",
    "    convlayr(p*2,dls.c),\n",
    "    Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train ###\n",
    "learn = Learner(dls, mod, metrics=accuracy)\n",
    "lr_min, lr_steep = learn.lr_find()\n",
    "learn.fit_one_cycle(4, slice(lr_min/1000,lr_min)) # trained in 1min 10s; acc = .315"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model & Train ###\n",
    "learn = cnn_learner(dls, resnet34, metrics=accuracy)\n",
    "lr_min, lr_steep = learn.lr_find()\n",
    "learn.fit_one_cycle(2, lr=lr_min) # trained in 32s; acc = .966"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.show_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi label classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "### Imports & Training Params ###\n",
    "\n",
    "## Imports\n",
    "from fastai.vision.all import *\n",
    "\n",
    "## Hyperparams\n",
    "subset_pct = .3\n",
    "im_size    = 128\n",
    "batch_size = 64*2\n",
    "\n",
    "## Paths\n",
    "path            = Path('/home/rory/data/coco2017')\n",
    "train_anno_json = 'annotations/instances_train2017.json'\n",
    "valid_anno_json = 'annotations/instances_val2017.json'\n",
    "train_im_folder = 'train2017'\n",
    "valid_im_folder = 'val2017'\n",
    "\n",
    "\n",
    "########################\n",
    "### Helper functions ###\n",
    "\n",
    "## Toy data \n",
    "l1 = [1,2,3]\n",
    "l2 = ['a','b','c']\n",
    "d = {k:v for k,v in zip(l1,l2)}\n",
    "ones = [1,1,1]\n",
    "zeros = [0,0,0]\n",
    "\n",
    "## Transpose lists\n",
    "def transpose(l): return list(zip(*l))\n",
    "test_eq(\n",
    "    transpose([l1,l2]),\n",
    "    [(1,'a'),\n",
    "     (2,'b'),\n",
    "     (3,'c')] )\n",
    "\n",
    "## Create function from dict\n",
    "def dict_k2v(d): return lambda x: d[x]\n",
    "test_eq( dict_k2v(d)(1),  'a' )\n",
    "\n",
    "## Get subset of an L \n",
    "def subset(l, pct=.1):  \n",
    "    if isinstance(l, list): l = L(l)\n",
    "    if pct >= 1: pct = pct/len(l)\n",
    "    idxs = RandomSplitter(pct)(l)[1]\n",
    "    return l[idxs]\n",
    "test_eq( subset(ones,.66),  L(1)   )\n",
    "test_eq( subset(ones, 2) ,  L(1,1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "### ETL ###\n",
    "\n",
    "## Get files and annos\n",
    "def get_annos(path, anno_file, im_folder):\n",
    "    xs, ys = get_annotations(path/anno_file)\n",
    "    return L(xs).map(lambda x: path/im_folder/x), ys\n",
    "train = get_annos(path, train_anno_json, train_im_folder)\n",
    "valid = get_annos(path, valid_anno_json, valid_im_folder)\n",
    "train_paths, train_annos = transpose(subset(transpose(train), subset_pct))\n",
    "valid_paths, valid_annos = transpose(subset(transpose(valid), subset_pct))\n",
    "paths  = train_paths + valid_paths\n",
    "annos  = train_annos + valid_annos\n",
    "bboxes = [a[0] for a in annos]\n",
    "lbls   = [a[1] for a in annos]\n",
    "\n",
    "\n",
    "## Process annos\n",
    "def bbox_area(transposed_anno):\n",
    "    b = transposed_anno[0]\n",
    "    return((b[2]-b[0])*(b[3]-b[1])) # b-t * l-r\n",
    "def sort_annos(o): return sorted(transpose(o), key=bbox_area, reverse=True)\n",
    "sorted_annos = L(sort_annos(i) for i in annos)\n",
    "largest_anno = L(i[0] for i in sorted_annos)\n",
    "largest_bbox = L(i[0] for i in largest_anno)\n",
    "largest_lbl  = L(i[1] for i in largest_anno)\n",
    "\n",
    "\n",
    "## Datasets\n",
    "splits = (L(range(len(train_paths))),\n",
    "          L(range(len(valid_paths))).map(lambda x: x + len(train_paths)))\n",
    "get_lbl = dict_k2v({p:l for p,l in zip(paths,lbls)})\n",
    "dss_tfms = [[PILImage.create], [get_lbl, MultiCategorize, OneHotEncode]]\n",
    "dss = Datasets(paths, tfms=dss_tfms, splits=splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DataLoaders\n",
    "cpu_tfms = [Resize(im_size, method=ResizeMethod.Squish), ToTensor()]\n",
    "gpu_tfms = [IntToFloatTensor()]\n",
    "dls = dss.dataloaders(bs=batch_size, after_item=cpu_tfms, after_batch=gpu_tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "### Model & Train ###\n",
    "\n",
    "## Model\n",
    "learn = cnn_learner(dls, resnet34,\n",
    "                    metrics=partial(accuracy_multi, thresh=0.5),\n",
    "                    loss_func=BCEWithLogitsLossFlat(),\n",
    "                    normalize=True)\n",
    "lr_min, lr_steep = learn.lr_find()\n",
    "\n",
    "## Train\n",
    "learn.fine_tune(3, base_lr=3e-3, freeze_epochs=4)\n",
    "learn.show_results(max_n=4, figsize=(12,12))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

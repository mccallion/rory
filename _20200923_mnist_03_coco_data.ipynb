{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Editing-MNIST-example-to-use-COCO-data\" data-toc-modified-id=\"Editing-MNIST-example-to-use-COCO-data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Editing MNIST example to use COCO data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data\" data-toc-modified-id=\"Data-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Original-COCO-ETL\" data-toc-modified-id=\"Original-COCO-ETL-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Original COCO ETL</a></span></li><li><span><a href=\"#Additions\" data-toc-modified-id=\"Additions-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>Additions</a></span></li></ul></li><li><span><a href=\"#Model-&amp;-Training\" data-toc-modified-id=\"Model-&amp;-Training-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Model &amp; Training</a></span></li><li><span><a href=\"#Train-model-once-w/-train_n_epochs\" data-toc-modified-id=\"Train-model-once-w/-train_n_epochs-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Train model once w/ <code>train_n_epochs</code></a></span></li><li><span><a href=\"#Train-model-many-times-w/-train_n_sessions\" data-toc-modified-id=\"Train-model-many-times-w/-train_n_sessions-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Train model many times w/ <code>train_n_sessions</code></a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Editing MNIST example to use COCO data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original COCO ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "from fastcore.test import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import PIL\n",
    "\n",
    "\n",
    "### Unzip data ###\n",
    "path = Path('/home/rory/data/coco2017')\n",
    "path_items = path.ls()\n",
    "from zipfile import ZipFile\n",
    "def unzip(*paths,dest):\n",
    "    \"\"\"Unzips files located at posixpaths to dest. Deps: zipfile, pathlib.\"\"\"\n",
    "    for path in paths:\n",
    "        if path.suffix == '.zip':\n",
    "            print(f\"Unzipping: {path} ---> {dest}.\")\n",
    "            with ZipFile(path, 'r') as archive:\n",
    "                archive.extractall(dest)\n",
    "    new_items = set(dest.ls()) - set(paths)\n",
    "    print(\"Finished. New items in dest:\",*new_items,sep='\\n')\n",
    "# unzip(*path_items, dest=path)\n",
    "\n",
    "\n",
    "### Get ims and annos ###\n",
    "ims_train, annos_train = get_annotations(path/'annotations/instances_train2017.json')\n",
    "ims_valid, annos_valid = get_annotations(path/'annotations/instances_val2017.json')\n",
    "ims_train = list(map(lambda x: Path('train2017/' + x), ims_train))\n",
    "ims_valid = list(map(lambda x: Path('val2017/' + x), ims_valid))\n",
    "ims    = ims_train + ims_valid\n",
    "paths  = list(map(lambda im: Path(path/im), ims))\n",
    "annos  = annos_train + annos_valid\n",
    "bboxes = [i[0] for i in annos]\n",
    "lbls   = [i[1] for i in annos]\n",
    "\n",
    "\n",
    "### Process annos ###\n",
    "def transpose(l): return list(zip(*l))\n",
    "def bbox_area(anno_t): # takes a transposed anno\n",
    "    b = anno_t[0]\n",
    "    return((b[2]-b[0])*(b[3]-b[1])) # b-t * l-r\n",
    "def sort_annos(o): return sorted(transpose(o), key=bbox_area, reverse=True)\n",
    "sorted_annos = [sort_annos(a) for a in annos]\n",
    "largest_anno = [i[0] for i in sorted_annos]\n",
    "largest_bbox = [i[0] for i in largest_anno]\n",
    "largest_lbl  = [i[1] for i in largest_anno]\n",
    "is_valid     = [False]*len(ims_train) + [True]*len(ims_valid)\n",
    "\n",
    "\n",
    "### Store outputs in dfd, df ###\n",
    "dfd_all = {\n",
    "    \"im\":ims,\n",
    "    \"path\":paths,\n",
    "    \"annos\":annos,\n",
    "    \"bboxes\":bboxes,\n",
    "    \"lbls\":lbls,\n",
    "    \"sorted_annos\":sorted_annos,\n",
    "    \"anno\":largest_anno,\n",
    "    \"bbox\":largest_bbox,\n",
    "    \"lbl\":largest_lbl,\n",
    "    \"is_valid\":is_valid}\n",
    "df_all = pd.DataFrame(dfd_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create subset and throw it in a dl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#6856) [(Path('/home/rory/data/coco2017/train2017/000000060760.jpg'), 'toilet'),(Path('/home/rory/data/coco2017/train2017/000000360772.jpg'), 'toilet'),(Path('/home/rory/data/coco2017/train2017/000000191381.jpg'), 'toilet'),(Path('/home/rory/data/coco2017/train2017/000000256668.jpg'), 'toilet'),(Path('/home/rory/data/coco2017/train2017/000000159537.jpg'), 'clock'),(Path('/home/rory/data/coco2017/train2017/000000075051.jpg'), 'toilet'),(Path('/home/rory/data/coco2017/train2017/000000368978.jpg'), 'airplane'),(Path('/home/rory/data/coco2017/train2017/000000078371.jpg'), 'airplane'),(Path('/home/rory/data/coco2017/train2017/000000561100.jpg'), 'airplane'),(Path('/home/rory/data/coco2017/train2017/000000009426.jpg'), 'airplane')...]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Grab data & grab singletons ###\n",
    "data = L(zip(paths, largest_lbl))\n",
    "num_labels = L(len(l) for l in lbls)\n",
    "singletons = data[num_labels.map(lambda n:n==1)]\n",
    "\n",
    "### Given a lbl, grab all paths with that label ###\n",
    "path2lbl = {p:l for p,l in singletons}\n",
    "def get_lbl(path): return path2lbl[path]\n",
    "transposed = list(zip(*singletons))\n",
    "lbl2paths = {l:[p for p in transposed[0] if get_lbl(p) == l]\n",
    "             for l in set(transposed[1])}\n",
    "\n",
    "### Grab labels with more than 500 singleton ims ###\n",
    "lbl_subset = []\n",
    "for lbl in lbl2paths:\n",
    "    l = len(lbl2paths[lbl])\n",
    "    if l > 500: lbl_subset += [lbl]\n",
    "\n",
    "### Grab ims in label_subset ###\n",
    "data_subset = L(s for s in singletons if s[1] in lbl_subset)\n",
    "\n",
    "### Open images as tensors ###\n",
    "def open_im(imp, size=128):\n",
    "    im = PIL.Image.open(imp).convert('RGB')\n",
    "    im = im.resize((size, size))\n",
    "    t = torch.Tensor(np.array(im))\n",
    "    t = t.permute(2,0,1).float()/255.0\n",
    "    return t.reshape([3*size*size])\n",
    "\n",
    "### Create 1HE encodings ###\n",
    "n_cls = len(lbl_subset)\n",
    "encs = []\n",
    "for i in range(n_cls):\n",
    "    l = L([0]*n_cls)\n",
    "    l[i] = 1\n",
    "    encs += [l]\n",
    "lbl2enc = {l:tensor(e) for l,e in zip(lbl_subset,encs)}\n",
    "\n",
    "### Create dataloader ###\n",
    "ds = [(open_im(path), lbl2enc[lbl]) for path, lbl in data_subset]\n",
    "dl = DataLoader(ds, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model ###\n",
    "def init_mod(im_size, n_cls, hidden_params):\n",
    "    mod = nn.Sequential(\n",
    "        nn.Linear(im_size,hidden_params),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_params,n_cls)\n",
    "    )\n",
    "    return mod\n",
    "\n",
    "### Loss ###\n",
    "def softmax(t):\n",
    "    if len(t.shape) == 1: return torch.exp(t) / torch.exp(t).sum()\n",
    "    else:                 return torch.exp(t) / torch.exp(t).sum(dim=1, keepdim=True)\n",
    "def loss(yp, y):\n",
    "    return (1 - (y*softmax(yp)).sum(dim=1, keepdim=True)).mean()  # softmax cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate gradients for use in train_once ###\n",
    "def calc_grad(x,y,mod):\n",
    "    yp = mod(x)     # get predictions\n",
    "    ls = loss(yp,y) # calculate loss\n",
    "    ls.backward()   # take gradient w.r.t. loss\n",
    "\n",
    "### Create SGD Stepper; args = (mod.parameters(), lr) ###\n",
    "class ParamStepper:\n",
    "    def __init__(self, p, lr): self.p,self.lr = list(p),lr # initialize w/ mod.params & lr\n",
    "        \n",
    "    def step(self, *args, **kwargs):                       # take step\n",
    "        for o in self.p: o.data -= o.grad.data * self.lr\n",
    "            \n",
    "    def zero_grad(self, *args, **kwargs):                  # reset grad\n",
    "        for o in self.p: o.grad = None\n",
    "\n",
    "### Train parameters by performing SGD on each mini-batch in the dl ###\n",
    "def train_one_epoch(dl, mod, stepper):\n",
    "    for xb,yb in dl:           # for every minibatch (xb,yb) in the dataloader:\n",
    "        calc_grad(xb, yb, mod) # calc grad(loss(mod(xb),yb))\n",
    "        stepper.step()         # take step\n",
    "        stepper.zero_grad()    # reset grad\n",
    "\n",
    "### Get accuracy of mod on a mini-batch ###\n",
    "def mb_acc(yp,y):\n",
    "    yp_max,yp_i = torch.max(yp, dim=1, keepdim=True)\n",
    "    y_max, y_i  = torch.max(y,  dim=1, keepdim=True)\n",
    "    return (yp_i==y_i).float().mean()\n",
    "        \n",
    "### Get accuracy of mod on a dataloader (takes avg of all mbs in dl) ###\n",
    "def epoch_acc(dl, mod):\n",
    "    a = [mb_acc(mod(xb), yb) for xb,yb in dl]\n",
    "    return round(torch.stack(a).mean().item(), 5)          # avg acc over all mini-batches\n",
    "\n",
    "### Run `train_once` `epochs` times given data `dl`, model `mod`, and stepper `stepper`\n",
    "def train_n_epochs(dl, mod, stepper, nepochs):\n",
    "    accs = L()\n",
    "    for i in range(nepochs):\n",
    "        print('.',end='')\n",
    "        train_one_epoch(dl, mod, stepper)\n",
    "        accs += epoch_acc(dl, mod)\n",
    "    print('',end='\\t')\n",
    "    return accs\n",
    "\n",
    "### Perform n training sessions ###\n",
    "def train_n_sessions(dl, im_size, n_cls, hidden_params, nepochs, lr, nsessions):\n",
    "    # accuracies and trained models are returned by train_n_sessions for access after training.\n",
    "    accs = L()\n",
    "    mods = L()\n",
    "    # train model and record accs, mods\n",
    "    print('Progress:',end='\\n')\n",
    "    for i in range(nsessions):\n",
    "        print(i,end='')\n",
    "        mod = init_mod(im_size, n_cls, hidden_params)\n",
    "        stepper = ParamStepper(mod.parameters(), lr)\n",
    "        accs += train_n_epochs(dl, mod, stepper, nepochs)\n",
    "        mods += mod\n",
    "    # restructure mods into nn.Sequential form for end-user convenience\n",
    "    nmods = len(mods)\n",
    "    trained_models = L()\n",
    "    for n in range(nsessions):\n",
    "        layers = L()\n",
    "        for j in range(nmods//nsessions): layers.append(mods.pop())\n",
    "        layers.reverse()\n",
    "        seqmod = nn.Sequential(*layers.items)\n",
    "        trained_models += [seqmod]\n",
    "    print('Done')\n",
    "    return accs, trained_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model once w/ `train_n_epochs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................\t(#20) [0.28979,0.31076,0.32364,0.33116,0.32827,0.33261,0.33898,0.32812,0.33955,0.33507...]\n",
      "0.37616\n"
     ]
    }
   ],
   "source": [
    "# params\n",
    "im_size       = 128*128*3\n",
    "batch_size    = 64\n",
    "hidden_params = 30\n",
    "lr            = .01\n",
    "nepochs       = 20\n",
    "n_cls         = len(lbl_subset)\n",
    "\n",
    "# inits\n",
    "mod           = init_mod(im_size, n_cls, hidden_params)\n",
    "stepper       = ParamStepper(mod.parameters(), lr)\n",
    "\n",
    "# train\n",
    "accs = train_n_epochs(dl, mod, stepper, nepochs)\n",
    "print(accs)\n",
    "print(max(accs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model many times w/ `train_n_sessions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:\n",
      "0..............................\t1..............................\t2..............................\t3..............................\tDone\n",
      "Max accuracy: 0.918\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.20321</td>\n",
       "      <td>0.36224</td>\n",
       "      <td>0.64941</td>\n",
       "      <td>0.72600</td>\n",
       "      <td>0.78859</td>\n",
       "      <td>0.79937</td>\n",
       "      <td>0.80784</td>\n",
       "      <td>0.87557</td>\n",
       "      <td>0.88299</td>\n",
       "      <td>0.88825</td>\n",
       "      <td>...</td>\n",
       "      <td>0.91078</td>\n",
       "      <td>0.91226</td>\n",
       "      <td>0.91305</td>\n",
       "      <td>0.91460</td>\n",
       "      <td>0.91491</td>\n",
       "      <td>0.91579</td>\n",
       "      <td>0.91647</td>\n",
       "      <td>0.91694</td>\n",
       "      <td>0.91773</td>\n",
       "      <td>0.91803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.28229</td>\n",
       "      <td>0.41783</td>\n",
       "      <td>0.55677</td>\n",
       "      <td>0.56697</td>\n",
       "      <td>0.72080</td>\n",
       "      <td>0.73564</td>\n",
       "      <td>0.74246</td>\n",
       "      <td>0.74530</td>\n",
       "      <td>0.74886</td>\n",
       "      <td>0.75083</td>\n",
       "      <td>...</td>\n",
       "      <td>0.76214</td>\n",
       "      <td>0.76183</td>\n",
       "      <td>0.76275</td>\n",
       "      <td>0.76358</td>\n",
       "      <td>0.76409</td>\n",
       "      <td>0.76411</td>\n",
       "      <td>0.76448</td>\n",
       "      <td>0.76473</td>\n",
       "      <td>0.76548</td>\n",
       "      <td>0.76562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.29635</td>\n",
       "      <td>0.52273</td>\n",
       "      <td>0.59737</td>\n",
       "      <td>0.77249</td>\n",
       "      <td>0.80325</td>\n",
       "      <td>0.81428</td>\n",
       "      <td>0.82119</td>\n",
       "      <td>0.82421</td>\n",
       "      <td>0.82798</td>\n",
       "      <td>0.82920</td>\n",
       "      <td>...</td>\n",
       "      <td>0.84383</td>\n",
       "      <td>0.84389</td>\n",
       "      <td>0.84510</td>\n",
       "      <td>0.84530</td>\n",
       "      <td>0.84617</td>\n",
       "      <td>0.84669</td>\n",
       "      <td>0.84775</td>\n",
       "      <td>0.84849</td>\n",
       "      <td>0.84820</td>\n",
       "      <td>0.87624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.39058</td>\n",
       "      <td>0.42651</td>\n",
       "      <td>0.64370</td>\n",
       "      <td>0.65673</td>\n",
       "      <td>0.66317</td>\n",
       "      <td>0.66713</td>\n",
       "      <td>0.73243</td>\n",
       "      <td>0.74230</td>\n",
       "      <td>0.74811</td>\n",
       "      <td>0.75052</td>\n",
       "      <td>...</td>\n",
       "      <td>0.76288</td>\n",
       "      <td>0.76335</td>\n",
       "      <td>0.76415</td>\n",
       "      <td>0.76439</td>\n",
       "      <td>0.76494</td>\n",
       "      <td>0.76504</td>\n",
       "      <td>0.76519</td>\n",
       "      <td>0.76547</td>\n",
       "      <td>0.76642</td>\n",
       "      <td>0.77723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0        1        2        3        4        5        6        7   \\\n",
       "0  0.20321  0.36224  0.64941  0.72600  0.78859  0.79937  0.80784  0.87557   \n",
       "1  0.28229  0.41783  0.55677  0.56697  0.72080  0.73564  0.74246  0.74530   \n",
       "2  0.29635  0.52273  0.59737  0.77249  0.80325  0.81428  0.82119  0.82421   \n",
       "3  0.39058  0.42651  0.64370  0.65673  0.66317  0.66713  0.73243  0.74230   \n",
       "\n",
       "        8        9   ...       20       21       22       23       24  \\\n",
       "0  0.88299  0.88825  ...  0.91078  0.91226  0.91305  0.91460  0.91491   \n",
       "1  0.74886  0.75083  ...  0.76214  0.76183  0.76275  0.76358  0.76409   \n",
       "2  0.82798  0.82920  ...  0.84383  0.84389  0.84510  0.84530  0.84617   \n",
       "3  0.74811  0.75052  ...  0.76288  0.76335  0.76415  0.76439  0.76494   \n",
       "\n",
       "        25       26       27       28       29  \n",
       "0  0.91579  0.91647  0.91694  0.91773  0.91803  \n",
       "1  0.76411  0.76448  0.76473  0.76548  0.76562  \n",
       "2  0.84669  0.84775  0.84849  0.84820  0.87624  \n",
       "3  0.76504  0.76519  0.76547  0.76642  0.77723  \n",
       "\n",
       "[4 rows x 30 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# params\n",
    "path          = untar_data(URLs.MNIST)\n",
    "n_cls         = 10\n",
    "im_size       = 28*28\n",
    "batch_size    = 64*2*2*2\n",
    "hidden_params = 30\n",
    "nepochs       = 30\n",
    "lr            = .1\n",
    "nsessions     = 4\n",
    "\n",
    "# inits\n",
    "# dl = init_data(path, im_size, n_cls, batch_size)\n",
    "\n",
    "# train\n",
    "accs,mods = train_n_sessions(dl, im_size, n_cls, hidden_params, nepochs, lr, nsessions)\n",
    "accs_t = tensor(accs).reshape(nsessions,nepochs)\n",
    "\n",
    "# print max acc\n",
    "print(\"Max accuracy:\",round(accs_t.max().item(),4))\n",
    "\n",
    "# print training sessions\n",
    "pd.DataFrame(accs_t.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

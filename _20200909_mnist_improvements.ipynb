{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Improvements-pursued-in-this-notebook\" data-toc-modified-id=\"Improvements-pursued-in-this-notebook-0.1\"><span class=\"toc-item-num\">0.1&nbsp;&nbsp;</span>Improvements pursued in this notebook</a></span></li></ul></li><li><span><a href=\"#Improvement-#1:-multi-label-classification\" data-toc-modified-id=\"Improvement-#1:-multi-label-classification-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Improvement #1: multi-label classification</a></span><ul class=\"toc-item\"><li><span><a href=\"#Performance-Tweaks\" data-toc-modified-id=\"Performance-Tweaks-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Performance Tweaks</a></span><ul class=\"toc-item\"><li><span><a href=\"#Leaky-ReLU\" data-toc-modified-id=\"Leaky-ReLU-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Leaky ReLU</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvements pursued in this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Change from binary classifier to multi category classifier:\n",
    "    - add ims 0-9\n",
    "    - add change loss fxn to cross entropy loss w/ softmax\n",
    "    - change shape of final activation from 1 to 10\n",
    "    - change label to 1HE\n",
    "2. Add RGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Super-short version with all of the helpers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvement #1: multi-label classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "\n",
    "### Data ###\n",
    "def init_data(path, im_size, n_cls, batch_size):\n",
    "    ## Train\n",
    "    # ims\n",
    "    for i in range(n_cls):\n",
    "        new_ims = torch.stack(\n",
    "            [tensor(Image.open(fn)) for fn in (path/'training'/f'{i}').ls()]\n",
    "        ).float()/255\n",
    "        if i == 0: ims = new_ims\n",
    "        else: ims = torch.cat([ims,new_ims])\n",
    "    train_ims = ims.view(-1,im_size)\n",
    "    # lbls\n",
    "    train_lbls = []\n",
    "    for i in range(n_cls):\n",
    "        l = L([0]*n_cls)\n",
    "        l[i] = 1\n",
    "        train_lbls += [l] * len((path/'training'/f'{i}').ls())    \n",
    "    train_lbls = tensor(train_lbls)\n",
    "    ## Valid\n",
    "    # ims\n",
    "    for i in range(n_cls):\n",
    "        new_ims = torch.stack(\n",
    "            [tensor(Image.open(fn)) for fn in (path/'testing'/f'{i}').ls()]\n",
    "        ).float()/255\n",
    "        if i == 0: ims = new_ims\n",
    "        else: ims = torch.cat([ims,new_ims])\n",
    "    valid_ims = ims.view(-1,im_size)\n",
    "    # lbls\n",
    "    valid_lbls = []\n",
    "    for i in range(n_cls):\n",
    "        l = L([0]*n_cls)\n",
    "        l[i] = 1\n",
    "        valid_lbls += [l] * len((path/'testing'/f'{i}').ls())    \n",
    "    valid_lbls = tensor(valid_lbls)\n",
    "    ## DataLoaders\n",
    "    train_ds = L(zip(train_ims, train_lbls))\n",
    "    valid_ds = L(zip(valid_ims, valid_lbls))\n",
    "    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    valid_dl = DataLoader(valid_ds, batch_size=batch_size, shuffle=True)\n",
    "    return train_dl\n",
    "\n",
    "### Model ###\n",
    "def init_mod(im_size, n_cls, hidden_params):\n",
    "    mod = nn.Sequential(\n",
    "        nn.Linear(im_size,hidden_params),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_params,n_cls)\n",
    "    )\n",
    "    return mod\n",
    "\n",
    "### Create SGD Stepper ###\n",
    "class ParamStepper:\n",
    "    def __init__(self, p, lr): self.p,self.lr = list(p),lr # remembers your params & lr\n",
    "        \n",
    "    def step(self, *args, **kwargs):                       # take one step in the optimal direction\n",
    "        for o in self.p: o.data -= o.grad.data * self.lr\n",
    "            \n",
    "    def zero_grad(self, *args, **kwargs):                  # zeros out gradients\n",
    "        for o in self.p: o.grad = None\n",
    "\n",
    "### Calculate accuracy over entire dl given dl,mod ###\n",
    "def validate_epoch(dl, mod):\n",
    "    a = [acc(mod(xb), yb) for xb,yb in dl]         # Gradients calculated & stored at mod(xb) call\n",
    "    return round(torch.stack(a).mean().item(), 5)  # Avg over all mini-batches, return scalar (not tensor)\n",
    "\n",
    "### Adjust parameters w/ stepper for each mini-batch in a dl\n",
    "def train_once(dl, mod, stepper):\n",
    "    for xb,yb in dl:\n",
    "        calc_grad(xb, yb, mod)\n",
    "        stepper.step()\n",
    "        stepper.zero_grad()\n",
    "\n",
    "### Calculate gradients for use in train_once ###\n",
    "def calc_grad(x,y,mod):\n",
    "    yp = mod(x)\n",
    "    ls = loss(yp,y)\n",
    "    ls.backward()\n",
    "\n",
    "### Run `train_once` `epochs` times given data `dl`, model `mod`, and stepper `stepper`\n",
    "def train_model(dl, mod, stepper, epochs):\n",
    "    l = L()\n",
    "    for i in range(epochs):\n",
    "        train_once(dl, mod, stepper)\n",
    "        # print(validate_epoch(dl, mod), end='\\t')\n",
    "        l += validate_epoch(dl, mod)\n",
    "    return l\n",
    "\n",
    "### Perform n training sessions ###\n",
    "def train_model_n_times(dl, im_size, n_cls, hidden_params, epochs, lr, n):\n",
    "    o = L()\n",
    "    print('Current Session: ')\n",
    "    for i in range(n):\n",
    "        mod = init_mod(im_size, n_cls, hidden_params)\n",
    "        stepper = ParamStepper(mod.parameters(), lr)\n",
    "        o += train_model(dl, mod, stepper, epochs)\n",
    "        print(i,end=', ')\n",
    "    return tensor(o).reshape(n,epochs)\n",
    "    \n",
    "### Loss & Accuracy ###\n",
    "def softmax(t):\n",
    "    if len(t.shape) == 1: return torch.exp(t) / torch.exp(t).sum()\n",
    "    else:                 return torch.exp(t) / torch.exp(t).sum(dim=1, keepdim=True)\n",
    "def loss(yp, y):\n",
    "    yps = softmax(yp)\n",
    "    return (1 - (y * yps).sum(dim=1, keepdim=True)).mean()\n",
    "def acc(yp,y):\n",
    "    yp_max,yp_i = torch.max(yp, dim=1, keepdim=True)\n",
    "    y_max, y_i  = torch.max(y,  dim=1, keepdim=True)\n",
    "    return (yp_i==y_i).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Init ###\n",
    "path          = untar_data(URLs.MNIST)\n",
    "n_cls         = 10\n",
    "im_size       = 28*28\n",
    "batch_size    = 64*2*2*2\n",
    "dl            = init_data(path, im_size, n_cls, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t1\t2\t3\t"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.2079, 0.3295, 0.5562, 0.7139, 0.7352, 0.7908, 0.8014, 0.8095, 0.8134,\n",
       "         0.8182, 0.8212, 0.8233, 0.8253, 0.8264, 0.8285, 0.8293, 0.8308, 0.8316,\n",
       "         0.8323, 0.8334, 0.8346, 0.8347, 0.8357, 0.8374, 0.8379, 0.8379, 0.8386,\n",
       "         0.8389, 0.8394, 0.8399, 0.8402, 0.8411, 0.8410, 0.8416, 0.8425, 0.8427,\n",
       "         0.8428, 0.8435, 0.8443, 0.8445],\n",
       "        [0.2649, 0.4934, 0.6481, 0.6592, 0.6651, 0.6688, 0.6711, 0.7343, 0.7453,\n",
       "         0.7495, 0.7529, 0.7544, 0.7566, 0.7572, 0.7580, 0.7595, 0.7601, 0.7609,\n",
       "         0.7614, 0.7628, 0.8073, 0.8162, 0.8200, 0.8234, 0.8252, 0.8268, 0.8278,\n",
       "         0.8291, 0.8300, 0.8319, 0.8324, 0.8332, 0.8334, 0.8345, 0.8355, 0.8360,\n",
       "         0.8368, 0.8373, 0.8385, 0.8381],\n",
       "        [0.2214, 0.4365, 0.6957, 0.7266, 0.7362, 0.8078, 0.8185, 0.8230, 0.8260,\n",
       "         0.8286, 0.8319, 0.8331, 0.8344, 0.8361, 0.8374, 0.8381, 0.8400, 0.8400,\n",
       "         0.8419, 0.8426, 0.8431, 0.8435, 0.8446, 0.8450, 0.8463, 0.8468, 0.8470,\n",
       "         0.8470, 0.8480, 0.8476, 0.8489, 0.8488, 0.8495, 0.8493, 0.8506, 0.8504,\n",
       "         0.8508, 0.8516, 0.8520, 0.8522],\n",
       "        [0.2587, 0.4658, 0.5786, 0.6218, 0.6576, 0.6655, 0.7190, 0.7313, 0.7365,\n",
       "         0.7387, 0.7419, 0.7446, 0.7465, 0.7474, 0.7480, 0.7497, 0.7499, 0.7512,\n",
       "         0.7517, 0.7522, 0.7529, 0.7537, 0.7540, 0.7550, 0.7560, 0.7563, 0.7563,\n",
       "         0.7565, 0.7571, 0.7568, 0.7576, 0.7583, 0.7582, 0.7586, 0.7585, 0.7592,\n",
       "         0.7594, 0.7600, 0.7604, 0.7609]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training ###\n",
    "test1 = train_model_n_times(dl, im_size, n_cls, hidden_params=30, epochs=40, lr=.1, n=4)\n",
    "test1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Tweaks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leaky ReLU"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Improvements-pursued-in-this-notebook\" data-toc-modified-id=\"Improvements-pursued-in-this-notebook-0.1\"><span class=\"toc-item-num\">0.1&nbsp;&nbsp;</span>Improvements pursued in this notebook</a></span></li></ul></li><li><span><a href=\"#First-round-of-improvements:-multi-label-classifier-&amp;-sessions\" data-toc-modified-id=\"First-round-of-improvements:-multi-label-classifier-&amp;-sessions-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>First round of improvements: multi-label classifier &amp; sessions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Tools-for-data,-model,-and-training\" data-toc-modified-id=\"Tools-for-data,-model,-and-training-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Tools for data, model, and training</a></span><ul class=\"toc-item\"><li><span><a href=\"#Train-model-once-(to-test-if-it's-working)\" data-toc-modified-id=\"Train-model-once-(to-test-if-it's-working)-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Train model once (to test if it's working)</a></span></li><li><span><a href=\"#Train-model-many-times-(to-understand-its-performance)\" data-toc-modified-id=\"Train-model-many-times-(to-understand-its-performance)-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>Train model many times (to understand its performance)</a></span></li><li><span><a href=\"#Diving-into-results-of-a-single-session\" data-toc-modified-id=\"Diving-into-results-of-a-single-session-1.1.3\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>Diving into results of a single session</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvements pursued in this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Change from binary classifier to multi category classifier:\n",
    "    - add ims 0-9\n",
    "    - add change loss fxn to cross entropy loss w/ softmax\n",
    "    - change shape of final activation from 1 to 10\n",
    "    - change label to 1HE\n",
    "2. Add RGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Super-short version with all of the helpers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First round of improvements: multi-label classifier & sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools for data, model, and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "\n",
    "### Data ###\n",
    "def init_data(path, im_size, n_cls, batch_size):\n",
    "    ## Train\n",
    "    # ims\n",
    "    for i in range(n_cls):\n",
    "        new_ims = torch.stack(\n",
    "            [tensor(Image.open(fn)) for fn in (path/'training'/f'{i}').ls()]\n",
    "        ).float()/255\n",
    "        if i == 0: ims = new_ims\n",
    "        else: ims = torch.cat([ims,new_ims])\n",
    "    train_ims = ims.view(-1,im_size)\n",
    "    # lbls\n",
    "    train_lbls = []\n",
    "    for i in range(n_cls):\n",
    "        l = L([0]*n_cls)\n",
    "        l[i] = 1\n",
    "        train_lbls += [l] * len((path/'training'/f'{i}').ls())    \n",
    "    train_lbls = tensor(train_lbls)\n",
    "    ## Valid\n",
    "    # ims\n",
    "    for i in range(n_cls):\n",
    "        new_ims = torch.stack(\n",
    "            [tensor(Image.open(fn)) for fn in (path/'testing'/f'{i}').ls()]\n",
    "        ).float()/255\n",
    "        if i == 0: ims = new_ims\n",
    "        else: ims = torch.cat([ims,new_ims])\n",
    "    valid_ims = ims.view(-1,im_size)\n",
    "    # lbls\n",
    "    valid_lbls = []\n",
    "    for i in range(n_cls):\n",
    "        l = L([0]*n_cls)\n",
    "        l[i] = 1\n",
    "        valid_lbls += [l] * len((path/'testing'/f'{i}').ls())    \n",
    "    valid_lbls = tensor(valid_lbls)\n",
    "    ## DataLoaders\n",
    "    train_ds = L(zip(train_ims, train_lbls))\n",
    "    valid_ds = L(zip(valid_ims, valid_lbls))\n",
    "    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    valid_dl = DataLoader(valid_ds, batch_size=batch_size, shuffle=True)\n",
    "    return train_dl\n",
    "\n",
    "### Model ###\n",
    "def init_mod(im_size, n_cls, hidden_params):\n",
    "    mod = nn.Sequential(\n",
    "        nn.Linear(im_size,hidden_params),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_params,n_cls)\n",
    "    )\n",
    "    return mod\n",
    "\n",
    "### Loss ###\n",
    "def softmax(t):\n",
    "    if len(t.shape) == 1: return torch.exp(t) / torch.exp(t).sum()\n",
    "    else:                 return torch.exp(t) / torch.exp(t).sum(dim=1, keepdim=True)\n",
    "def loss(yp, y):\n",
    "    return (1 - (y*softmax(yp)).sum(dim=1, keepdim=True)).mean()\n",
    "\n",
    "### Calculate gradients for use in train_once ###\n",
    "def calc_grad(x,y,mod):\n",
    "    yp = mod(x)     # get predictions\n",
    "    ls = loss(yp,y) # calculate loss\n",
    "    ls.backward()   # take gradient w.r.t. loss\n",
    "\n",
    "### Create SGD Stepper; args = (mod.parameters(), lr) ###\n",
    "class ParamStepper:\n",
    "    def __init__(self, p, lr): self.p,self.lr = list(p),lr # initialize w/ mod.params & lr\n",
    "        \n",
    "    def step(self, *args, **kwargs):                       # take step\n",
    "        for o in self.p: o.data -= o.grad.data * self.lr\n",
    "            \n",
    "    def zero_grad(self, *args, **kwargs):                  # reset grad\n",
    "        for o in self.p: o.grad = None\n",
    "\n",
    "### Train parameters by performing SGD on each mini-batch in the dl ###\n",
    "def train_one_epoch(dl, mod, stepper):\n",
    "    for xb,yb in dl:           # for every minibatch (xb,yb) in the dataloader:\n",
    "        calc_grad(xb, yb, mod) # calc grad(loss(mod(xb),yb))\n",
    "        stepper.step()         # take step\n",
    "        stepper.zero_grad()    # reset grad\n",
    "\n",
    "### Get accuracy of mod on a mini-batch ###\n",
    "def mb_acc(yp,y):\n",
    "    yp_max,yp_i = torch.max(yp, dim=1, keepdim=True)\n",
    "    y_max, y_i  = torch.max(y,  dim=1, keepdim=True)\n",
    "    return (yp_i==y_i).float().mean()\n",
    "        \n",
    "### Get accuracy of mod on a dataloader (takes avg of all mbs in dl) ###\n",
    "def epoch_acc(dl, mod):\n",
    "    a = [mb_acc(mod(xb), yb) for xb,yb in dl]\n",
    "    return round(torch.stack(a).mean().item(), 5)          # avg acc over all mini-batches\n",
    "\n",
    "### Run `train_once` `epochs` times given data `dl`, model `mod`, and stepper `stepper`\n",
    "def train_n_epochs(dl, mod, stepper, nepochs):\n",
    "    accs = L()\n",
    "    for i in range(nepochs):\n",
    "        print('.',end='')\n",
    "        train_one_epoch(dl, mod, stepper)\n",
    "        accs += epoch_acc(dl, mod)\n",
    "    print('',end='\\t')\n",
    "    return accs\n",
    "\n",
    "### Perform n training sessions ###\n",
    "def train_n_sessions(dl, im_size, n_cls, hidden_params, nepochs, lr, nsessions):\n",
    "    accs = L()\n",
    "    mods = L()\n",
    "    print('Progress:',end='\\n')\n",
    "    for i in range(nsessions):\n",
    "        print(i,end='')\n",
    "        mod = init_mod(im_size, n_cls, hidden_params)\n",
    "        stepper = ParamStepper(mod.parameters(), lr)\n",
    "        accs += train_n_epochs(dl, mod, stepper, nepochs)\n",
    "        mods += mod\n",
    "        \n",
    "    # repack mods into nn.Sequential form\n",
    "    nmods = len(mods)\n",
    "    trained_models = L()\n",
    "    for n in range(nsessions):\n",
    "        layers = L()\n",
    "        for j in range(nmods//nsessions): layers.append(mods.pop())\n",
    "        layers.reverse()\n",
    "        seqmod = nn.Sequential(*layers.items)\n",
    "        trained_models += [seqmod]\n",
    "        \n",
    "    print('Done')\n",
    "    return accs, trained_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model once (to test if it's working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................\t"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(#20) [0.1911,0.46966,0.64053,0.65437,0.66237,0.66664,0.73297,0.74156,0.74684,0.74923...]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# params\n",
    "path          = untar_data(URLs.MNIST)\n",
    "n_cls         = 10\n",
    "im_size       = 28*28\n",
    "batch_size    = 64*2*2*2\n",
    "hidden_params = 30\n",
    "lr            = .1\n",
    "nepochs       = 20\n",
    "\n",
    "# inits\n",
    "dl            = init_data(path, im_size, n_cls, batch_size)\n",
    "mod           = init_mod(im_size, n_cls, hidden_params)\n",
    "stepper       = ParamStepper(mod.parameters(), lr)\n",
    "\n",
    "# train\n",
    "train_n_epochs(dl, mod, stepper, nepochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model many times (to understand its performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:\n",
      "0...\t1...\tDone\n",
      "Max accuracy: 0.7045300006866455\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.31146</td>\n",
       "      <td>0.46612</td>\n",
       "      <td>0.48164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.19789</td>\n",
       "      <td>0.36898</td>\n",
       "      <td>0.70453</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0        1        2\n",
       "0  0.31146  0.46612  0.48164\n",
       "1  0.19789  0.36898  0.70453"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# params\n",
    "path          = untar_data(URLs.MNIST)\n",
    "n_cls         = 10\n",
    "im_size       = 28*28\n",
    "batch_size    = 64*2*2*2\n",
    "hidden_params = 30\n",
    "nepochs       = 3\n",
    "lr            = .1\n",
    "nsessions     = 2\n",
    "\n",
    "# inits\n",
    "# dl = init_data(path, im_size, n_cls, batch_size)\n",
    "\n",
    "# train\n",
    "accs,mods = train_n_sessions(dl, im_size, n_cls, hidden_params, nepochs, lr, nsessions)\n",
    "accs_t = tensor(accs).reshape(nsessions,nepochs)\n",
    "\n",
    "# print max acc\n",
    "print(\"Max accuracy:\",accs_t.max().item())\n",
    "\n",
    "# print training sessions\n",
    "pd.DataFrame(accs_t.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diving into results of a single session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,yb = list(dl)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f08248a2a90>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_image(xb[0].view(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypb = mods[0](xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8.8069e-04, 4.1182e-02, 3.6174e-02, 1.2745e-02, 8.9230e-02, 3.8748e-04,\n",
       "        1.3135e-02, 7.4666e-03, 7.9591e-01, 2.8935e-03],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(ypb[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It thought the five was an 8."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

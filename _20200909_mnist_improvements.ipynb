{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Improvements-pursued-in-this-notebook\" data-toc-modified-id=\"Improvements-pursued-in-this-notebook-0.1\"><span class=\"toc-item-num\">0.1&nbsp;&nbsp;</span>Improvements pursued in this notebook</a></span></li></ul></li><li><span><a href=\"#First-round-of-improvements:-multi-label-classifier-&amp;-sessions\" data-toc-modified-id=\"First-round-of-improvements:-multi-label-classifier-&amp;-sessions-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>First round of improvements: multi-label classifier &amp; sessions</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvements pursued in this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Change from binary classifier to multi category classifier:\n",
    "    - add ims 0-9\n",
    "    - add change loss fxn to cross entropy loss w/ softmax\n",
    "    - change shape of final activation from 1 to 10\n",
    "    - change label to 1HE\n",
    "2. Add RGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Super-short version with all of the helpers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First round of improvements: multi-label classifier & sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create tools for training a model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "\n",
    "### Data ###\n",
    "def init_data(path, im_size, n_cls, batch_size):\n",
    "    ## Train\n",
    "    # ims\n",
    "    for i in range(n_cls):\n",
    "        new_ims = torch.stack(\n",
    "            [tensor(Image.open(fn)) for fn in (path/'training'/f'{i}').ls()]\n",
    "        ).float()/255\n",
    "        if i == 0: ims = new_ims\n",
    "        else: ims = torch.cat([ims,new_ims])\n",
    "    train_ims = ims.view(-1,im_size)\n",
    "    # lbls\n",
    "    train_lbls = []\n",
    "    for i in range(n_cls):\n",
    "        l = L([0]*n_cls)\n",
    "        l[i] = 1\n",
    "        train_lbls += [l] * len((path/'training'/f'{i}').ls())    \n",
    "    train_lbls = tensor(train_lbls)\n",
    "    ## Valid\n",
    "    # ims\n",
    "    for i in range(n_cls):\n",
    "        new_ims = torch.stack(\n",
    "            [tensor(Image.open(fn)) for fn in (path/'testing'/f'{i}').ls()]\n",
    "        ).float()/255\n",
    "        if i == 0: ims = new_ims\n",
    "        else: ims = torch.cat([ims,new_ims])\n",
    "    valid_ims = ims.view(-1,im_size)\n",
    "    # lbls\n",
    "    valid_lbls = []\n",
    "    for i in range(n_cls):\n",
    "        l = L([0]*n_cls)\n",
    "        l[i] = 1\n",
    "        valid_lbls += [l] * len((path/'testing'/f'{i}').ls())    \n",
    "    valid_lbls = tensor(valid_lbls)\n",
    "    ## DataLoaders\n",
    "    train_ds = L(zip(train_ims, train_lbls))\n",
    "    valid_ds = L(zip(valid_ims, valid_lbls))\n",
    "    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    valid_dl = DataLoader(valid_ds, batch_size=batch_size, shuffle=True)\n",
    "    return train_dl\n",
    "\n",
    "### Model ###\n",
    "def init_mod(im_size, n_cls, hidden_params):\n",
    "    mod = nn.Sequential(\n",
    "        nn.Linear(im_size,hidden_params),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_params,n_cls)\n",
    "    )\n",
    "    return mod\n",
    "\n",
    "### Loss ###\n",
    "def softmax(t):\n",
    "    if len(t.shape) == 1: return torch.exp(t) / torch.exp(t).sum()\n",
    "    else:                 return torch.exp(t) / torch.exp(t).sum(dim=1, keepdim=True)\n",
    "def loss(yp, y):\n",
    "    return (1 - (y*softmax(yp)).sum(dim=1, keepdim=True)).mean()\n",
    "\n",
    "### Calculate gradients for use in train_once ###\n",
    "def calc_grad(x,y,mod):\n",
    "    yp = mod(x)     # get predictions\n",
    "    ls = loss(yp,y) # calculate loss\n",
    "    ls.backward()   # take gradient w.r.t. loss\n",
    "\n",
    "### Create SGD Stepper; args = (mod.parameters(), lr) ###\n",
    "class ParamStepper:\n",
    "    def __init__(self, p, lr): self.p,self.lr = list(p),lr # initialize w/ mod.params & lr\n",
    "        \n",
    "    def step(self, *args, **kwargs):                       # take step\n",
    "        for o in self.p: o.data -= o.grad.data * self.lr\n",
    "            \n",
    "    def zero_grad(self, *args, **kwargs):                  # reset grad\n",
    "        for o in self.p: o.grad = None\n",
    "\n",
    "### Train parameters by performing SGD on each mini-batch in the dl ###\n",
    "def train_one_epoch(dl, mod, stepper):\n",
    "    for xb,yb in dl:           # for every minibatch (xb,yb) in the dataloader:\n",
    "        calc_grad(xb, yb, mod) # calc grad(loss(mod(xb),yb))\n",
    "        stepper.step()         # take step\n",
    "        stepper.zero_grad()    # reset grad\n",
    "\n",
    "### Get accuracy of mod on a mini-batch ###\n",
    "def mb_acc(yp,y):\n",
    "    yp_max,yp_i = torch.max(yp, dim=1, keepdim=True)\n",
    "    y_max, y_i  = torch.max(y,  dim=1, keepdim=True)\n",
    "    return (yp_i==y_i).float().mean()\n",
    "        \n",
    "### Get accuracy of mod on a dataloader (takes avg of all mbs in dl) ###\n",
    "def epoch_acc(dl, mod):\n",
    "    a = [mb_acc(mod(xb), yb) for xb,yb in dl]\n",
    "    return round(torch.stack(a).mean().item(), 5)          # avg acc over all mini-batches\n",
    "\n",
    "### Run `train_once` `epochs` times given data `dl`, model `mod`, and stepper `stepper`\n",
    "def train_n_epochs(dl, mod, stepper, nepochs):\n",
    "    l = L()\n",
    "    for i in range(nepochs):\n",
    "        print('.',end='')\n",
    "        train_one_epoch(dl, mod, stepper)\n",
    "        l += epoch_acc(dl, mod)\n",
    "    print('',end='\\t')\n",
    "    return l\n",
    "\n",
    "### Perform n training sessions ###\n",
    "def train_n_sessions(dl, im_size, n_cls, hidden_params, nepochs, lr, nsessions):\n",
    "    l = L()\n",
    "    print('Progress:',end='\\n')\n",
    "    for i in range(nsessions):\n",
    "        print(i,end='')\n",
    "        mod = init_mod(im_size, n_cls, hidden_params)\n",
    "        stepper = ParamStepper(mod.parameters(), lr)\n",
    "        l += train_n_epochs(dl, mod, stepper, nepochs)\n",
    "    print('Done')\n",
    "    return tensor(l).reshape(nsessions,nepochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train model once to make sure it works:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................\t"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(#20) [0.1911,0.46966,0.64053,0.65437,0.66237,0.66664,0.73297,0.74156,0.74684,0.74923...]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Train n epochs ###\n",
    "# params\n",
    "path          = untar_data(URLs.MNIST)\n",
    "n_cls         = 10\n",
    "im_size       = 28*28\n",
    "batch_size    = 64*2*2*2\n",
    "hidden_params = 30\n",
    "lr            = .1\n",
    "nepochs       = 20\n",
    "\n",
    "# inits\n",
    "dl            = init_data(path, im_size, n_cls, batch_size)\n",
    "mod           = init_mod(im_size, n_cls, hidden_params)\n",
    "stepper       = ParamStepper(mod.parameters(), lr)\n",
    "\n",
    "# train\n",
    "train_n_epochs(dl, mod, stepper, nepochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train model n sessions to find max accuracy:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:\n",
      "0..................................................\t1..................................................\t2..................................................\t3..................................................\t4..................................................\t5..................................................\t6..................................................\t7..................................................\tDone\n",
      "Max accuracy: 0.9184899926185608\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.19816</td>\n",
       "      <td>0.46343</td>\n",
       "      <td>0.64134</td>\n",
       "      <td>0.65687</td>\n",
       "      <td>0.66370</td>\n",
       "      <td>0.66759</td>\n",
       "      <td>0.73384</td>\n",
       "      <td>0.74391</td>\n",
       "      <td>0.74940</td>\n",
       "      <td>0.75184</td>\n",
       "      <td>...</td>\n",
       "      <td>0.85144</td>\n",
       "      <td>0.85216</td>\n",
       "      <td>0.85299</td>\n",
       "      <td>0.85288</td>\n",
       "      <td>0.85288</td>\n",
       "      <td>0.85292</td>\n",
       "      <td>0.85344</td>\n",
       "      <td>0.85316</td>\n",
       "      <td>0.85412</td>\n",
       "      <td>0.85388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.19196</td>\n",
       "      <td>0.37878</td>\n",
       "      <td>0.57947</td>\n",
       "      <td>0.64849</td>\n",
       "      <td>0.65720</td>\n",
       "      <td>0.66395</td>\n",
       "      <td>0.66559</td>\n",
       "      <td>0.66729</td>\n",
       "      <td>0.66886</td>\n",
       "      <td>0.67025</td>\n",
       "      <td>...</td>\n",
       "      <td>0.75618</td>\n",
       "      <td>0.75630</td>\n",
       "      <td>0.75632</td>\n",
       "      <td>0.75618</td>\n",
       "      <td>0.75589</td>\n",
       "      <td>0.75707</td>\n",
       "      <td>0.75724</td>\n",
       "      <td>0.75636</td>\n",
       "      <td>0.09861</td>\n",
       "      <td>0.09840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.33979</td>\n",
       "      <td>0.54150</td>\n",
       "      <td>0.62218</td>\n",
       "      <td>0.71938</td>\n",
       "      <td>0.73084</td>\n",
       "      <td>0.73652</td>\n",
       "      <td>0.74053</td>\n",
       "      <td>0.74377</td>\n",
       "      <td>0.74542</td>\n",
       "      <td>0.74738</td>\n",
       "      <td>...</td>\n",
       "      <td>0.83571</td>\n",
       "      <td>0.83559</td>\n",
       "      <td>0.83667</td>\n",
       "      <td>0.83729</td>\n",
       "      <td>0.83769</td>\n",
       "      <td>0.83786</td>\n",
       "      <td>0.83818</td>\n",
       "      <td>0.83907</td>\n",
       "      <td>0.83917</td>\n",
       "      <td>0.83998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.32561</td>\n",
       "      <td>0.39221</td>\n",
       "      <td>0.55516</td>\n",
       "      <td>0.57349</td>\n",
       "      <td>0.65191</td>\n",
       "      <td>0.66174</td>\n",
       "      <td>0.66531</td>\n",
       "      <td>0.66680</td>\n",
       "      <td>0.68988</td>\n",
       "      <td>0.73967</td>\n",
       "      <td>...</td>\n",
       "      <td>0.83193</td>\n",
       "      <td>0.83251</td>\n",
       "      <td>0.83293</td>\n",
       "      <td>0.83427</td>\n",
       "      <td>0.83475</td>\n",
       "      <td>0.83443</td>\n",
       "      <td>0.09854</td>\n",
       "      <td>0.09861</td>\n",
       "      <td>0.09890</td>\n",
       "      <td>0.09911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.42040</td>\n",
       "      <td>0.52091</td>\n",
       "      <td>0.63931</td>\n",
       "      <td>0.72160</td>\n",
       "      <td>0.73606</td>\n",
       "      <td>0.80492</td>\n",
       "      <td>0.81567</td>\n",
       "      <td>0.82236</td>\n",
       "      <td>0.82609</td>\n",
       "      <td>0.82894</td>\n",
       "      <td>...</td>\n",
       "      <td>0.91747</td>\n",
       "      <td>0.09883</td>\n",
       "      <td>0.09897</td>\n",
       "      <td>0.09875</td>\n",
       "      <td>0.09868</td>\n",
       "      <td>0.09854</td>\n",
       "      <td>0.09868</td>\n",
       "      <td>0.09890</td>\n",
       "      <td>0.09854</td>\n",
       "      <td>0.09875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.20793</td>\n",
       "      <td>0.43703</td>\n",
       "      <td>0.54513</td>\n",
       "      <td>0.70941</td>\n",
       "      <td>0.77675</td>\n",
       "      <td>0.80554</td>\n",
       "      <td>0.81683</td>\n",
       "      <td>0.82273</td>\n",
       "      <td>0.82707</td>\n",
       "      <td>0.83012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.85274</td>\n",
       "      <td>0.85304</td>\n",
       "      <td>0.85336</td>\n",
       "      <td>0.85382</td>\n",
       "      <td>0.85374</td>\n",
       "      <td>0.85432</td>\n",
       "      <td>0.85440</td>\n",
       "      <td>0.85451</td>\n",
       "      <td>0.85502</td>\n",
       "      <td>0.85538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.32405</td>\n",
       "      <td>0.28171</td>\n",
       "      <td>0.47103</td>\n",
       "      <td>0.68722</td>\n",
       "      <td>0.78754</td>\n",
       "      <td>0.80963</td>\n",
       "      <td>0.81622</td>\n",
       "      <td>0.82260</td>\n",
       "      <td>0.82670</td>\n",
       "      <td>0.82913</td>\n",
       "      <td>...</td>\n",
       "      <td>0.09861</td>\n",
       "      <td>0.09847</td>\n",
       "      <td>0.09854</td>\n",
       "      <td>0.09911</td>\n",
       "      <td>0.09890</td>\n",
       "      <td>0.09825</td>\n",
       "      <td>0.09847</td>\n",
       "      <td>0.09890</td>\n",
       "      <td>0.09861</td>\n",
       "      <td>0.09875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.21432</td>\n",
       "      <td>0.36160</td>\n",
       "      <td>0.68459</td>\n",
       "      <td>0.77192</td>\n",
       "      <td>0.79975</td>\n",
       "      <td>0.81397</td>\n",
       "      <td>0.81914</td>\n",
       "      <td>0.82467</td>\n",
       "      <td>0.82668</td>\n",
       "      <td>0.83039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.85334</td>\n",
       "      <td>0.85391</td>\n",
       "      <td>0.85433</td>\n",
       "      <td>0.85418</td>\n",
       "      <td>0.85461</td>\n",
       "      <td>0.85513</td>\n",
       "      <td>0.85529</td>\n",
       "      <td>0.85502</td>\n",
       "      <td>0.85561</td>\n",
       "      <td>0.83147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0        1        2        3        4        5        6        7   \\\n",
       "0  0.19816  0.46343  0.64134  0.65687  0.66370  0.66759  0.73384  0.74391   \n",
       "1  0.19196  0.37878  0.57947  0.64849  0.65720  0.66395  0.66559  0.66729   \n",
       "2  0.33979  0.54150  0.62218  0.71938  0.73084  0.73652  0.74053  0.74377   \n",
       "3  0.32561  0.39221  0.55516  0.57349  0.65191  0.66174  0.66531  0.66680   \n",
       "4  0.42040  0.52091  0.63931  0.72160  0.73606  0.80492  0.81567  0.82236   \n",
       "5  0.20793  0.43703  0.54513  0.70941  0.77675  0.80554  0.81683  0.82273   \n",
       "6  0.32405  0.28171  0.47103  0.68722  0.78754  0.80963  0.81622  0.82260   \n",
       "7  0.21432  0.36160  0.68459  0.77192  0.79975  0.81397  0.81914  0.82467   \n",
       "\n",
       "        8        9   ...       40       41       42       43       44  \\\n",
       "0  0.74940  0.75184  ...  0.85144  0.85216  0.85299  0.85288  0.85288   \n",
       "1  0.66886  0.67025  ...  0.75618  0.75630  0.75632  0.75618  0.75589   \n",
       "2  0.74542  0.74738  ...  0.83571  0.83559  0.83667  0.83729  0.83769   \n",
       "3  0.68988  0.73967  ...  0.83193  0.83251  0.83293  0.83427  0.83475   \n",
       "4  0.82609  0.82894  ...  0.91747  0.09883  0.09897  0.09875  0.09868   \n",
       "5  0.82707  0.83012  ...  0.85274  0.85304  0.85336  0.85382  0.85374   \n",
       "6  0.82670  0.82913  ...  0.09861  0.09847  0.09854  0.09911  0.09890   \n",
       "7  0.82668  0.83039  ...  0.85334  0.85391  0.85433  0.85418  0.85461   \n",
       "\n",
       "        45       46       47       48       49  \n",
       "0  0.85292  0.85344  0.85316  0.85412  0.85388  \n",
       "1  0.75707  0.75724  0.75636  0.09861  0.09840  \n",
       "2  0.83786  0.83818  0.83907  0.83917  0.83998  \n",
       "3  0.83443  0.09854  0.09861  0.09890  0.09911  \n",
       "4  0.09854  0.09868  0.09890  0.09854  0.09875  \n",
       "5  0.85432  0.85440  0.85451  0.85502  0.85538  \n",
       "6  0.09825  0.09847  0.09890  0.09861  0.09875  \n",
       "7  0.85513  0.85529  0.85502  0.85561  0.83147  \n",
       "\n",
       "[8 rows x 50 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Train n sessions ###\n",
    "# params\n",
    "path          = untar_data(URLs.MNIST)\n",
    "n_cls         = 10\n",
    "im_size       = 28*28\n",
    "batch_size    = 64*2*2*2\n",
    "\n",
    "# inits\n",
    "dl = init_data(path, im_size, n_cls, batch_size)\n",
    "\n",
    "# train\n",
    "test1 = train_n_sessions(dl, im_size, n_cls, hidden_params=30, nepochs=50, lr=.1, nsessions=8)\n",
    "\n",
    "# print max acc\n",
    "print(\"Max accuracy:\",test1.max().item())\n",
    "\n",
    "# print training sessions\n",
    "pd.DataFrame(test1.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

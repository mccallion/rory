{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Improvements-pursued-in-this-notebook\" data-toc-modified-id=\"Improvements-pursued-in-this-notebook-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Improvements pursued in this notebook</a></span></li><li><span><a href=\"#MNIST-Basics:-Final-Code\" data-toc-modified-id=\"MNIST-Basics:-Final-Code-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>MNIST Basics: Final Code</a></span><ul class=\"toc-item\"><li><span><a href=\"#With-SGD-&quot;by-hand&quot;\" data-toc-modified-id=\"With-SGD-&quot;by-hand&quot;-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>With SGD \"by hand\"</a></span></li><li><span><a href=\"#With-fastai.Learner\" data-toc-modified-id=\"With-fastai.Learner-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>With fastai.Learner</a></span></li></ul></li><li><span><a href=\"#Classify-all-10-digits\" data-toc-modified-id=\"Classify-all-10-digits-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Classify all 10 digits</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvements pursued in this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Add RGB\n",
    "- Change from binary classifier to multi category classifier:\n",
    "    - add ims 0-9\n",
    "    - add change loss fxn to cross entropy loss w/ softmax\n",
    "    - change shape of final activation from 1 to 10\n",
    "    - change label to 1HE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Basics: Final Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With SGD \"by hand\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports ###\n",
    "from fastai.vision.all import *\n",
    "from fastcore.test import *\n",
    "from fastbook import plot_function\n",
    "\n",
    "\n",
    "### Data ###\n",
    "path = untar_data(URLs.MNIST_SAMPLE)\n",
    "Path.BASE_PATH = path\n",
    "\n",
    "# train\n",
    "train3 = torch.stack( \n",
    "    [tensor(Image.open(o)) for o in (path/'train'/'3').ls()]         # Create list of 6131 28x28 im tensors, and\n",
    "    ).float()/255                                                    ## stack them into one [6131,28,28] tensor.\n",
    "train7 = torch.stack(\n",
    "    [tensor(Image.open(o)) for o in (path/'train'/'7').ls()]         # Repeat for sevens.\n",
    "    ).float()/255\n",
    "train_ims = torch.cat([train3,train7]).view(-1, 28*28)               # Combine 3s a& 7s then reshape as [6131,786].\n",
    "train_lbls = tensor([1]*len(train3) + [0]*len(train7)).unsqueeze(1)  # Create lbl tensors: 1 if im is 3 else 0.\n",
    "train_ds = list(zip(train_ims,train_lbls))                           # Zip im,lbl to create dataset.\n",
    "train_dl = DataLoader(train_ds, batch_size = 64*2*2*2, shuffle=True) # Create batches; create DataLoader (an iter).\n",
    "\n",
    "# valid (not used until fastai.Learner)\n",
    "valid3 = torch.stack(\n",
    "    [tensor(Image.open(o)) for o in (path/'valid'/'3').ls()]\n",
    "    ).float()/255\n",
    "valid7 = torch.stack(\n",
    "    [tensor(Image.open(o)) for o in (path/'valid'/'7').ls()]\n",
    "    ).float()/255\n",
    "valid_ims = torch.cat([valid3,valid7]).view(-1, 28*28)\n",
    "valid_lbls = tensor([1]*len(valid3) + [0]*len(valid7)).unsqueeze(1)\n",
    "valid_ds = list(zip(valid_ims,valid_lbls))\n",
    "valid_dl = DataLoader(valid_ds, batch_size = 64*2*2*2, shuffle=True)\n",
    "\n",
    "\n",
    "### Loss Function ###\n",
    "def calibrated_confidence(yp, y, pctile=.95,top_activ=20):           # Loss function \"Calibrated Confidence\":\n",
    "    pred = yp.sigmoid()                                              # - correct   & high confidence → low loss\n",
    "    return torch.where(y==1, 1-pred, pred).mean()                    # - incorrect & high confidence → high loss\n",
    "\n",
    "\n",
    "### Create Model & Initialize Params ###\n",
    "three_layer_nn = nn.Sequential(                            # nn.Sequential() defines a model of composed fxns\n",
    "    nn.Linear(28*28,30),                                   # nn.Linear.__init__(w,b) creates w,b with rand values\n",
    "    nn.ReLU(),                                             # nn.Linear.__main__(t) returns t@w+b\n",
    "    nn.Linear(30,1))                                       # nn.ReLU().__main__(t) returns t.max(tensor(0.))\n",
    "\n",
    "\n",
    "### Create SGD Stepper ###\n",
    "class ParamStepper:\n",
    "    def __init__(self, p, lr): self.p,self.lr = list(p),lr # remembers your params & an lr\n",
    "        \n",
    "    def step(self, *args, **kwargs):                       # take one step in the optimal direction\n",
    "        for o in self.p: o.data -= o.grad.data * self.lr\n",
    "            \n",
    "    def zero_grad(self, *args, **kwargs):                  # zeros out gradients\n",
    "        for o in self.p: o.grad = None\n",
    "\n",
    "\n",
    "### Calculate accuracy over one mini-batch given yp,y ###\n",
    "def avg_batch_acc(yp,y):\n",
    "    sig_yp = yp.sigmoid()\n",
    "    correct = (sig_yp > 0.5) == y\n",
    "    return correct.float().mean()\n",
    "\n",
    "\n",
    "### Calculate accuracy over entire dl given dl,mod ###\n",
    "def validate_epoch(dl, mod):                               # Given data `dl` and a model & params `mod`\n",
    "    acc = [avg_batch_acc(mod(xb), yb) for xb,yb in dl]     # Gradients calculated & stored at mod(xb) call\n",
    "    return round(torch.stack(acc).mean().item(), 5)        # Avg over all mini-batches, return scalar (not tensor)\n",
    "\n",
    "\n",
    "### Adjust parameters w/ stepper for each mini-batch in a dl\n",
    "def train_once(dl, mod, stepper):\n",
    "    for xb,yb in dl:\n",
    "        calc_grad(xb, yb, mod)\n",
    "        stepper.step()\n",
    "        stepper.zero_grad()\n",
    "\n",
    "\n",
    "### Run `train_once` `n` times given data `dl`, model `mod`, and stepper `stepper`\n",
    "def train_model(dl, mod, stepper, n):\n",
    "    for i in range(n):\n",
    "        train_once(dl, mod, stepper)\n",
    "        print(validate_epoch(dl, mod), end='\\t')\n",
    "\n",
    "# together\n",
    "sgd = ParamStepper(three_layer_nn.parameters(), lr=1.)\n",
    "train_model(train_dl, three_layer_nn, sgd, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With fastai.Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports ###\n",
    "from fastai.vision.all import *\n",
    "from fastcore.test import *\n",
    "\n",
    "\n",
    "### Data ###\n",
    "path = untar_data(URLs.MNIST_SAMPLE)\n",
    "Path.BASE_PATH = path\n",
    "\n",
    "# train\n",
    "train3 = torch.stack( \n",
    "    [tensor(Image.open(o)) for o in (path/'train'/'3').ls()]         # Create list of 6131 28x28 im tensors, and\n",
    "    ).float()/255                                                    ## stack them into one [6131,28,28] tensor.\n",
    "train7 = torch.stack(\n",
    "    [tensor(Image.open(o)) for o in (path/'train'/'7').ls()]         # Repeat for sevens.\n",
    "    ).float()/255\n",
    "train_ims = torch.cat([train3,train7]).view(-1, 28*28)               # Combine 3s a& 7s then reshape as [6131,786].\n",
    "train_lbls = tensor([1]*len(train3) + [0]*len(train7)).unsqueeze(1)  # Create lbl tensors: 1 if im is 3 else 0.\n",
    "train_ds = list(zip(train_ims,train_lbls))                           # Zip im,lbl to create dataset.\n",
    "train_dl = DataLoader(train_ds, batch_size = 64*2*2*2, shuffle=True) # Create batches; create DataLoader (an iter).\n",
    "\n",
    "# valid\n",
    "valid3 = torch.stack(\n",
    "    [tensor(Image.open(o)) for o in (path/'valid'/'3').ls()]\n",
    "    ).float()/255\n",
    "valid7 = torch.stack(\n",
    "    [tensor(Image.open(o)) for o in (path/'valid'/'7').ls()]\n",
    "    ).float()/255\n",
    "valid_ims = torch.cat([valid3,valid7]).view(-1, 28*28)\n",
    "valid_lbls = tensor([1]*len(valid3) + [0]*len(valid7)).unsqueeze(1)\n",
    "valid_ds = list(zip(valid_ims,valid_lbls))\n",
    "valid_dl = DataLoader(valid_ds, batch_size = 64*2*2*2, shuffle=True)\n",
    "\n",
    "dls = DataLoaders(train_dl, valid_dl)\n",
    "\n",
    "\n",
    "## Model ###\n",
    "three_layer_nn = nn.Sequential( # nn.Sequential composes fxns. Each fxn is a layer, ∴ this is a 3 layer nn.\n",
    "    nn.Linear(28*28,30),        # nn.Linear creates linear parameters W and B as in Y = X@W+B.\n",
    "    nn.ReLU(),                  # nn.Linear is a class. When called, it's __main__(x) function computes X@W+B.\n",
    "    nn.Linear(30,1))            # nn.ReLU is the same as item-wise max(t, 0), which replaces all negs with 0s.\n",
    "\n",
    "\n",
    "### Loss Function \"Calibrated Confidence\" ###\n",
    "def loss(yp, y):                                  # I like to call this \"Calibrated Confidence\":\n",
    "    pred = yp.sigmoid()                           # - correct   & high confidence → low loss\n",
    "    return torch.where(y==1, 1-pred, pred).mean() # - incorrect & high confidence → high loss\n",
    "\n",
    "\n",
    "### Mini-batch Average Accuracy given yp,y ###\n",
    "def avg_batch_acc(yp,y):\n",
    "    sig_yp = yp.sigmoid()\n",
    "    correct = (sig_yp > 0.5) == y\n",
    "    return correct.float().mean()\n",
    "\n",
    "\n",
    "### Combine data, model, stepper, loss, accuracy in a Learner ###\n",
    "learn = Learner(dls,                   # train and valid dls\n",
    "                three_layer_nn,        # model\n",
    "                opt_func=SGD,          # fastai.SGD optimizer\n",
    "                loss_func=loss,        # loss fxn\n",
    "                metrics=avg_batch_acc) # judgement metric\n",
    "\n",
    "\n",
    "### Train ###\n",
    "epochs = 40\n",
    "lr = 0.1\n",
    "learn.fit(epochs,lr)\n",
    "plt.plot(L(learn.recorder.values).itemgot(2)) # L.itemgot(2) returns every 2nd row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Super-short version with all of the helpers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.056472</td>\n",
       "      <td>0.009414</td>\n",
       "      <td>0.998037</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from fastai.vision.all import *\n",
    "\n",
    "path = untar_data(URLs.MNIST_SAMPLE)\n",
    "Path.BASE_PATH = path\n",
    "\n",
    "dls = ImageDataLoaders.from_folder(path)\n",
    "learn = cnn_learner(dls, resnet18, pretrained=False, loss_func=F.cross_entropy, metrics=accuracy)\n",
    "learn.fit_one_cycle(1, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify all 10 digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "\n",
    "### Data ###\n",
    "path = untar_data(URLs.MNIST)\n",
    "n_cls = 10\n",
    "batch_size = 64*2*2*2\n",
    "\n",
    "# Train\n",
    "# ims\n",
    "for i in range(n_cls):\n",
    "    new_ims = torch.stack(\n",
    "        [tensor(Image.open(fn)) for fn in (path/'training'/f'{i}').ls()]\n",
    "    ).float()/255\n",
    "    if i == 0: ims = new_ims\n",
    "    else: ims = torch.cat([ims,new_ims])\n",
    "train_ims = ims.view(-1,28*28)\n",
    "# lbls\n",
    "train_lbls = []\n",
    "for i in range(n_cls):\n",
    "    l = L([0]*n_cls)\n",
    "    l[i] = 1\n",
    "    lbls += [l] * len((path/'training'/f'{i}').ls())    \n",
    "train_lbls = tensor(lbls)\n",
    "\n",
    "# Valid\n",
    "# ims\n",
    "for i in range(n_cls):\n",
    "    new_ims = torch.stack(\n",
    "        [tensor(Image.open(fn)) for fn in (path/'testing'/f'{i}').ls()]\n",
    "    ).float()/255\n",
    "    if i == 0: ims = new_ims\n",
    "    else: ims = torch.cat([ims,new_ims])\n",
    "valid_ims = ims.view(-1,28*28)\n",
    "# lbls\n",
    "valid_lbls = []\n",
    "for i in range(n_cls):\n",
    "    l = L([0]*n_cls)\n",
    "    l[i] = 1\n",
    "    lbls += [l] * len((path/'testing'/f'{i}').ls())    \n",
    "valid_lbls = tensor(lbls)\n",
    "\n",
    "# DataLoaders\n",
    "train_ds = L(zip(train_ims, train_lbls))\n",
    "valid_ds = L(zip(valid_ims, valid_lbls))\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size, shuffle=True)\n",
    "dls = DataLoaders(train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.185\t0.21428\t0.22531\t0.13134\t0.22749\t0.31196\t0.34463\t0.37202\t0.39915\t0.41556\t0.43454\t0.43852\t0.44263\t0.44541\t0.44966\t0.45151\t0.45441\t0.45545\t0.45853\t0.46076\t"
     ]
    }
   ],
   "source": [
    "### Model ###\n",
    "my_nn = nn.Sequential(\n",
    "    nn.Linear(28*28,50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50,30),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(30,10))\n",
    "\n",
    "\n",
    "### Loss & Accuracy ###\n",
    "def softmax(t):\n",
    "    if len(t.shape) == 1: return torch.exp(t) / torch.exp(t).sum()\n",
    "    else:                 return torch.exp(t) / torch.exp(t).sum(dim=1, keepdim=True)\n",
    "\n",
    "def loss(yp, y):\n",
    "    yps = softmax(yp)\n",
    "    return (1 - (y * yps).sum(dim=1, keepdim=True)).mean()\n",
    "\n",
    "def acc(yp,y):\n",
    "    yp_max,yp_i = torch.max(yp, dim=1, keepdim=True)\n",
    "    y_max, y_i  = torch.max(y,  dim=1, keepdim=True)\n",
    "    return (yp_i==y_i).float().mean()\n",
    "\n",
    "### Create SGD Stepper ###\n",
    "class ParamStepper:\n",
    "    def __init__(self, p, lr): self.p,self.lr = list(p),lr # remembers your params & lr\n",
    "        \n",
    "    def step(self, *args, **kwargs):                       # take one step in the optimal direction\n",
    "        for o in self.p: o.data -= o.grad.data * self.lr\n",
    "            \n",
    "    def zero_grad(self, *args, **kwargs):                  # zeros out gradients\n",
    "        for o in self.p: o.grad = None\n",
    "\n",
    "### Calculate accuracy over entire dl given dl,mod ###\n",
    "def validate_epoch(dl, mod):                       # Given data `dl` and a model & params `mod`\n",
    "    a = [acc(mod(xb), yb) for xb,yb in dl]         # Gradients calculated & stored at mod(xb) call\n",
    "    return round(torch.stack(a).mean().item(), 5)  # Avg over all mini-batches, return scalar (not tensor)\n",
    "\n",
    "\n",
    "### Adjust parameters w/ stepper for each mini-batch in a dl\n",
    "def train_once(dl, mod, stepper):\n",
    "    for xb,yb in dl:\n",
    "        calc_grad(xb, yb, mod)\n",
    "        stepper.step()\n",
    "        stepper.zero_grad()\n",
    "\n",
    "\n",
    "### Calculate gradients for use in train_once ###\n",
    "def calc_grad(x,y,model):\n",
    "    yp = model(x)\n",
    "    ls = loss(yp,y)\n",
    "    ls.backward()\n",
    "\n",
    "\n",
    "### Run `train_once` `n` times given data `dl`, model `mod`, and stepper `stepper`\n",
    "def train_model(dl, mod, stepper, n):\n",
    "    for i in range(n):\n",
    "        train_once(dl, mod, stepper)\n",
    "        print(validate_epoch(dl, mod), end='\\t')\n",
    "\n",
    "\n",
    "### Train ###\n",
    "dl = train_dl\n",
    "sgd = ParamStepper(p=my_nn.parameters(), lr=.1)\n",
    "train_model(dl, my_nn, sgd, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

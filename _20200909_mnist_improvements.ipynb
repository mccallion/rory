{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Improvements-pursued-in-this-notebook\" data-toc-modified-id=\"Improvements-pursued-in-this-notebook-0.1\"><span class=\"toc-item-num\">0.1&nbsp;&nbsp;</span>Improvements pursued in this notebook</a></span></li></ul></li><li><span><a href=\"#Improvement-#1:-multi-label-classification\" data-toc-modified-id=\"Improvement-#1:-multi-label-classification-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Improvement #1: multi-label classification</a></span><ul class=\"toc-item\"><li><span><a href=\"#Performance-Tweaks\" data-toc-modified-id=\"Performance-Tweaks-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Performance Tweaks</a></span><ul class=\"toc-item\"><li><span><a href=\"#Leaky-ReLU\" data-toc-modified-id=\"Leaky-ReLU-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Leaky ReLU</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvements pursued in this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Change from binary classifier to multi category classifier:\n",
    "    - add ims 0-9\n",
    "    - add change loss fxn to cross entropy loss w/ softmax\n",
    "    - change shape of final activation from 1 to 10\n",
    "    - change label to 1HE\n",
    "2. Add RGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Super-short version with all of the helpers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvement #1: multi-label classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "\n",
    "### Data ###\n",
    "def init_data(path, im_size, n_cls, batch_size):\n",
    "    ## Train\n",
    "    # ims\n",
    "    for i in range(n_cls):\n",
    "        new_ims = torch.stack(\n",
    "            [tensor(Image.open(fn)) for fn in (path/'training'/f'{i}').ls()]\n",
    "        ).float()/255\n",
    "        if i == 0: ims = new_ims\n",
    "        else: ims = torch.cat([ims,new_ims])\n",
    "    train_ims = ims.view(-1,im_size)\n",
    "    # lbls\n",
    "    train_lbls = []\n",
    "    for i in range(n_cls):\n",
    "        l = L([0]*n_cls)\n",
    "        l[i] = 1\n",
    "        train_lbls += [l] * len((path/'training'/f'{i}').ls())    \n",
    "    train_lbls = tensor(train_lbls)\n",
    "    ## Valid\n",
    "    # ims\n",
    "    for i in range(n_cls):\n",
    "        new_ims = torch.stack(\n",
    "            [tensor(Image.open(fn)) for fn in (path/'testing'/f'{i}').ls()]\n",
    "        ).float()/255\n",
    "        if i == 0: ims = new_ims\n",
    "        else: ims = torch.cat([ims,new_ims])\n",
    "    valid_ims = ims.view(-1,im_size)\n",
    "    # lbls\n",
    "    valid_lbls = []\n",
    "    for i in range(n_cls):\n",
    "        l = L([0]*n_cls)\n",
    "        l[i] = 1\n",
    "        valid_lbls += [l] * len((path/'testing'/f'{i}').ls())    \n",
    "    valid_lbls = tensor(valid_lbls)\n",
    "    ## DataLoaders\n",
    "    train_ds = L(zip(train_ims, train_lbls))\n",
    "    valid_ds = L(zip(valid_ims, valid_lbls))\n",
    "    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    valid_dl = DataLoader(valid_ds, batch_size=batch_size, shuffle=True)\n",
    "    return train_dl\n",
    "\n",
    "### Model ###\n",
    "def init_mod(im_size, n_cls, hidden_params):\n",
    "    mod = nn.Sequential(\n",
    "        nn.Linear(im_size,hidden_params),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_params,n_cls)\n",
    "    )\n",
    "    return mod\n",
    "\n",
    "### Create SGD Stepper; args = (mod.parameters(), lr) ###\n",
    "class ParamStepper:\n",
    "    def __init__(self, p, lr): self.p,self.lr = list(p),lr # remembers your params & lr\n",
    "        \n",
    "    def step(self, *args, **kwargs):                       # take one step in the optimal direction\n",
    "        for o in self.p: o.data -= o.grad.data * self.lr\n",
    "            \n",
    "    def zero_grad(self, *args, **kwargs):                  # zeros out gradients\n",
    "        for o in self.p: o.grad = None\n",
    "\n",
    "### Calculate accuracy over entire dl ###\n",
    "def validate_epoch(dl, mod):\n",
    "    a = [acc(mod(xb), yb) for xb,yb in dl]         # Gradients calculated & stored at mod(xb) call\n",
    "    return round(torch.stack(a).mean().item(), 5)  # Avg over all mini-batches, return scalar (not tensor)\n",
    "\n",
    "### Adjust parameters w/ stepper for each mini-batch in a dl\n",
    "def train_once(dl, mod, stepper):\n",
    "    for xb,yb in dl:\n",
    "        calc_grad(xb, yb, mod)\n",
    "        stepper.step()\n",
    "        stepper.zero_grad()\n",
    "\n",
    "### Calculate gradients for use in train_once ###\n",
    "def calc_grad(x,y,mod):\n",
    "    yp = mod(x)\n",
    "    ls = loss(yp,y)\n",
    "    ls.backward()\n",
    "\n",
    "### Run `train_once` `epochs` times given data `dl`, model `mod`, and stepper `stepper`\n",
    "def train_model(dl, mod, stepper, epochs):\n",
    "    l = L()\n",
    "    for i in range(epochs):\n",
    "        train_once(dl, mod, stepper)\n",
    "        # print(validate_epoch(dl, mod), end='\\t')\n",
    "        l += validate_epoch(dl, mod)\n",
    "    return l\n",
    "\n",
    "### Perform n training sessions ###\n",
    "def train_model_n_times(dl, im_size, n_cls, hidden_params, epochs, lr, n):\n",
    "    o = L()\n",
    "    print('Current Session:',end='  ')\n",
    "    for i in range(n):\n",
    "        print(i,end='  ')\n",
    "        mod = init_mod(im_size, n_cls, hidden_params)\n",
    "        stepper = ParamStepper(mod.parameters(), lr)\n",
    "        o += train_model(dl, mod, stepper, epochs)\n",
    "    print('Done')\n",
    "    return tensor(o).reshape(n,epochs)\n",
    "    \n",
    "### Loss & Accuracy ###\n",
    "def softmax(t):\n",
    "    if len(t.shape) == 1: return torch.exp(t) / torch.exp(t).sum()\n",
    "    else:                 return torch.exp(t) / torch.exp(t).sum(dim=1, keepdim=True)\n",
    "def loss(yp, y):\n",
    "    yps = softmax(yp)\n",
    "    return (1 - (y * yps).sum(dim=1, keepdim=True)).mean()\n",
    "def acc(yp,y):\n",
    "    yp_max,yp_i = torch.max(yp, dim=1, keepdim=True)\n",
    "    y_max, y_i  = torch.max(y,  dim=1, keepdim=True)\n",
    "    return (yp_i==y_i).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Init ###\n",
    "path          = untar_data(URLs.MNIST)\n",
    "n_cls         = 10\n",
    "im_size       = 28*28\n",
    "batch_size    = 64*2*2*2\n",
    "dl            = init_data(path, im_size, n_cls, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#20) [0.33621,0.49281,0.65015,0.78121,0.8036,0.81546,0.8212,0.82522,0.82848,0.8305...]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod = init_mod(im_size, n_cls, hidden_params)\n",
    "stepper = ParamStepper(mod.parameters(),lr)\n",
    "train_model(dl, mod, stepper, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Session:  0  1  2  3  4  5  6  7  8  9  Done\n"
     ]
    }
   ],
   "source": [
    "### Training ###\n",
    "test1 = train_model_n_times(dl, im_size, n_cls, hidden_params=30, epochs=50, lr=.1, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.34518</td>\n",
       "      <td>0.38481</td>\n",
       "      <td>0.63659</td>\n",
       "      <td>0.65775</td>\n",
       "      <td>0.72319</td>\n",
       "      <td>0.73800</td>\n",
       "      <td>0.74455</td>\n",
       "      <td>0.74843</td>\n",
       "      <td>0.80427</td>\n",
       "      <td>0.81278</td>\n",
       "      <td>...</td>\n",
       "      <td>0.84084</td>\n",
       "      <td>0.84109</td>\n",
       "      <td>0.84129</td>\n",
       "      <td>0.84086</td>\n",
       "      <td>0.84203</td>\n",
       "      <td>0.84222</td>\n",
       "      <td>0.84172</td>\n",
       "      <td>0.84287</td>\n",
       "      <td>0.84270</td>\n",
       "      <td>0.84317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.27709</td>\n",
       "      <td>0.50341</td>\n",
       "      <td>0.59779</td>\n",
       "      <td>0.71376</td>\n",
       "      <td>0.73304</td>\n",
       "      <td>0.74010</td>\n",
       "      <td>0.74500</td>\n",
       "      <td>0.74773</td>\n",
       "      <td>0.75045</td>\n",
       "      <td>0.75266</td>\n",
       "      <td>...</td>\n",
       "      <td>0.92039</td>\n",
       "      <td>0.92127</td>\n",
       "      <td>0.92131</td>\n",
       "      <td>0.09890</td>\n",
       "      <td>0.09868</td>\n",
       "      <td>0.09868</td>\n",
       "      <td>0.09868</td>\n",
       "      <td>0.09875</td>\n",
       "      <td>0.09868</td>\n",
       "      <td>0.09868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.54159</td>\n",
       "      <td>0.48585</td>\n",
       "      <td>0.64357</td>\n",
       "      <td>0.65320</td>\n",
       "      <td>0.65737</td>\n",
       "      <td>0.66076</td>\n",
       "      <td>0.66368</td>\n",
       "      <td>0.66490</td>\n",
       "      <td>0.66585</td>\n",
       "      <td>0.70725</td>\n",
       "      <td>...</td>\n",
       "      <td>0.75614</td>\n",
       "      <td>0.75692</td>\n",
       "      <td>0.75694</td>\n",
       "      <td>0.75679</td>\n",
       "      <td>0.75701</td>\n",
       "      <td>0.75762</td>\n",
       "      <td>0.75804</td>\n",
       "      <td>0.75808</td>\n",
       "      <td>0.75792</td>\n",
       "      <td>0.75898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.24039</td>\n",
       "      <td>0.47394</td>\n",
       "      <td>0.64107</td>\n",
       "      <td>0.65256</td>\n",
       "      <td>0.65881</td>\n",
       "      <td>0.66215</td>\n",
       "      <td>0.66439</td>\n",
       "      <td>0.66647</td>\n",
       "      <td>0.67853</td>\n",
       "      <td>0.72142</td>\n",
       "      <td>...</td>\n",
       "      <td>0.91837</td>\n",
       "      <td>0.91861</td>\n",
       "      <td>0.91999</td>\n",
       "      <td>0.92063</td>\n",
       "      <td>0.92152</td>\n",
       "      <td>0.92169</td>\n",
       "      <td>0.92247</td>\n",
       "      <td>0.92234</td>\n",
       "      <td>0.09890</td>\n",
       "      <td>0.09883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.24443</td>\n",
       "      <td>0.59117</td>\n",
       "      <td>0.65282</td>\n",
       "      <td>0.66003</td>\n",
       "      <td>0.66554</td>\n",
       "      <td>0.66843</td>\n",
       "      <td>0.67730</td>\n",
       "      <td>0.74410</td>\n",
       "      <td>0.74721</td>\n",
       "      <td>0.75038</td>\n",
       "      <td>...</td>\n",
       "      <td>0.76985</td>\n",
       "      <td>0.77029</td>\n",
       "      <td>0.76995</td>\n",
       "      <td>0.77002</td>\n",
       "      <td>0.77106</td>\n",
       "      <td>0.77064</td>\n",
       "      <td>0.77116</td>\n",
       "      <td>0.77111</td>\n",
       "      <td>0.77193</td>\n",
       "      <td>0.77128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.24930</td>\n",
       "      <td>0.47674</td>\n",
       "      <td>0.59392</td>\n",
       "      <td>0.70975</td>\n",
       "      <td>0.72639</td>\n",
       "      <td>0.73461</td>\n",
       "      <td>0.73931</td>\n",
       "      <td>0.74260</td>\n",
       "      <td>0.80062</td>\n",
       "      <td>0.81894</td>\n",
       "      <td>...</td>\n",
       "      <td>0.91788</td>\n",
       "      <td>0.91874</td>\n",
       "      <td>0.09890</td>\n",
       "      <td>0.09868</td>\n",
       "      <td>0.09854</td>\n",
       "      <td>0.09890</td>\n",
       "      <td>0.09847</td>\n",
       "      <td>0.09847</td>\n",
       "      <td>0.09875</td>\n",
       "      <td>0.09868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.51975</td>\n",
       "      <td>0.51805</td>\n",
       "      <td>0.67444</td>\n",
       "      <td>0.72549</td>\n",
       "      <td>0.80854</td>\n",
       "      <td>0.81771</td>\n",
       "      <td>0.82331</td>\n",
       "      <td>0.83686</td>\n",
       "      <td>0.87542</td>\n",
       "      <td>0.88332</td>\n",
       "      <td>...</td>\n",
       "      <td>0.92269</td>\n",
       "      <td>0.92348</td>\n",
       "      <td>0.92337</td>\n",
       "      <td>0.09875</td>\n",
       "      <td>0.09847</td>\n",
       "      <td>0.09904</td>\n",
       "      <td>0.09861</td>\n",
       "      <td>0.09875</td>\n",
       "      <td>0.09890</td>\n",
       "      <td>0.09890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.52406</td>\n",
       "      <td>0.48904</td>\n",
       "      <td>0.62592</td>\n",
       "      <td>0.70262</td>\n",
       "      <td>0.71614</td>\n",
       "      <td>0.72606</td>\n",
       "      <td>0.73039</td>\n",
       "      <td>0.73366</td>\n",
       "      <td>0.73588</td>\n",
       "      <td>0.73756</td>\n",
       "      <td>...</td>\n",
       "      <td>0.83001</td>\n",
       "      <td>0.83048</td>\n",
       "      <td>0.83027</td>\n",
       "      <td>0.83114</td>\n",
       "      <td>0.83071</td>\n",
       "      <td>0.81577</td>\n",
       "      <td>0.09890</td>\n",
       "      <td>0.09854</td>\n",
       "      <td>0.09832</td>\n",
       "      <td>0.09854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.28422</td>\n",
       "      <td>0.54772</td>\n",
       "      <td>0.63704</td>\n",
       "      <td>0.71645</td>\n",
       "      <td>0.76977</td>\n",
       "      <td>0.81235</td>\n",
       "      <td>0.81922</td>\n",
       "      <td>0.82369</td>\n",
       "      <td>0.82690</td>\n",
       "      <td>0.82964</td>\n",
       "      <td>...</td>\n",
       "      <td>0.85110</td>\n",
       "      <td>0.85181</td>\n",
       "      <td>0.85203</td>\n",
       "      <td>0.85225</td>\n",
       "      <td>0.85240</td>\n",
       "      <td>0.85226</td>\n",
       "      <td>0.09832</td>\n",
       "      <td>0.09875</td>\n",
       "      <td>0.09890</td>\n",
       "      <td>0.09868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.25515</td>\n",
       "      <td>0.44452</td>\n",
       "      <td>0.69613</td>\n",
       "      <td>0.72194</td>\n",
       "      <td>0.73582</td>\n",
       "      <td>0.74087</td>\n",
       "      <td>0.74502</td>\n",
       "      <td>0.74759</td>\n",
       "      <td>0.74974</td>\n",
       "      <td>0.75161</td>\n",
       "      <td>...</td>\n",
       "      <td>0.76849</td>\n",
       "      <td>0.76867</td>\n",
       "      <td>0.76893</td>\n",
       "      <td>0.77047</td>\n",
       "      <td>0.80339</td>\n",
       "      <td>0.81678</td>\n",
       "      <td>0.82339</td>\n",
       "      <td>0.82577</td>\n",
       "      <td>0.82960</td>\n",
       "      <td>0.83089</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0        1        2        3        4        5        6        7   \\\n",
       "0  0.34518  0.38481  0.63659  0.65775  0.72319  0.73800  0.74455  0.74843   \n",
       "1  0.27709  0.50341  0.59779  0.71376  0.73304  0.74010  0.74500  0.74773   \n",
       "2  0.54159  0.48585  0.64357  0.65320  0.65737  0.66076  0.66368  0.66490   \n",
       "3  0.24039  0.47394  0.64107  0.65256  0.65881  0.66215  0.66439  0.66647   \n",
       "4  0.24443  0.59117  0.65282  0.66003  0.66554  0.66843  0.67730  0.74410   \n",
       "5  0.24930  0.47674  0.59392  0.70975  0.72639  0.73461  0.73931  0.74260   \n",
       "6  0.51975  0.51805  0.67444  0.72549  0.80854  0.81771  0.82331  0.83686   \n",
       "7  0.52406  0.48904  0.62592  0.70262  0.71614  0.72606  0.73039  0.73366   \n",
       "8  0.28422  0.54772  0.63704  0.71645  0.76977  0.81235  0.81922  0.82369   \n",
       "9  0.25515  0.44452  0.69613  0.72194  0.73582  0.74087  0.74502  0.74759   \n",
       "\n",
       "        8        9   ...       40       41       42       43       44  \\\n",
       "0  0.80427  0.81278  ...  0.84084  0.84109  0.84129  0.84086  0.84203   \n",
       "1  0.75045  0.75266  ...  0.92039  0.92127  0.92131  0.09890  0.09868   \n",
       "2  0.66585  0.70725  ...  0.75614  0.75692  0.75694  0.75679  0.75701   \n",
       "3  0.67853  0.72142  ...  0.91837  0.91861  0.91999  0.92063  0.92152   \n",
       "4  0.74721  0.75038  ...  0.76985  0.77029  0.76995  0.77002  0.77106   \n",
       "5  0.80062  0.81894  ...  0.91788  0.91874  0.09890  0.09868  0.09854   \n",
       "6  0.87542  0.88332  ...  0.92269  0.92348  0.92337  0.09875  0.09847   \n",
       "7  0.73588  0.73756  ...  0.83001  0.83048  0.83027  0.83114  0.83071   \n",
       "8  0.82690  0.82964  ...  0.85110  0.85181  0.85203  0.85225  0.85240   \n",
       "9  0.74974  0.75161  ...  0.76849  0.76867  0.76893  0.77047  0.80339   \n",
       "\n",
       "        45       46       47       48       49  \n",
       "0  0.84222  0.84172  0.84287  0.84270  0.84317  \n",
       "1  0.09868  0.09868  0.09875  0.09868  0.09868  \n",
       "2  0.75762  0.75804  0.75808  0.75792  0.75898  \n",
       "3  0.92169  0.92247  0.92234  0.09890  0.09883  \n",
       "4  0.77064  0.77116  0.77111  0.77193  0.77128  \n",
       "5  0.09890  0.09847  0.09847  0.09875  0.09868  \n",
       "6  0.09904  0.09861  0.09875  0.09890  0.09890  \n",
       "7  0.81577  0.09890  0.09854  0.09832  0.09854  \n",
       "8  0.85226  0.09832  0.09875  0.09890  0.09868  \n",
       "9  0.81678  0.82339  0.82577  0.82960  0.83089  \n",
       "\n",
       "[10 rows x 50 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(test1.numpy())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Tweaks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leaky ReLU"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Data\" data-toc-modified-id=\"Data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Data</a></span></li><li><span><a href=\"#Arch\" data-toc-modified-id=\"Arch-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Arch</a></span></li><li><span><a href=\"#Loss\" data-toc-modified-id=\"Loss-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Loss</a></span><ul class=\"toc-item\"><li><span><a href=\"#JH-code\" data-toc-modified-id=\"JH-code-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>JH code</a></span></li></ul></li><li><span><a href=\"#Train\" data-toc-modified-id=\"Train-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Train</a></span></li><li><span><a href=\"#Stepping-Through-a-Batch\" data-toc-modified-id=\"Stepping-Through-a-Batch-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Stepping Through a Batch</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**~ My Code ~**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports & Paths ###\n",
    "from fastai.vision.all import *\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "### Params ###\n",
    "im_sz   = 224\n",
    "bs      = 64\n",
    "val_pct = .2\n",
    "sub_pct = 1\n",
    "path = untar_data(URLs.PASCAL_2007)\n",
    "annos_path = path/'train.json'\n",
    "ims_path = path/'train'\n",
    "\n",
    "\n",
    "### Items ###\n",
    "fns, annos = get_annotations(annos_path)\n",
    "fn2anno = {f:a for f,a in zip(fns,annos)}\n",
    "def get_im(f):   return ims_path/f\n",
    "def get_bbox(f): return fn2anno[f][0]\n",
    "def get_lbl(f):  return fn2anno[f][1]\n",
    "\n",
    "\n",
    "### DataLoaders ###\n",
    "itfms = Resize(im_sz, method='squish')\n",
    "btfms = setup_aug_tfms([Rotate(), Brightness(), Contrast(), Flip(),\n",
    "                       Normalize.from_stats(*imagenet_stats)])\n",
    "db = DataBlock(\n",
    "    blocks=[ImageBlock, BBoxBlock, BBoxLblBlock(add_na=False)],\n",
    "    get_x=get_im, get_y=[get_bbox, get_lbl], n_inp=1,\n",
    "    splitter=RandomSplitter(val_pct),\n",
    "    item_tfms=itfms, batch_tfms=btfms)\n",
    "subset = L(fns).shuffle()[0:int(len(fns)*sub_pct)]\n",
    "dls = db.dataloaders(subset, bs=bs)\n",
    "dls.v = dls.vocab\n",
    "dls.ncls = len(dls.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab: (#20) ['aeroplane','bicycle','bird','boat','bottle','bus','car','cat','chair','cow'...]\n",
      "Size of train data: 2001\n",
      "Size of valid data: 500\n",
      "batch[0]: \t torch.float32 \t torch.Size([64, 3, 224, 224])\n",
      "batch[1]: \t torch.float32 \t torch.Size([64, 13, 4])\n",
      "batch[2]: \t torch.int64 \t torch.Size([64, 13])\n"
     ]
    }
   ],
   "source": [
    "### Inspection ###\n",
    "print(\"Vocab:\", dls.v)\n",
    "print(\"Size of train data:\",len(dls.train.items))\n",
    "print(\"Size of valid data:\",len(dls.valid.items))\n",
    "for i,t in enumerate(dls.one_batch()):\n",
    "    print(f\"batch[{i}]:\",'\\t',t.dtype,'\\t',t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation of tensor shapes:\n",
    "- torch.Size([128, 3, 224, 224]): bs, channels (rgb), im_sz, im_sz\n",
    "- torch.Size([128, 20, 4]): bs, max objs for a single im in batch, bb coords\n",
    "- torch.Size([128, 20]): bs, max objs for a single im in batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Anchors ###\n",
    "def create_anchors(subdivs, zooms, ratios, device='cuda'):\n",
    "    # create list of permutations per default anchor box\n",
    "    perms = [(z*r1,z*r2) for z in zooms for (r1,r2) in ratios]\n",
    "    k = len(perms)\n",
    "    offsets = [1/(sd*2) for sd in subdivs]\n",
    "    xs = np.concatenate([np.tile(  np.linspace(o,1-o,sd),sd) for o,sd in zip(offsets,subdivs)])\n",
    "    ys = np.concatenate([np.repeat(np.linspace(o,1-o,sd),sd) for o,sd in zip(offsets,subdivs)])\n",
    "    ctrs = np.repeat(np.stack([xs,ys], axis=1), k, axis=0)\n",
    "    hws = np.concatenate([np.array([[o/sd,p/sd] for i in range(sd*sd) for o,p in perms]) for sd in subdivs])\n",
    "    box_sizes = tensor(np.concatenate([np.array([1/sd for i in range(sd*sd) for o,p in perms])\n",
    "                                      for sd in subdivs]), requires_grad=False).unsqueeze(1)\n",
    "    anchors = tensor(np.concatenate([ctrs, hws], axis=1), requires_grad=False).float()\n",
    "    return anchors.to(device), box_sizes.to(device)\n",
    "\n",
    "def create_anchor_boxes(ctr, hw):\n",
    "    return 2*torch.cat((tensor(ctr-hw/2), tensor(ctr+hw/2)), axis=1)-1\n",
    "\n",
    "device = 'cpu'\n",
    "subdivs = [4, 2, 1]\n",
    "zooms   = [0.75, 1.0, 1.3]\n",
    "ratios  = [(1.,1.), (1.,.5), (.5,1)]\n",
    "k = len(zooms) * len(ratios)\n",
    "anchors, box_size = create_anchors(subdivs, zooms, ratios, device)\n",
    "anchor_boxes = create_anchor_boxes(anchors[:,:2], anchors[:,2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([189, 4]), tensor([0.8750, 0.6250, 0.1875, 0.0938]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchors.shape, anchors[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Architecture ###\n",
    "def flatten_conv(x,k):\n",
    "    bs,nf,gx,gy = x.size()\n",
    "    return x.permute(0,2,3,1).contiguous().view(bs,-1,nf//k)\n",
    "\n",
    "class StdConv(Module):\n",
    "    \"\"\"Wraps together the standard conv2d→ batchnorm→ dropout.\"\"\"\n",
    "    def __init__(self, nin, nout, stride=2, drop=0.1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(nin, nout, 3, stride=stride, padding=1)\n",
    "        self.bn = nn.BatchNorm2d(nout)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.drop(self.bn(F.relu(self.conv(x))))\n",
    "\n",
    "class OutConv(Module):\n",
    "    \"\"\"Outputs two sets of acts: one for bbs, one for lbls.\"\"\"\n",
    "    def __init__(self, ncls, k, nin, bias):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "        self.bb_acts  = nn.Conv2d(nin,        4*k, 3, padding=1) # bbs\n",
    "        self.lbl_acts = nn.Conv2d(nin, (ncls+1)*k, 3, padding=1) # lbls\n",
    "        self.lbl_acts.bias.data.zero_().add_(bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return [flatten_conv(self.bb_acts(x),  self.k), # bbs,lbls\n",
    "                flatten_conv(self.lbl_acts(x), self.k)]         \n",
    "\n",
    "class SSDHead(Module):\n",
    "    \"\"\"Wraps StdConv and OutConv into a head module.\n",
    "       Defaults to resnet34 backbone.\"\"\"\n",
    "    def __init__(self, ncls, k, bias, drop, body='resnet34'):\n",
    "        super().__init__()\n",
    "        test(body, ['resnet34','resnet50'], operator.in_)\n",
    "        self.body  = body\n",
    "        self.drop  = nn.Dropout(drop)\n",
    "        self.re_sz = StdConv(2048, 512, stride=1)\n",
    "        self.conv0 = StdConv( 512, 256, drop=drop)\n",
    "        self.conv1 = StdConv( 256, 256, drop=drop)\n",
    "        self.conv2 = StdConv( 256, 256, drop=drop)\n",
    "        self.out0  = OutConv(ncls, k, 256, bias)\n",
    "        self.out1  = OutConv(ncls, k, 256, bias)\n",
    "        self.out2  = OutConv(ncls, k, 256, bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.body == 'resnet34': x = F.relu(x)\n",
    "        x = self.drop(x)\n",
    "        if self.body == 'resnet50': x = self.re_sz(x)\n",
    "        x = self.conv0(x)\n",
    "        bb0,lbl0 = self.out0(x)\n",
    "        x = self.conv1(x)\n",
    "        bb1,lbl1 = self.out1(x)\n",
    "        x = self.conv2(x)\n",
    "        bb2,lbl2 = self.out2(x)\n",
    "        return [torch.cat([ bb0, bb1, bb2], dim=1),\n",
    "                torch.cat([lbl0,lbl1,lbl2], dim=1)]\n",
    "\n",
    "class CustMod(Module):\n",
    "    \"\"\"A module made from a pretrained body and an untrained head.\"\"\"\n",
    "    def __init__(self, body, head):\n",
    "        self.body, self.head = body, head\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.head(self.body(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FocalLoss ###\n",
    "def one_hot_embedding(lbls, ncls, device='cuda'):\n",
    "    return torch.eye(ncls)[lbls.data].to(device)\n",
    "\n",
    "class BCELoss(nn.Module):\n",
    "    def __init__(self, ncls, device='cuda'):\n",
    "        super().__init__()\n",
    "        self.ncls = ncls\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, acts, targs):\n",
    "        t = one_hot_embedding(targs, self.ncls+1, self.device)\n",
    "        t = tensor(t[:,:-1].contiguous())\n",
    "        a = acts[:,:-1]\n",
    "        w = self.get_weight(a,t).detach()\n",
    "        return F.binary_cross_entropy_with_logits(a,t,w,reduction='sum')/self.ncls\n",
    "    \n",
    "    def get_weight(self,a,t): return None \n",
    "    \n",
    "class FocalLoss(BCELoss):\n",
    "    def get_weight(self, a, t):\n",
    "        alpha, gamma = 0.25, 2.0 # vals from paper\n",
    "        p = a.sigmoid()\n",
    "        pt = p*t + (1-p)*(1-t)\n",
    "        w = alpha*t + (1-alpha)*(1-t)\n",
    "        return w * (1-pt).pow(gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IoU ###\n",
    "def intersxn(b1,b2):\n",
    "    x1 = torch.max((b1)[:,None,0], (b2)[None,:,0])\n",
    "    y1 = torch.max((b1)[:,None,1], (b2)[None,:,1])\n",
    "    x2 = torch.min((b1)[:,None,2], (b2)[None,:,2])\n",
    "    y2 = torch.min((b1)[:,None,3], (b2)[None,:,3])\n",
    "    return torch.clamp((x2-x1), min=0) * torch.clamp((y2-y1), min=0)\n",
    "\n",
    "def area(b):\n",
    "    return (b[:,2]-b[:,0]) * (b[:,3]-b[:,1])\n",
    "\n",
    "def get_iou(b1, b2):\n",
    "    inter = intersxn(b1,b2)\n",
    "    union = area(b1).unsqueeze(1) + area(b2).unsqueeze(0) - inter\n",
    "    return inter / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ssd_loss ###\n",
    "def remove_padding(bb, lbl):\n",
    "    bb = bb.view(-1,4)\n",
    "    padding = (bb[:,2]-bb[:,0])==0\n",
    "    return bb[~padding],lbl[~padding]\n",
    "\n",
    "def get_pred_bbs(act_bb, ancs, anc_sz, device):\n",
    "    # scale acts between -1 and 1 w/ tanh\n",
    "    acts = torch.tanh(act_bb)\n",
    "    # move ctrs by up to box_size/2\n",
    "    ctrs = ancs.to(device)[:,:2] + (acts.to(device)[:,:2]/2 * anc_sz.to(device))\n",
    "    # adjust hw up to 1/2\n",
    "    hws  = ancs.to(device)[:,2:] * (acts.to(device)[:,2:]/2+1)        \n",
    "    return create_anchor_boxes(ctrs, hws)\n",
    "\n",
    "def map_to_gt(ious):\n",
    "    max_iou_per_bb, anc_idxs = ious.max(1)\n",
    "    max_iou_per_anc, bb_idxs = ious.max(0)\n",
    "    max_iou_per_anc[anc_idxs] = 1.99\n",
    "    for i,iou in enumerate(anc_idxs): bb_idxs[iou] = i\n",
    "    return max_iou_per_anc, bb_idxs\n",
    "\n",
    "def ssd_item_loss(act_bbs, act_lbls, bbs, lbls, device='cuda'):\n",
    "    \"\"\"SSD item loss takes single items from a minibatch, creates hundreds of preds, maps gt\n",
    "       to the preds, prunes the preds, then calcs & returns the bb and lbl loss for that item.\"\"\"\n",
    "# prep\n",
    "    bbs,lbls = remove_padding(bbs,lbls)                      # remove gt padding inserted during training\n",
    "    pred_bbs = get_pred_bbs(act_bbs,anchors,box_size,device) # make 196 pred bbs from acts and ancs\n",
    "    # map gt to preds\n",
    "    iou_gt_grid = get_iou(bbs.data, anchor_boxes.data)       # get iou(gt_bbs,anc_bbs); used to map gt → ancs\n",
    "    iou_gt_preds, mapped_gt_idx = map_to_gt(iou_gt_grid)     # assign each pred an index of a gt object\n",
    "    mapped_bbs  = bbs[mapped_gt_idx]                         # project gt bbs into pred space\n",
    "    mapped_lbls = lbls[mapped_gt_idx]                        # project gt lbls into pred space\n",
    "    # remove low-iou bb preds & set mapped lbl to bg\n",
    "    high_iou = iou_gt_preds > 0.4                            # only include bb preds that overlap w/a gt obj and\n",
    "    incl = torch.nonzero(high_iou)[:,0]                      #  are not predicting background\n",
    "    mapped_lbls[~high_iou] = dls.ncls                        # assign gt class of bg to preds w/ low max gt iou\n",
    "    # compute loss\n",
    "    bb_res  = F.l1_loss(pred_bbs[incl], mapped_bbs[incl])\n",
    "    lbl_res = loss_f(act_lbls, mapped_lbls)\n",
    "    return bb_res, lbl_res\n",
    "\n",
    "def ssd_loss(acts, bbs, lbls, device='cuda', print_it=True):\n",
    "    bb_sum, lbl_sum = 0., 0.\n",
    "    for o in zip(*acts, bbs, lbls):\n",
    "        bb_loss, lbl_loss = ssd_item_loss(*o, device)\n",
    "        bb_sum  += bb_loss\n",
    "        lbl_sum += lbl_loss\n",
    "    if print_it: print(f\"bb:{bb_sum:.02f} | lbl: {lbl_sum:.02f}\")\n",
    "    return bb_sum + lbl_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JH code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Anchors ###\n",
    "anc_grids = [4,2,1]\n",
    "# anc_grids = [4]\n",
    "anc_zooms = [0.75, 1., 1.3]\n",
    "# anc_zooms = [1.]\n",
    "anc_ratios = [(1.,1.), (1.,0.5), (0.5,1.)]\n",
    "# anc_ratios = [(1.,1.)]\n",
    "anchor_scales = [(anz*i,anz*j) for anz in anc_zooms for (i,j) in anc_ratios]\n",
    "k = len(anchor_scales)\n",
    "anc_offsets = [1/(o*2) for o in anc_grids]\n",
    "anc_x = np.concatenate([np.tile(np.linspace(ao, 1-ao, ag), ag)\n",
    "                        for ao,ag in zip(anc_offsets,anc_grids)])\n",
    "anc_y = np.concatenate([np.repeat(np.linspace(ao, 1-ao, ag), ag)\n",
    "                        for ao,ag in zip(anc_offsets,anc_grids)])\n",
    "anc_ctrs = np.repeat(np.stack([anc_x,anc_y], axis=1), k, axis=0)\n",
    "anc_sizes  =   np.concatenate([np.array([[o/ag,p/ag] for i in range(ag*ag) for o,p in anchor_scales])\n",
    "               for ag in anc_grids])\n",
    "grid_sizes = tensor(np.concatenate([np.array([ 1/ag       for i in range(ag*ag) for o,p in anchor_scales])\n",
    "               for ag in anc_grids]), requires_grad=False).unsqueeze(1)\n",
    "anchors = tensor(np.concatenate([anc_ctrs, anc_sizes], axis=1), requires_grad=False).float()\n",
    "\n",
    "def hw2corners(ctr, hw): return torch.cat([ctr-hw/2,ctr+hw/2], dim=1)\n",
    "anchor_cnr = hw2corners(anchors[:,:2], anchors[:,2:])\n",
    "\n",
    "### Architecture ###\n",
    "class StdConv(nn.Module):\n",
    "    def __init__(self, n_in,n_out,stride=2,dp = 0.1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(n_in,n_out,3,stride=stride,padding=1)\n",
    "        self.bn = nn.BatchNorm2d(n_out)\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.dropout(self.bn(F.relu(self.conv(x))))\n",
    "    \n",
    "def flatten_conv(x,k):\n",
    "    bs,nf,gx,gy = x.size()\n",
    "    x = x.permute(0,2,3,1).contiguous()\n",
    "    return x.view(bs,-1,nf//k)\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, k, n_in, bias):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "        self.bbs  = nn.Conv2d(n_in,            4*k, 3, padding=1)\n",
    "        self.lbls = nn.Conv2d(n_in, (dls.ncls+1)*k, 3, padding=1)\n",
    "        self.lbls.bias.data.zero_().add_(bias)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return [flatten_conv(self.bbs(x),  self.k),\n",
    "                flatten_conv(self.lbls(x), self.k)]\n",
    "    \n",
    "drop=0.4\n",
    "class SSD_MultiHead(nn.Module):\n",
    "    def __init__(self, k, bias):\n",
    "        super().__init__()\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        self.sconv1 = StdConv(512,256, dp=drop)\n",
    "        self.sconv2 = StdConv(256,256, dp=drop)\n",
    "        self.sconv3 = StdConv(256,256, dp=drop)\n",
    "        self.out1 = OutConv(k, 256, bias)\n",
    "        self.out2 = OutConv(k, 256, bias)\n",
    "        self.out3 = OutConv(k, 256, bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.drop(F.relu(x))\n",
    "        x = self.sconv1(x)\n",
    "        bbs1,lbls1 = self.out1(x)\n",
    "        x = self.sconv2(x)\n",
    "        bbs2,lbls2 = self.out2(x)\n",
    "        x = self.sconv3(x)\n",
    "        bbs3,lbls3 = self.out3(x)\n",
    "        return [torch.cat([ bbs1, bbs2, bbs3], dim=1),\n",
    "                torch.cat([lbls1,lbls2,lbls3], dim=1)]\n",
    "    \n",
    "class CustMod(Module):\n",
    "    \"\"\"A module made from a pretrained body and an untrained head.\"\"\"\n",
    "    def __init__(self, body, head):\n",
    "        self.body, self.head = body, head\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.head(self.body(x))\n",
    "\n",
    "### FocalLoss ###\n",
    "def one_hot_embedding(labels, num_classes):\n",
    "    return torch.eye(num_classes)[labels.data].to('cuda')\n",
    "\n",
    "class BCE_Loss(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "    \n",
    "    def forward(self, preds, targets):\n",
    "        t = one_hot_embedding(targets, self.num_classes+1)\n",
    "        t = tensor(t[:,:-1].contiguous())\n",
    "        x = preds[:,:-1]\n",
    "        w = self.get_weight(x,t).detach()\n",
    "        return F.binary_cross_entropy_with_logits(x, t, w, reduction='sum') / self.num_classes\n",
    "    \n",
    "    def get_weight(self,x,t):\n",
    "        return None\n",
    "\n",
    "class FocalLoss(BCE_Loss):\n",
    "    def get_weight(self,x,t):\n",
    "        alpha,gamma = 0.25,2.\n",
    "        p = x.sigmoid()\n",
    "        pt = p*t + (1-p)*(1-t)\n",
    "        w = alpha*t + (1-alpha)*(1-t)\n",
    "        return w * (1-pt).pow(gamma)\n",
    "\n",
    "loss_f = FocalLoss(dls.ncls)\n",
    "\n",
    "### IoU ###\n",
    "def intersection(box_a,box_b):\n",
    "    min_xy = torch.max(box_a[:,None,:2],box_b[None,:,:2])\n",
    "    max_xy = torch.min(box_a[:,None,2:],box_b[None,:,2:])\n",
    "    inter = torch.clamp(max_xy-min_xy,min=0)\n",
    "    return inter[:,:,0] * inter[:,:,1]\n",
    "\n",
    "def get_size(box):\n",
    "    return (box[:,2]-box[:,0]) * (box[:,3] - box[:,1])\n",
    "\n",
    "def jaccard(box_a,box_b):\n",
    "    inter = intersection(box_a,box_b)\n",
    "    union = get_size(box_a).unsqueeze(1) + get_size(box_b).unsqueeze(0) - inter\n",
    "    return inter/union\n",
    "\n",
    "### ssd_loss ###\n",
    "def get_y(bbox,clas):\n",
    "    bbox = bbox.view(-1,4)/size\n",
    "    bb_keep = ((bbox[:,2] - bbox[:,0])>0.).nonzero()[:,0]\n",
    "    return bbox[bb_keep], clas[bb_keep]\n",
    "    \n",
    "def actn_to_bb(actn, anchors):\n",
    "    actn_bbs = torch.tanh(actn)\n",
    "    actn_ctrs = (actn_bbs.cuda()[:,:2] * grid_sizes.cuda()/2) + anchors.cuda()[:,:2]\n",
    "    actn_hw = (1 + actn_bbs.cuda()[:,2:]/2) * anchors.cuda()[:,2:]\n",
    "    return hw2corners(actn_ctrs,actn_hw)\n",
    "\n",
    "def map_to_ground_truth(overlaps, print_it=False):\n",
    "    prior_overlap, prior_idx = overlaps.max(1)\n",
    "    if print_it: print(prior_overlap)\n",
    "    gt_overlap, gt_idx = overlaps.max(0)\n",
    "    gt_overlap[prior_idx] = 1.99\n",
    "    for i,o in enumerate(prior_idx): gt_idx[o] = i\n",
    "    return gt_overlap,gt_idx\n",
    "\n",
    "def ssd_1_loss(b_bb, b_c, bbox, clas, print_it=False, use_ab=True):\n",
    "    bbox,clas = get_y(bbox,clas)\n",
    "    a_ic = actn_to_bb(b_bb, anchors)\n",
    "    overlaps = jaccard(bbox.data, (anchor_cnr.cuda() if use_ab else a_ic).data)\n",
    "    gt_overlap,gt_idx = map_to_ground_truth(overlaps)\n",
    "    gt_clas = clas[gt_idx]\n",
    "    pos = gt_overlap > 0.4\n",
    "    pos_idx = torch.nonzero(pos)[:,0]\n",
    "    gt_clas[~pos] = dls.ncls\n",
    "    gt_bbox = bbox[gt_idx]\n",
    "    loc_loss = ((a_ic[pos_idx] - gt_bbox[pos_idx]).abs()).mean()\n",
    "    clas_loss  = loss_f(b_c, gt_clas)\n",
    "    return loc_loss, clas_loss\n",
    "\n",
    "def ssd_loss(pred, targ_bb, targ_lbl, print_it=True):\n",
    "    bb_sum,lbl_sum = 0.,0.\n",
    "    for pred_bb,pred_lbl,bbox,clas in zip(*pred,targ_bb,targ_lbl):\n",
    "        bb_loss,lbl_loss = ssd_1_loss(pred_bb,pred_lbl,bbox,clas,print_it)\n",
    "        bb_sum += bb_loss\n",
    "        lbl_sum += lbl_loss\n",
    "    if print_it: print(f\"bb:{bb_sum:.02f} | lbl: {lbl_sum:.02f}\")\n",
    "    return bb_sum + lbl_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Batch n Acts ###\n",
    "# Get batch, acts\n",
    "head_reg4_JH = SSD_MultiHead(k, -4.)\n",
    "mod_JH = CustMod(create_body(resnet34, pretrained=True), head_reg4_JH)\n",
    "mod_JH.cpu().eval()\n",
    "\n",
    "batch_JH = next(iter(dls.cpu().valid))\n",
    "acts_JH = mod(batch_JH[0])\n",
    "batch_bbs_JH, batch_lbls_JH = batch_JH[1], batch_JH[2]\n",
    "\n",
    "# Get acts and targs for a single im\n",
    "#bidx = ... already set\n",
    "act_bbs_JH,act_lbls_JH,bbs_JH,lbls_JH = list(zip(*acts_JH,batch_bbs_JH,batch_lbls_JH))[bidx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Not Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get_preds ###\n",
    "def nms(boxes, scores, iou_thresh, top_k=100):\n",
    "    keep = scores.new(scores.size(0)).zero_().long()\n",
    "    if boxes.numel() == 0: return keep\n",
    "    x1 = boxes[:, 0]\n",
    "    y1 = boxes[:, 1]\n",
    "    x2 = boxes[:, 2]\n",
    "    y2 = boxes[:, 3]\n",
    "    area = torch.mul(x2 - x1, y2 - y1)\n",
    "    v, idx = scores.sort(0)  # sort asc\n",
    "    idx = idx[-top_k:]       # indices of k largest vals\n",
    "    xx1 = boxes.new()\n",
    "    yy1 = boxes.new()\n",
    "    xx2 = boxes.new()\n",
    "    yy2 = boxes.new()\n",
    "    w = boxes.new()\n",
    "    h = boxes.new()\n",
    "\n",
    "    count = 0\n",
    "    while idx.numel() > 0:\n",
    "        i = idx[-1]  # index of current largest val\n",
    "        keep[count] = i\n",
    "        count += 1\n",
    "        if idx.size(0) == 1: break\n",
    "        idx = idx[:-1]  # remove kept element from view\n",
    "        # load bboxes of next highest vals\n",
    "        torch.index_select(x1, 0, idx, out=xx1)\n",
    "        torch.index_select(y1, 0, idx, out=yy1)\n",
    "        torch.index_select(x2, 0, idx, out=xx2)\n",
    "        torch.index_select(y2, 0, idx, out=yy2)\n",
    "        # store element-wise max with next highest score\n",
    "        xx1 = torch.clamp(xx1, min=x1[i])\n",
    "        yy1 = torch.clamp(yy1, min=y1[i])\n",
    "        xx2 = torch.clamp(xx2, max=x2[i])\n",
    "        yy2 = torch.clamp(yy2, max=y2[i])\n",
    "        w.resize_as_(xx2)\n",
    "        h.resize_as_(yy2)\n",
    "        w = xx2 - xx1\n",
    "        h = yy2 - yy1\n",
    "        # check sizes of xx1 and xx2.. after each iteration\n",
    "        w = torch.clamp(w, min=0.0)\n",
    "        h = torch.clamp(h, min=0.0)\n",
    "        inter = w*h\n",
    "        # IoU = i / (area(a) + area(b) - i)\n",
    "        rem_areas = torch.index_select(area, 0, idx)  # load remaining areas)\n",
    "        union = (rem_areas - inter) + area[i]\n",
    "        IoU = inter/union  # store result in iou\n",
    "        # keep only elements with an IoU <= iou_thresh\n",
    "        idx = idx[IoU.le(iou_thresh)]\n",
    "    return keep, count\n",
    "\n",
    "def acts_to_preds(abb, albl, ancs, anc_sz, iou_thresh, conf_thresh, device):\n",
    "    \"\"\"Turn model acts into preds: abbs use get_pred_bbs, and albls use sigmoid().max().\n",
    "       Used in ResultShower and mAP (and could possibly be used in loss fxn).\"\"\"\n",
    "     # convert acts to preds\n",
    "    pbb = get_pred_bbs(abb, ancs, anc_sz, device)\n",
    "    conf, plbl = albl.sigmoid().max(1)\n",
    "    # filter out preds w/ nms\n",
    "    nms_idxs, nms_n = nms(pbb.data, conf, iou_thresh)\n",
    "    nms_idxs = nms_idxs[:nms_n]\n",
    "    pbb  = pbb[nms_idxs]\n",
    "    plbl = plbl[nms_idxs]\n",
    "    conf = conf[nms_idxs]\n",
    "    # filter out bg and low-conf preds\n",
    "    is_not_bg = (plbl!=0)\n",
    "    is_confident = conf > conf_thresh\n",
    "    mask = is_not_bg & is_confident\n",
    "    return pbb[mask], plbl[mask], conf[mask]\n",
    "\n",
    "def get_batch_preds(abb, albl, ancs, anc_sz, iou_thresh=.5, conf_thresh=.25, device='cpu'):\n",
    "    \"\"\"Loop through a batch and of activations and turn them into predictions.\"\"\"\n",
    "    ancs.to(device); anc_sz.to(device)\n",
    "    pbbs, plbls, confs = [], [], []\n",
    "    for abb, albl in zip(abb, albl):\n",
    "        pbb, plbl, conf = acts_to_preds(abb,albl,ancs,anc_sz,iou_thresh,conf_thresh,device)\n",
    "        pbbs  += [pbb]\n",
    "        plbls += [plbl]\n",
    "        confs += [conf]\n",
    "    return pbbs, plbls, confs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Metric ###\n",
    "def _format_inps(acts, batch, anchors, box_size, iou_thresh, conf_thresh, device):\n",
    "    \"\"\"Format acts and targs for AP score calc. Input expects learner.acts & learner.batch,\n",
    "       output format: (im_idx, pred_bbs, pred_cls, cls_conf) and (im_idx, bbs, cls).\n",
    "       Ex: (46.0, tensor([0.1, 0.2, 0.9, 0.9]), tensor(3), tensor(0.78))\"\"\"\n",
    "    preds = get_batch_preds(*acts, anchors, box_size, iou_thresh, conf_thresh, device)\n",
    "    p_idxs = torch.cat([torch.tensor([i]*len(o)) for i,o in enumerate(preds[0])]).numpy().tolist()\n",
    "    gt_idxs = torch.cat([torch.tensor([i]*len(o)) for i,o in enumerate(batch[1])]).numpy().tolist()\n",
    "    batch_preds = list(zip(p_idxs, *[torch.cat(o) for o in preds]))\n",
    "    batch_gts = list(zip(gt_idxs, *[o.flatten(end_dim=1) for o in batch[1:]]))\n",
    "    return batch_preds, batch_gts\n",
    "\n",
    "def _flatten_list(l, ret_L=False):\n",
    "    \"\"\"Flatten a list-of-lists; lists can be python `list`s or a fastai `L`s.\"\"\"\n",
    "    def _recur(l,res):\n",
    "        for o in l:\n",
    "            if   isinstance(o,list): _recur(o,res)\n",
    "            elif isinstance(o,L)   : _recur(o,res)\n",
    "            else: res.append(o)\n",
    "        return res\n",
    "    res = _recur(l, [])\n",
    "    return res if not ret_L else L(res)\n",
    "\n",
    "def _get_tp_bbs(preds_tp):\n",
    "    \"\"\"Output list of tp bbs per im in batch. Not used to calculate mAP; only\n",
    "       used to grab true positive bb preds for visualizing in ResultShower.\"\"\"\n",
    "    # Each row in preds_tps is a formatted pred (see format_ap_inputs) and a list of\n",
    "    # 1s and 0s (signifying tps and fps) for a cls. No preds for a cls → empty lists.\n",
    "    batch_idxs, pred_bbs, tpfps = [], [], []\n",
    "    for preds,tp in preds_tp:\n",
    "        batch_idxs.append([o[0] for o in preds])\n",
    "        pred_bbs.append([o[1] for o in preds])\n",
    "        tpfps.append(tp)\n",
    "    flat_idxs  = _flatten_list(batch_idxs)\n",
    "    flat_bbs   = _flatten_list(pred_bbs)\n",
    "    flat_tpfps = torch.cat(tpfps)\n",
    "\n",
    "    scored_preds = list(zip(flat_idxs, flat_bbs, flat_tpfps))\n",
    "    true_bbs = [(int(o[0]), o[1]) for o in scored_preds if o[2]==True]\n",
    "\n",
    "    true_preds = [torch.zeros(4).view(1,4) for i in range(0,bs)]\n",
    "    for i,bb in true_bbs:\n",
    "        if true_preds[i].sum()==0: true_preds[i] = bb.view(1,4)\n",
    "        else: true_preds[i] = torch.cat([true_preds[int(i)], bb.view(1,4)], dim=0)\n",
    "    return true_preds\n",
    "\n",
    "def ap_per_cls(acts, batch, n_cls, ancs, anc_sz, iou_thresh, conf_thresh, device='cuda'):\n",
    "    \"\"\"Calculate AP score per class. Returns avg AP over all classes.\"\"\"\n",
    "    batch_preds, batch_gts = _format_inps(acts,batch,ancs,anc_sz,iou_thresh,conf_thresh,device)\n",
    "    # avg_precs holds ap per cls; other accumulators only used for vizing results\n",
    "    avg_precs, preds_tp, n_objs_accum = [],[],[] \n",
    "    for c in range(1,n_cls): # start at 1 to ignore gt\n",
    "        # store preds and gts for current cls\n",
    "        preds = [b for b in batch_preds if b[2]==c]\n",
    "        gts   = [b for b in batch_gts   if b[2]==c]\n",
    "                \n",
    "        # sort preds by conf desc\n",
    "        preds.sort(key=lambda x: x[3], reverse=True)\n",
    "        \n",
    "        # make dict of im_idx:zeros(n_objs)\n",
    "        n_objs = Counter([gt[0] for gt in gts])\n",
    "        for k,v in n_objs.items(): n_objs[k] = torch.zeros(v)\n",
    "        \n",
    "        # init tp: a bool tensor for each im s.t. 1s indicate a pred is a tp\n",
    "        tp = torch.zeros((len(preds))).bool()\n",
    "        total_gt_objs = len(gts)\n",
    "        \n",
    "        for pred_idx, pred in enumerate(preds):\n",
    "            gt_objs = [o for o in gts if o[0] == pred[0]]\n",
    "            n_gt_objs = len(gt_objs)\n",
    "            max_iou = 0\n",
    "            \n",
    "            for idx, gt in enumerate(gt_objs):\n",
    "                iou = get_iou(pred[1].view(1,4), gt[1].view(1,4))\n",
    "                if iou > max_iou: max_iou, idx_of_max = iou, idx\n",
    "                    \n",
    "            # update idx of gt_obj to indicate it's been used\n",
    "            if max_iou > iou_thresh:\n",
    "                if n_objs[pred[0]][idx_of_max]==0:\n",
    "                    tp[pred_idx] = 1\n",
    "                    n_objs[pred[0]][idx_of_max] = 1\n",
    "                    \n",
    "        # store tp_bbs: use tp ask mask on preds and take 1th item (the bb) from each\n",
    "        preds_tp.append(([preds,tp]))\n",
    "        n_objs_accum.append(n_objs)\n",
    "        \n",
    "        # calc avg_prec and store\n",
    "        tps = torch.cumsum(tp, dim=0)               # 1. tp csum: [0,1,1,0,0] → [0,1,2,2,2]\n",
    "        fps = torch.cumsum(~tp, dim=0)              # (basically same steps for fp)\n",
    "        prec = torch.div(tps, (tps + fps + 1e-6))   # 2. divide each tps item by n_preds\n",
    "        prec = torch.cat((torch.tensor([1]), prec)) # 3. slap on a 1 at the beginning\n",
    "        rec = tps / (total_gt_objs + 1e-6)\n",
    "        rec = torch.cat((torch.tensor([0]), rec))\n",
    "        avg_prec = torch.trapz(prec, rec) # calc AP w/ trap rule\n",
    "        avg_precs.append(avg_prec)        # store AP of this cls in accum\n",
    "    return avg_precs, _get_tp_bbs(preds_tp), n_objs_accum\n",
    "\n",
    "def get_ap_scores(dls, model, ancs, anc_sz, iou, conf, device='cpu'):\n",
    "    for o in [dls, model, anchors, box_size]: o.to(device)\n",
    "    model.eval()\n",
    "    n_cls = len(dls.vocab)\n",
    "\n",
    "    res=[]\n",
    "    for b in dls.valid:\n",
    "        scores,_,_ = ap_per_cls(model(b[0]),b,n_cls,ancs,anc_sz,iou,conf,device)\n",
    "        res.append(scores)\n",
    "    ap_scores = torch.stack([tensor(o) for o in res]).sum(axis=0)/len(res)\n",
    "    ap_scores = ap_scores.numpy().tolist()\n",
    "    return sum(ap_scores)/len(ap_scores), ap_scores\n",
    "\n",
    "class MeanAveragePrecision(Metric):\n",
    "    def __init__(self, func, n_cls):\n",
    "        self.func,self.n_cls = func,n_cls\n",
    "    def reset(self):\n",
    "        self.res = []\n",
    "    def accumulate(self, learn):\n",
    "        is_last_epoch = learn.epoch==learn.n_epoch-1\n",
    "        if is_last_epoch:\n",
    "            cls_aps,_,_ = self.func(learn.pred,(*learn.xb,*learn.yb),self.n_cls,anchors,box_size)\n",
    "            self.res.append(cls_aps)\n",
    "    @property\n",
    "    def value(self):\n",
    "        if self.res==[]:\n",
    "            return 0\n",
    "        else:\n",
    "            ap_scores = torch.stack([tensor(o) for o in self.res]).sum(axis=0)/len(self.res)\n",
    "            return sum(ap_scores)/len(ap_scores)\n",
    "    @property\n",
    "    def name(self):\n",
    "        return \"mAP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Viz Results ###\n",
    "def show_bb(im, bb=None, lbl=[''], title=None, color='white',\n",
    "            ctx=None, sz=im_sz, figsize=5):\n",
    "    # process empties and nones\n",
    "    if bb.shape[-1]==0 or bb==None: bb  = tensor([[0.,0,0,0]])\n",
    "    if lbl==['']:                   lbl = ['']*bb.shape[0]\n",
    "        \n",
    "    # process tensors to take advantage of fastai show methods\n",
    "    bbox = TensorBBox((bb+1)*sz//2)\n",
    "    labeledbbox = LabeledBBox(bbox,lbl)\n",
    "    \n",
    "    if ctx:     show_image(im, figsize=[figsize,figsize], title=title, ctx=ctx)\n",
    "    else: ctx = show_image(im, figsize=[figsize,figsize], title=title)\n",
    "    \n",
    "    labeledbbox.show(ctx=ctx)       # first, draw white lbl bbs...\n",
    "    bbox.show(ctx=ctx, color=color) # ... then overlay color bbs.\n",
    "    return ctx\n",
    "\n",
    "def get_im_tpfpfns(res):\n",
    "    \"\"\"QnD function to output tpfpfns for each im in a batch given a ResultShower.\"\"\"\n",
    "    # TPs are found by counting \n",
    "    tps = []\n",
    "    for bbs in res.tp_bbs:\n",
    "        if bbs.sum()==0: tps.append(0)\n",
    "        else: tps.append(bbs.shape[0])\n",
    "    # FPs are found by subtracing tps from preds_per_im\n",
    "    preds_per_im = [o.shape[0] for o in res.preds[0]]\n",
    "    fps = [pred-tp for pred,tp in zip(preds_per_im, tps)]\n",
    "    # FNs (relies on res.nobj which is a hack)\n",
    "    # res.nobj is a dict where each key is a cls and the values are some combo of\n",
    "    #  im_idx and a bool list st. the len of the list is the number of gt_objs for\n",
    "    #  that class in that im, and the truth value of the bool represents whether\n",
    "    #  the obj is a TP or FN.\n",
    "    im_dict = defaultdict(lambda: [])\n",
    "    for cls in res.nobj:\n",
    "        for im_idx,tp_tensor in cls.items():\n",
    "            im_dict[im_idx] += ~tp_tensor.bool()\n",
    "    idx_fns = [(k,sum(v).item()) for k,v in im_dict.items()]\n",
    "    idx_fns.sort()\n",
    "    fns = [o[1] for o in idx_fns]\n",
    "    return tps, fps, fns\n",
    "\n",
    "class ResultShower():\n",
    "    def __init__(self, dls, lrn, ancs, anc_sz, iou, conf):\n",
    "        # store init's args\n",
    "        self.dls    = dls\n",
    "        self.mod    = lrn.model.eval().cpu()\n",
    "        self.ancs   = ancs.cpu()\n",
    "        self.anc_sz = anc_sz.cpu()\n",
    "        self.iou    = iou\n",
    "        self.conf   = conf\n",
    "        # compute attrs\n",
    "        self.batch    = next(iter(self.dls.cpu().valid))\n",
    "        self.acts     = [a.data for a in self.mod(self.batch[0])]\n",
    "        self.preds    = get_batch_preds(*self.acts,self.ancs,self.anc_sz,self.iou,self.conf,'cpu')\n",
    "        self.dec_ims  = self.dls.decode(self.batch)[0]\n",
    "        self.bs       = self.dls.bs\n",
    "        self.voc      = self.dls.vocab\n",
    "        self.im_sz    = self.batch[0].shape[-1]\n",
    "        self.last_res = 0\n",
    "        self.fig_sz   = [8,8]\n",
    "        # compute metrics\n",
    "        aps,tp_bbs,nobj = ap_per_cls(self.acts,self.batch,len(self.voc),\n",
    "                                     self.ancs,self.anc_sz,self.iou,self.conf,'cpu')\n",
    "        self.ap_scores  = [o.item() for o in aps]\n",
    "        self.tp_bbs     = tp_bbs\n",
    "        self.nobj       = nobj\n",
    "        self.im_tpfpfns = get_im_tpfpfns(self)\n",
    "        # clean up\n",
    "        self.dls.cuda(); self.mod.cuda()\n",
    "        \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.show_next(*args, **kwargs)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        # get everything to draw\n",
    "        ims             = self.dec_ims\n",
    "        _,bbs,lbls      = self.batch\n",
    "        pbbs,plbls,conf = self.preds\n",
    "        tbbs            = self.tp_bbs\n",
    "        tps,fps,fns     = self.im_tpfpfns\n",
    "        # titles\n",
    "        t_gt = f\"Idx {i} (nobj={(lbls[i] > 0).sum()})\"\n",
    "        t_p  = f\"TPs:{tps[i]} | FPs:{fps[i]} | FNs:{fns[i]}\"\n",
    "        # two ctx: gts and all preds. lime bbs drawn over TP preds.\n",
    "        ctx = get_grid(2, figsize=self.fig_sz)\n",
    "        show_bb(ims[i],bbs[i], self.voc[lbls[i]], t_gt,'white', ctx[0], self.im_sz)\n",
    "        show_bb(ims[i],pbbs[i],self.voc[plbls[i]],t_p, 'magenta',ctx[1],self.im_sz)\n",
    "        show_bb(ims[i],tbbs[i],color='lime',ctx=ctx[1])\n",
    "                 \n",
    "    def show_next(self, n=1):\n",
    "        for i in range(n): self[(i + self.last_res)%self.bs]\n",
    "        self.last_res += n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lil' helprs\n",
    "def pad_strs(strs):\n",
    "    nchars = [len(s) for s in strs]\n",
    "    maxn = max(nchars)\n",
    "    nspc = [maxn-n for n in nchars]\n",
    "    return [s+' '*n for s,n in zip(strs,nspc)]\n",
    "def print_lists(*lists):\n",
    "    for l in [*lists]: test_eq(len(lists[0]), len(l))        # test lens eq\n",
    "    pstrs = [pad_strs([str(o) for o in l]) for l in lists]   # get list of padding strs\n",
    "    for zpstr in list(zip(*pstrs)): print(' | '.join(zpstr)) # print rows joined with ' | '\n",
    "def batch_info(l):\n",
    "    \"\"\"Print idx, type, shape for items in l (a list of tensors)\"\"\"\n",
    "    idxs = list(range(len(l)))\n",
    "    shapes = apply(lambda t:t.shape, l)\n",
    "    types = apply(type, l)\n",
    "    print_lists(idxs, shapes, types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Init Anchors ###\n",
    "subdivs = [4, 2, 1]\n",
    "zooms   = [0.75, 1.0, 1.3]\n",
    "ratios  = [(1.,1.), (1.,.5), (.5,1)]\n",
    "k = len(zooms) * len(ratios)\n",
    "anchors, box_size = create_anchors(subdivs, zooms, ratios)\n",
    "anchor_boxes = create_anchor_boxes(anchors[:,:2], anchors[:,2:])\n",
    "\n",
    "### Label Loss, Metric ###\n",
    "loss_f = FocalLoss(dls.ncls, 'cuda')\n",
    "# met = MeanAveragePrecision(ap_per_cls, n_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fresh_mod(ncls=dls.ncls, k=k, bias=-4., drop=.4, body='resnet34', device='cuda'):\n",
    "    test(body, ['resnet34','resnet50'], operator.in_)\n",
    "    arch = resnet34 if body=='resnet34' else resnet50\n",
    "    return CustMod(create_body(arch, pretrained=True), SSDHead(ncls, k, bias, drop, body))\n",
    "mod = get_fresh_mod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = Learner(dls, mod, loss_func=ssd_loss).to_fp16()\n",
    "learner.freeze()\n",
    "# lr_min, lr_steep = learner.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.0022\n"
     ]
    }
   ],
   "source": [
    "# lr = (lr_min+lr_steep)/2; print(\"lr:\",round(lr,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>42.545944</td>\n",
       "      <td>83.527405</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>36.724854</td>\n",
       "      <td>36.029850</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>31.668453</td>\n",
       "      <td>22.878479</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>28.094711</td>\n",
       "      <td>22.700785</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>25.523470</td>\n",
       "      <td>20.782619</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bb:16.82 | lbl: 27.95\n",
      "bb:16.85 | lbl: 25.12\n",
      "bb:17.32 | lbl: 32.56\n",
      "bb:17.04 | lbl: 27.40\n",
      "bb:17.56 | lbl: 28.57\n",
      "bb:17.68 | lbl: 27.87\n",
      "bb:16.84 | lbl: 25.60\n",
      "bb:16.79 | lbl: 27.23\n",
      "bb:17.87 | lbl: 27.12\n",
      "bb:17.68 | lbl: 25.02\n",
      "bb:17.72 | lbl: 26.32\n",
      "bb:17.65 | lbl: 27.91\n",
      "bb:17.85 | lbl: 27.26\n",
      "bb:17.64 | lbl: 24.85\n",
      "bb:16.31 | lbl: 25.95\n",
      "bb:17.14 | lbl: 28.21\n",
      "bb:16.83 | lbl: 26.55\n",
      "bb:15.66 | lbl: 26.40\n",
      "bb:16.88 | lbl: 28.57\n",
      "bb:16.10 | lbl: 25.90\n",
      "bb:15.88 | lbl: 27.10\n",
      "bb:16.49 | lbl: 27.01\n",
      "bb:16.42 | lbl: 27.35\n",
      "bb:16.89 | lbl: 23.59\n",
      "bb:15.63 | lbl: 22.65\n",
      "bb:15.49 | lbl: 26.69\n",
      "bb:15.75 | lbl: 25.40\n",
      "bb:14.70 | lbl: 23.87\n",
      "bb:15.03 | lbl: 20.64\n",
      "bb:14.56 | lbl: 21.53\n",
      "bb:14.46 | lbl: 28.38\n",
      "bb:21.36 | lbl: 71.12\n",
      "bb:19.58 | lbl: 69.91\n",
      "bb:20.18 | lbl: 49.46\n",
      "bb:20.70 | lbl: 56.05\n",
      "bb:20.19 | lbl: 61.92\n",
      "bb:22.01 | lbl: 62.38\n",
      "bb:20.82 | lbl: 74.60\n",
      "bb:16.91 | lbl: 59.73\n",
      "bb:13.50 | lbl: 26.17\n",
      "bb:14.23 | lbl: 23.29\n",
      "bb:14.24 | lbl: 22.13\n",
      "bb:13.75 | lbl: 22.82\n",
      "bb:13.11 | lbl: 24.65\n",
      "bb:13.30 | lbl: 22.30\n",
      "bb:12.78 | lbl: 24.11\n",
      "bb:14.57 | lbl: 24.56\n",
      "bb:13.71 | lbl: 22.36\n",
      "bb:12.95 | lbl: 23.14\n",
      "bb:13.05 | lbl: 22.57\n",
      "bb:13.32 | lbl: 18.35\n",
      "bb:13.36 | lbl: 21.60\n",
      "bb:13.49 | lbl: 19.93\n",
      "bb:12.85 | lbl: 21.54\n",
      "bb:11.66 | lbl: 19.97\n",
      "bb:12.59 | lbl: 22.46\n",
      "bb:12.60 | lbl: 22.08\n",
      "bb:12.44 | lbl: 20.54\n",
      "bb:12.14 | lbl: 20.96\n",
      "bb:11.96 | lbl: 22.46\n",
      "bb:12.00 | lbl: 18.67\n",
      "bb:11.75 | lbl: 19.63\n",
      "bb:11.84 | lbl: 22.95\n",
      "bb:12.50 | lbl: 20.91\n",
      "bb:12.06 | lbl: 19.78\n",
      "bb:11.88 | lbl: 17.09\n",
      "bb:11.63 | lbl: 16.73\n",
      "bb:12.08 | lbl: 17.56\n",
      "bb:11.65 | lbl: 19.30\n",
      "bb:12.12 | lbl: 19.60\n",
      "bb:13.49 | lbl: 22.95\n",
      "bb:12.33 | lbl: 23.39\n",
      "bb:12.87 | lbl: 22.52\n",
      "bb:13.55 | lbl: 20.65\n",
      "bb:12.78 | lbl: 26.57\n",
      "bb:11.47 | lbl: 24.17\n",
      "bb:12.49 | lbl: 23.49\n",
      "bb:10.84 | lbl: 24.56\n",
      "bb:12.77 | lbl: 18.10\n",
      "bb:12.32 | lbl: 16.76\n",
      "bb:10.82 | lbl: 19.33\n",
      "bb:11.40 | lbl: 17.42\n",
      "bb:11.52 | lbl: 17.19\n",
      "bb:11.30 | lbl: 17.06\n",
      "bb:12.28 | lbl: 16.81\n",
      "bb:11.14 | lbl: 17.87\n",
      "bb:12.95 | lbl: 16.45\n",
      "bb:11.78 | lbl: 15.59\n",
      "bb:10.94 | lbl: 15.43\n",
      "bb:11.94 | lbl: 15.93\n",
      "bb:10.60 | lbl: 13.51\n",
      "bb:11.23 | lbl: 17.46\n",
      "bb:11.32 | lbl: 16.36\n",
      "bb:10.75 | lbl: 16.72\n",
      "bb:10.78 | lbl: 16.00\n",
      "bb:11.15 | lbl: 17.47\n",
      "bb:10.10 | lbl: 15.90\n",
      "bb:11.87 | lbl: 16.57\n",
      "bb:10.74 | lbl: 15.89\n",
      "bb:10.65 | lbl: 18.27\n",
      "bb:11.33 | lbl: 15.48\n",
      "bb:10.91 | lbl: 15.94\n",
      "bb:11.36 | lbl: 15.93\n",
      "bb:11.12 | lbl: 15.87\n",
      "bb:10.81 | lbl: 17.74\n",
      "bb:11.65 | lbl: 14.74\n",
      "bb:11.06 | lbl: 13.83\n",
      "bb:10.72 | lbl: 12.97\n",
      "bb:11.45 | lbl: 16.89\n",
      "bb:9.87 | lbl: 13.35\n",
      "bb:9.70 | lbl: 15.67\n",
      "bb:10.23 | lbl: 12.31\n",
      "bb:9.81 | lbl: 11.07\n",
      "bb:9.82 | lbl: 12.20\n",
      "bb:9.93 | lbl: 13.71\n",
      "bb:10.33 | lbl: 13.86\n",
      "bb:7.90 | lbl: 12.90\n",
      "bb:10.66 | lbl: 15.20\n",
      "bb:10.93 | lbl: 13.09\n",
      "bb:10.17 | lbl: 12.66\n",
      "bb:11.94 | lbl: 16.26\n",
      "bb:10.13 | lbl: 15.51\n",
      "bb:11.42 | lbl: 14.44\n",
      "bb:10.79 | lbl: 13.93\n",
      "bb:10.15 | lbl: 13.42\n",
      "bb:10.34 | lbl: 15.02\n",
      "bb:10.26 | lbl: 13.30\n",
      "bb:10.90 | lbl: 15.12\n",
      "bb:11.06 | lbl: 13.55\n",
      "bb:10.61 | lbl: 14.86\n",
      "bb:10.22 | lbl: 12.10\n",
      "bb:10.58 | lbl: 12.90\n",
      "bb:10.53 | lbl: 14.90\n",
      "bb:10.45 | lbl: 14.32\n",
      "bb:10.22 | lbl: 12.88\n",
      "bb:10.14 | lbl: 14.28\n",
      "bb:9.95 | lbl: 13.51\n",
      "bb:10.98 | lbl: 14.40\n",
      "bb:11.77 | lbl: 15.28\n",
      "bb:11.46 | lbl: 14.86\n",
      "bb:10.57 | lbl: 12.63\n",
      "bb:11.29 | lbl: 13.85\n",
      "bb:10.34 | lbl: 12.29\n",
      "bb:10.49 | lbl: 13.68\n",
      "bb:10.52 | lbl: 15.23\n",
      "bb:10.54 | lbl: 13.88\n",
      "bb:10.21 | lbl: 13.32\n",
      "bb:10.35 | lbl: 13.84\n",
      "bb:9.63 | lbl: 12.59\n",
      "bb:9.78 | lbl: 15.83\n",
      "bb:11.69 | lbl: 13.29\n",
      "bb:9.06 | lbl: 10.57\n",
      "bb:9.61 | lbl: 11.08\n",
      "bb:10.45 | lbl: 13.87\n",
      "bb:10.18 | lbl: 13.21\n",
      "bb:7.69 | lbl: 12.62\n",
      "bb:9.68 | lbl: 11.46\n",
      "bb:9.85 | lbl: 12.18\n",
      "bb:10.52 | lbl: 12.77\n",
      "bb:9.92 | lbl: 14.52\n",
      "bb:10.45 | lbl: 13.66\n",
      "bb:10.19 | lbl: 11.92\n",
      "bb:9.87 | lbl: 11.72\n",
      "bb:10.79 | lbl: 13.18\n",
      "bb:10.10 | lbl: 11.37\n",
      "bb:10.27 | lbl: 15.82\n",
      "bb:10.14 | lbl: 12.48\n",
      "bb:10.31 | lbl: 12.39\n",
      "bb:9.82 | lbl: 12.05\n",
      "bb:10.28 | lbl: 14.12\n",
      "bb:9.35 | lbl: 13.80\n",
      "bb:10.33 | lbl: 13.27\n",
      "bb:10.42 | lbl: 11.18\n",
      "bb:10.23 | lbl: 13.06\n",
      "bb:10.10 | lbl: 12.30\n",
      "bb:9.76 | lbl: 11.75\n",
      "bb:10.03 | lbl: 11.63\n",
      "bb:9.56 | lbl: 12.64\n",
      "bb:9.59 | lbl: 13.23\n",
      "bb:9.58 | lbl: 12.10\n",
      "bb:9.83 | lbl: 11.94\n",
      "bb:10.62 | lbl: 13.37\n",
      "bb:10.16 | lbl: 12.57\n",
      "bb:9.59 | lbl: 15.25\n",
      "bb:9.62 | lbl: 12.49\n",
      "bb:9.14 | lbl: 12.79\n",
      "bb:11.10 | lbl: 13.05\n",
      "bb:8.99 | lbl: 11.63\n",
      "bb:8.89 | lbl: 13.98\n",
      "bb:9.75 | lbl: 11.21\n",
      "bb:8.82 | lbl: 10.07\n",
      "bb:8.94 | lbl: 10.66\n",
      "bb:9.29 | lbl: 12.45\n",
      "bb:9.45 | lbl: 12.71\n",
      "bb:7.33 | lbl: 11.78\n"
     ]
    }
   ],
   "source": [
    "learner.fit_one_cycle(5, lr=2e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('models/s1.pth')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.save('s1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.export('models/20201214_pascal2007_rory.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learner.export('models/20201210_pascal2007.pkl')      # trained for 10 epochs\n",
    "# learner.export('models/20201210_pascal2007_bad.pkl')  # trained for 5 epochs\n",
    "# learner.export('models/20201211_pascal2007_focalfix.pkl') # corrected weight in focal loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stepping Through a Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### batch loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "# device='cpu'\n",
    "\n",
    "# subdivs = [4, 2, 1]\n",
    "# zooms   = [0.75, 1.0, 1.3]\n",
    "# ratios  = [(1.,1.), (1.,.5), (.5,1)]\n",
    "# k = len(zooms) * len(ratios)\n",
    "# anchors, box_size = create_anchors(subdivs, zooms, ratios, device)\n",
    "# anchor_boxes = create_anchor_boxes(anchors[:,:2], anchors[:,2:])\n",
    "# loss_f = FocalLoss(dls.ncls, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get batch, acts\n",
    "mod = get_fresh_mod(device=device)\n",
    "mod.eval()\n",
    "\n",
    "batch = next(iter(dls.cpu().valid))\n",
    "acts = mod(batch[0])\n",
    "batch_bbs, batch_lbls = batch[1], batch[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1392, grad_fn=<L1LossBackward>),\n",
       " tensor(1.1336, grad_fn=<DivBackward0>))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get acts and targs for a single im\n",
    "bidx = 0\n",
    "act_bbs,act_lbls,bbs,lbls = list(zip(*acts,batch_bbs,batch_lbls))[bidx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### item loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbs,lbls = remove_padding(bbs,lbls)                      # remove gt padding inserted during training\n",
    "bbs,lbls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_bbs = get_pred_bbs(act_bbs,anchors,box_size,device) # make 196 pred bbs from acts and ancs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map gt to preds\n",
    "iou_gt_grid = get_iou(bbs.data, anchor_boxes.data)       # get iou(gt_bbs,anc_bbs); used to map gt → ancs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_gt_preds, mapped_gt_idx = map_to_gt(iou_gt_grid)     # assign each pred an index of a gt object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_bbs  = bbs[mapped_gt_idx]                         # project gt bbs into pred space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_lbls = lbls[mapped_gt_idx]                        # project gt lbls into pred space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove low-iou bb preds & set mapped lbl to bg\n",
    "high_iou = iou_gt_preds > 0.4                            # only include bb preds that overlap w/a gt obj and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incl = torch.nonzero(high_iou)[:,0]                      #  are not predicting background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_lbls[~high_iou] = dls.ncls                        # assign gt class of bg to preds w/ low max gt iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute loss\n",
    "bb_res  = F.l1_loss(pred_bbs[incl], mapped_bbs[incl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl_res = loss_f(act_lbls, mapped_lbls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Data\" data-toc-modified-id=\"Data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Data</a></span></li><li><span><a href=\"#Code\" data-toc-modified-id=\"Code-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Code</a></span><ul class=\"toc-item\"><li><span><a href=\"#RM\" data-toc-modified-id=\"RM-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>RM</a></span></li></ul></li><li><span><a href=\"#Train\" data-toc-modified-id=\"Train-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Train</a></span><ul class=\"toc-item\"><li><span><a href=\"#Train\" data-toc-modified-id=\"Train-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Train</a></span></li><li><span><a href=\"#View-Results\" data-toc-modified-id=\"View-Results-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>View Results</a></span></li></ul></li><li><span><a href=\"#Stepping-Through-a-Batch\" data-toc-modified-id=\"Stepping-Through-a-Batch-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Stepping Through a Batch</a></span><ul class=\"toc-item\"><li><span><a href=\"#RM\" data-toc-modified-id=\"RM-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>RM</a></span><ul class=\"toc-item\"><li><span><a href=\"#Init\" data-toc-modified-id=\"Init-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>Init</a></span></li><li><span><a href=\"#Get-batch,-acts,-item\" data-toc-modified-id=\"Get-batch,-acts,-item-4.1.2\"><span class=\"toc-item-num\">4.1.2&nbsp;&nbsp;</span>Get batch, acts, item</a></span></li><li><span><a href=\"#mAP\" data-toc-modified-id=\"mAP-4.1.3\"><span class=\"toc-item-num\">4.1.3&nbsp;&nbsp;</span>mAP</a></span></li><li><span><a href=\"#ssd-item-loss\" data-toc-modified-id=\"ssd-item-loss-4.1.4\"><span class=\"toc-item-num\">4.1.4&nbsp;&nbsp;</span>ssd item loss</a></span></li><li><span><a href=\"#lbl-loss\" data-toc-modified-id=\"lbl-loss-4.1.5\"><span class=\"toc-item-num\">4.1.5&nbsp;&nbsp;</span>lbl loss</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**~ My Code ~**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports & Paths ###\n",
    "from fastai.vision.all import *\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def random_seed(s, use_cuda):\n",
    "    #Also, remember to use num_workers=0 when creating the DataBunch\n",
    "    np.random.seed(s)\n",
    "    torch.manual_seed(s)\n",
    "    random.seed(s)\n",
    "    if use_cuda:\n",
    "        torch.cuda.manual_seed(s)\n",
    "        torch.cuda.manual_seed_all(s)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False      \n",
    "random_seed(42,True)\n",
    "\n",
    "\n",
    "### Params ###\n",
    "im_sz   = 224\n",
    "bs      = 64\n",
    "val_pct = .2\n",
    "sub_pct = 1\n",
    "path = untar_data(URLs.PASCAL_2007)\n",
    "annos_path = path/'train.json'\n",
    "ims_path = path/'train'\n",
    "\n",
    "\n",
    "### Items ###\n",
    "fns, annos = get_annotations(annos_path)\n",
    "fn2anno = {f:a for f,a in zip(fns,annos)}\n",
    "def get_im(f):   return ims_path/f\n",
    "def get_bbox(f): return fn2anno[f][0]\n",
    "def get_lbl(f):  return fn2anno[f][1]\n",
    "\n",
    "\n",
    "### DataLoaders ###\n",
    "itfms = Resize(im_sz, method='squish')\n",
    "btfms = setup_aug_tfms([Rotate(), Brightness(), Contrast(), Flip(),\n",
    "                       Normalize.from_stats(*imagenet_stats)])\n",
    "db = DataBlock(\n",
    "    blocks=[ImageBlock, BBoxBlock, BBoxLblBlock(add_na=False)],\n",
    "    get_x=get_im, get_y=[get_bbox, get_lbl], n_inp=1,\n",
    "    splitter=RandomSplitter(val_pct),\n",
    "    item_tfms=itfms, batch_tfms=btfms)\n",
    "# subset = L(fns).shuffle()[0:int(len(fns)*sub_pct)]\n",
    "# dls = db.dataloaders(subset, bs=bs)\n",
    "dls = db.dataloaders(fns, bs=bs)\n",
    "dls.v = dls.vocab\n",
    "dls.ncls = len(dls.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab: (#20) ['aeroplane','bicycle','bird','boat','bottle','bus','car','cat','chair','cow'...]\n",
      "Size of train data: 2001\n",
      "Size of valid data: 500\n",
      "batch[0]: \t torch.float32 \t torch.Size([64, 3, 224, 224])\n",
      "batch[1]: \t torch.float32 \t torch.Size([64, 24, 4])\n",
      "batch[2]: \t torch.int64 \t torch.Size([64, 24])\n"
     ]
    }
   ],
   "source": [
    "### Inspection ###\n",
    "print(\"Vocab:\", dls.v)\n",
    "print(\"Size of train data:\",len(dls.train.items))\n",
    "print(\"Size of valid data:\",len(dls.valid.items))\n",
    "for i,t in enumerate(dls.one_batch()):\n",
    "    print(f\"batch[{i}]:\",'\\t',t.dtype,'\\t',t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation of tensor shapes:\n",
    "- torch.Size([128, 3, 224, 224]): bs, channels (rgb), im_sz, im_sz\n",
    "- torch.Size([128, 20, 4]): bs, max objs for a single im in batch, bb coords\n",
    "- torch.Size([128, 20]): bs, max objs for a single im in batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Anchors ###\n",
    "def create_anchors(subdivs, zooms, ratios, device='cuda'):\n",
    "    # create list of permutations per default anchor box\n",
    "    perms = [(z*r1,z*r2) for z in zooms for (r1,r2) in ratios]\n",
    "    k = len(perms)\n",
    "    offsets = [1/(sd*2) for sd in subdivs]\n",
    "    xs = np.concatenate([np.tile(  np.linspace(o,1-o,sd),sd) for o,sd in zip(offsets,subdivs)])\n",
    "    ys = np.concatenate([np.repeat(np.linspace(o,1-o,sd),sd) for o,sd in zip(offsets,subdivs)])\n",
    "    ctrs = np.repeat(np.stack([xs,ys], axis=1), k, axis=0)\n",
    "    hws = np.concatenate([np.array([[o/sd,p/sd] for i in range(sd*sd) for o,p in perms]) for sd in subdivs])\n",
    "    box_sizes = tensor(np.concatenate([np.array([1/sd for i in range(sd*sd) for o,p in perms])\n",
    "                                      for sd in subdivs]), requires_grad=False).unsqueeze(1)\n",
    "    anchors = tensor(np.concatenate([ctrs, hws], axis=1), requires_grad=False).float()\n",
    "    return anchors.to(device), box_sizes.to(device)\n",
    "\n",
    "def hw2pp(ctr, hw):\n",
    "    return torch.cat([ctr-hw/2, ctr+hw/2], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Architecture ###\n",
    "def flatten_conv(x,k):\n",
    "    bs,nf,gx,gy = x.size()\n",
    "    return x.permute(0,2,3,1).contiguous().view(bs,-1,nf//k)\n",
    "\n",
    "class StdConv(Module):\n",
    "    \"\"\"Wraps together the standard conv2d→ batchnorm→ dropout.\"\"\"\n",
    "    def __init__(self, nin, nout, stride=2, drop=0.1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(nin, nout, 3, stride=stride, padding=1)\n",
    "        self.bn = nn.BatchNorm2d(nout)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.drop(self.bn(F.relu(self.conv(x))))\n",
    "\n",
    "class OutConv(Module):\n",
    "    \"\"\"Outputs two sets of acts: one for bbs, one for lbls.\"\"\"\n",
    "    def __init__(self, k, nin, bias):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "        self.bbs  = nn.Conv2d(nin,            4*k, 3, padding=1) # bbs\n",
    "        self.lbls = nn.Conv2d(nin, (dls.ncls+1)*k, 3, padding=1) # lbls\n",
    "        self.lbls.bias.data.zero_().add_(bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return [flatten_conv(self.bbs(x),  self.k), # bbs,lbls\n",
    "                flatten_conv(self.lbls(x), self.k)]         \n",
    "\n",
    "class SSDHead(Module):\n",
    "    \"\"\"Wraps StdConv and OutConv into a head module.\n",
    "       Defaults to resnet34 backbone.\"\"\"\n",
    "    def __init__(self, k, bias, drop=0.4, body='resnet34'):\n",
    "        super().__init__()\n",
    "        test(body, ['resnet34','resnet50'], operator.in_)\n",
    "        self.body  = body\n",
    "        self.drop  = nn.Dropout(drop)\n",
    "        self.conv0 = StdConv( 512, 256, drop=drop)\n",
    "        self.conv1 = StdConv( 256, 256, drop=drop)\n",
    "        self.conv2 = StdConv( 256, 256, drop=drop)\n",
    "        self.out0  = OutConv(k, 256, bias)\n",
    "        self.out1  = OutConv(k, 256, bias)\n",
    "        self.out2  = OutConv(k, 256, bias)\n",
    "        self.re_sz = StdConv(2048, 512, stride=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.body == 'resnet34': x = F.relu(x)\n",
    "        x = self.drop(x)\n",
    "        if self.body == 'resnet50': x = self.re_sz(x)\n",
    "        x = self.conv0(x)\n",
    "        bb0,lbl0 = self.out0(x)\n",
    "        x = self.conv1(x)\n",
    "        bb1,lbl1 = self.out1(x)\n",
    "        x = self.conv2(x)\n",
    "        bb2,lbl2 = self.out2(x)\n",
    "        return [torch.cat([ bb0, bb1, bb2], dim=1),\n",
    "                torch.cat([lbl0,lbl1,lbl2], dim=1)]\n",
    "\n",
    "class CustMod(Module):\n",
    "    \"\"\"A module made from a pretrained body and an untrained head.\"\"\"\n",
    "    def __init__(self, body, head):\n",
    "        self.body, self.head = body, head\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.head(self.body(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FocalLoss ###\n",
    "def one_hot_embedding(lbls, ncls, device='cuda'):\n",
    "    return torch.eye(ncls)[lbls.data].to(device)\n",
    "\n",
    "class BCELoss(nn.Module):\n",
    "    def __init__(self, ncls, device='cuda'):\n",
    "        super().__init__()\n",
    "        self.ncls = ncls\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, acts, targs):\n",
    "        t = one_hot_embedding(targs, self.ncls+1, self.device)\n",
    "        t = tensor(t[:,:-1].contiguous())\n",
    "        a = acts[:,:-1]\n",
    "        w = self.get_weight(a,t).detach()\n",
    "        return F.binary_cross_entropy_with_logits(a,t,w,reduction='sum')/self.ncls\n",
    "    \n",
    "    def get_weight(self,a,t): return None \n",
    "    \n",
    "class FocalLoss(BCELoss):\n",
    "    def get_weight(self, a, t):\n",
    "        alpha, gamma = 0.25, 2.0 # vals from paper\n",
    "        p = a.sigmoid()\n",
    "        pt = p*t + (1-p)*(1-t)\n",
    "        w = alpha*t + (1-alpha)*(1-t)\n",
    "        return w * (1-pt).pow(gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IoU ###\n",
    "def intersxn(b1,b2):\n",
    "    x1 = torch.max((b1)[:,None,0], (b2)[None,:,0])\n",
    "    y1 = torch.max((b1)[:,None,1], (b2)[None,:,1])\n",
    "    x2 = torch.min((b1)[:,None,2], (b2)[None,:,2])\n",
    "    y2 = torch.min((b1)[:,None,3], (b2)[None,:,3])\n",
    "    return torch.clamp((x2-x1), min=0) * torch.clamp((y2-y1), min=0)\n",
    "\n",
    "def area(b):\n",
    "    return (b[:,2]-b[:,0]) * (b[:,3]-b[:,1])\n",
    "\n",
    "def get_iou(b1, b2):\n",
    "    inter = intersxn(b1,b2)\n",
    "    union = area(b1).unsqueeze(1) + area(b2).unsqueeze(0) - inter\n",
    "    return inter / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ssd_loss ###\n",
    "def remove_padding(bb, lbl, rescale=True):\n",
    "    z = (bb[:,2]-bb[:,0])==0\n",
    "    return ((1+bb[~z])/2,lbl[~z]) if rescale else (bb[~z],lbl[~z])\n",
    "\n",
    "def get_pred_bbs(abbs, ancs, anc_sz, device):\n",
    "    act_bbs = torch.tanh(abbs)\n",
    "    ctrs = ancs.to(device)[:,:2] + (act_bbs.to(device)[:,:2]/2 * anc_sz.to(device))\n",
    "    hws  = ancs.to(device)[:,2:] * (act_bbs.to(device)[:,2:]/2+1)        \n",
    "    return hw2pp(ctrs, hws)\n",
    "\n",
    "def map_to_gt(ious):\n",
    "    max_iou_per_bb, anc_idxs = ious.max(1)\n",
    "    max_iou_per_anc, bb_idxs = ious.max(0)\n",
    "    max_iou_per_anc[anc_idxs] = 1.99\n",
    "    for i,iou in enumerate(anc_idxs): bb_idxs[iou] = i\n",
    "    return max_iou_per_anc, bb_idxs\n",
    "\n",
    "def ssd_item_loss(act_bbs, act_lbls, bbs, lbls, device='cuda'):\n",
    "    \"\"\"SSD item loss takes single items from a minibatch, creates hundreds of preds, maps gt\n",
    "       to the preds, prunes the preds, then calcs & returns the bb and lbl loss for that item.\"\"\"\n",
    "   # prep\n",
    "    bbs,lbls = remove_padding(bbs,lbls)                      # remove gt padding inserted during training\n",
    "    pred_bbs = get_pred_bbs(act_bbs,anchors,box_size,device) # make 196 pred bbs from acts and ancs\n",
    "    # map gt to preds\n",
    "    iou_gt_grid = get_iou(bbs.data, anchor_boxes.data)       # get iou(gt_bbs,anc_bbs); used to map gt → ancs\n",
    "    iou_gt_preds, mapped_gt_idx = map_to_gt(iou_gt_grid)     # assign each pred an index of a gt object\n",
    "    mapped_bbs  = bbs[mapped_gt_idx]                         # project gt bbs into pred space\n",
    "    mapped_lbls = lbls[mapped_gt_idx]                        # project gt lbls into pred space\n",
    "    # remove low-iou bb preds & set mapped lbl to bg\n",
    "    high_iou = iou_gt_preds > 0.4                            # only include bb preds that overlap w/a gt obj and\n",
    "    incl = torch.nonzero(high_iou)[:,0]                      #  are not predicting background\n",
    "    mapped_lbls[~high_iou] = dls.ncls                        # assign gt class of bg to preds w/ low max gt iou\n",
    "    # compute loss\n",
    "    bb_res  = F.l1_loss(pred_bbs[incl], mapped_bbs[incl])\n",
    "    lbl_res = loss_f(act_lbls, mapped_lbls)\n",
    "    return bb_res, lbl_res\n",
    "\n",
    "def ssd_loss(acts, bbs, lbls, device='cuda', print_it=False):\n",
    "    bb_sum, lbl_sum = 0., 0.\n",
    "    for o in zip(*acts, bbs, lbls):\n",
    "        bb_loss, lbl_loss = ssd_item_loss(*o, device)\n",
    "        bb_sum  += bb_loss\n",
    "        lbl_sum += lbl_loss\n",
    "    if print_it: print(f\"bb:{bb_sum:.02f} | lbl: {lbl_sum:.02f}\")\n",
    "    return bb_sum + lbl_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Metric & Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### nms ###\n",
    "def nms(boxes, scores, iou_thresh, top_k=100):\n",
    "    keep = scores.new(scores.size(0)).zero_().long()\n",
    "    if boxes.numel() == 0: return keep\n",
    "    x1 = boxes[:, 0]\n",
    "    y1 = boxes[:, 1]\n",
    "    x2 = boxes[:, 2]\n",
    "    y2 = boxes[:, 3]\n",
    "    area = torch.mul(x2 - x1, y2 - y1)\n",
    "    v, idx = scores.sort(0)  # sort asc\n",
    "    idx = idx[-top_k:]       # indices of k largest vals\n",
    "    xx1 = boxes.new()\n",
    "    yy1 = boxes.new()\n",
    "    xx2 = boxes.new()\n",
    "    yy2 = boxes.new()\n",
    "    w = boxes.new()\n",
    "    h = boxes.new()\n",
    "\n",
    "    count = 0\n",
    "    while idx.numel() > 0:\n",
    "        i = idx[-1]  # index of current largest val\n",
    "        keep[count] = i\n",
    "        count += 1\n",
    "        if idx.size(0) == 1: break\n",
    "        idx = idx[:-1]  # remove kept element from view\n",
    "        # load bboxes of next highest vals\n",
    "        torch.index_select(x1, 0, idx, out=xx1)\n",
    "        torch.index_select(y1, 0, idx, out=yy1)\n",
    "        torch.index_select(x2, 0, idx, out=xx2)\n",
    "        torch.index_select(y2, 0, idx, out=yy2)\n",
    "        # store element-wise max with next highest score\n",
    "        xx1 = torch.clamp(xx1, min=x1[i])\n",
    "        yy1 = torch.clamp(yy1, min=y1[i])\n",
    "        xx2 = torch.clamp(xx2, max=x2[i])\n",
    "        yy2 = torch.clamp(yy2, max=y2[i])\n",
    "        w.resize_as_(xx2)\n",
    "        h.resize_as_(yy2)\n",
    "        w = xx2 - xx1\n",
    "        h = yy2 - yy1\n",
    "        # check sizes of xx1 and xx2.. after each iteration\n",
    "        w = torch.clamp(w, min=0.0)\n",
    "        h = torch.clamp(h, min=0.0)\n",
    "        inter = w*h\n",
    "        # IoU = i / (area(a) + area(b) - i)\n",
    "        rem_areas = torch.index_select(area, 0, idx)  # load remaining areas)\n",
    "        union = (rem_areas - inter) + area[i]\n",
    "        IoU = inter/union  # store result in iou\n",
    "        # keep only elements with an IoU <= iou_thresh\n",
    "        idx = idx[IoU.le(iou_thresh)]\n",
    "    return keep, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get_batch_preds ###\n",
    "def acts_to_preds(abb, albl, ancs, anc_sz, iou_thresh, conf_thresh, device):\n",
    "    \"\"\"Turn model acts into preds: abbs use get_pred_bbs, and albls use sigmoid().max().\n",
    "       Used in ResultShower and mAP (and could possibly be used in loss fxn).\"\"\"\n",
    "     # convert acts to preds\n",
    "    pbb = get_pred_bbs(abb, ancs, anc_sz, device)\n",
    "    conf, plbl = albl.sigmoid().max(1)\n",
    "    # filter out preds w/ nms\n",
    "    nms_idxs, nms_n = nms(pbb.data, conf, iou_thresh)\n",
    "    nms_idxs = nms_idxs[:nms_n]\n",
    "    pbb  = pbb[nms_idxs]\n",
    "    plbl = plbl[nms_idxs]\n",
    "    conf = conf[nms_idxs]\n",
    "    # filter out bg and low-conf preds\n",
    "    is_not_bg = (plbl!=20)\n",
    "    is_confident = conf > conf_thresh\n",
    "    mask = is_not_bg & is_confident\n",
    "    return 2*pbb[mask]-1, plbl[mask], conf[mask]\n",
    "\n",
    "def get_batch_preds(abb, albl, ancs, anc_sz, iou_thresh=.5, conf_thresh=.3, device='cpu'):\n",
    "    \"\"\"Loop through a batch and of activations and turn them into predictions.\"\"\"\n",
    "    ancs.to(device); anc_sz.to(device)\n",
    "    pbbs, plbls, confs = [], [], []\n",
    "    for abb, albl in zip(abb, albl):\n",
    "        pbb, plbl, conf = acts_to_preds(abb,albl,ancs,anc_sz,iou_thresh,conf_thresh,device)\n",
    "        pbbs  += [pbb]\n",
    "        plbls += [plbl]\n",
    "        confs += [conf]\n",
    "    return pbbs, plbls, confs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Metric ###\n",
    "def format_inps(acts, batch, anchors, box_size, iou=.5, conf=.3, device='cuda'):\n",
    "    \"\"\"Format acts and targs for AP score calc. Input expects learner.acts & learner.batch,\n",
    "       output format: (im_idx, pred_bbs, pred_cls, cls_conf) and (im_idx, bbs, cls).\n",
    "       Ex: (46.0, tensor([0.1, 0.2, 0.9, 0.9]), tensor(3), tensor(0.78))\"\"\"\n",
    "    \n",
    "    preds = get_batch_preds(acts[0].data, acts[1].data, anchors, box_size, iou, conf, device)\n",
    "    p_idxs  = torch.cat([torch.tensor([i]*len(o)) for i,o in enumerate(preds[0])]).numpy().tolist()\n",
    "    batch_preds = list(zip(p_idxs, *[torch.cat(o) for o in preds]))\n",
    "    \n",
    "    unp = [remove_padding(b,l,False) for b,l in zip(batch[1],batch[2])]\n",
    "    unp = [o[0] for o in unp], [o[1] for o in unp]\n",
    "    unp_flat = torch.cat(unp[0]), torch.cat(unp[1])\n",
    "    gt_idxs = torch.cat([torch.tensor([i]*len(o)) for i,o in enumerate(unp[0])]).numpy().tolist()\n",
    "    batch_gts = list(zip(gt_idxs, *unp_flat))\n",
    "    return batch_preds, batch_gts\n",
    "\n",
    "def flatten_list(l, ret_L=False):\n",
    "    \"\"\"Flatten a list-of-lists; lists can be python `list`s or a fastai `L`s.\"\"\"\n",
    "    def _recur(l,res):\n",
    "        for o in l:\n",
    "            if   isinstance(o,list): _recur(o,res)\n",
    "            elif isinstance(o,L)   : _recur(o,res)\n",
    "            else: res.append(o)\n",
    "        return res\n",
    "    res = _recur(l, [])\n",
    "    return res if not ret_L else L(res)\n",
    "\n",
    "def _get_tp_bbs(preds_tp):\n",
    "    \"\"\"Output list of tp bbs per im in batch. Not used to calculate mAP; only\n",
    "       used to grab true positive bb preds for visualizing in ResultShower.\"\"\"\n",
    "    # Each row in preds_tps is a formatted pred (see format_ap_inputs) and a list of\n",
    "    # 1s and 0s (signifying tps and fps) for a cls. No preds for a cls → empty lists.\n",
    "    batch_idxs, pred_bbs, tpfps = [], [], []\n",
    "    for preds,tp in preds_tp:\n",
    "        batch_idxs.append([o[0] for o in preds])\n",
    "        pred_bbs.append([o[1] for o in preds])\n",
    "        tpfps.append(tp)\n",
    "    flat_idxs  = flatten_list(batch_idxs)\n",
    "    flat_bbs   = flatten_list(pred_bbs)\n",
    "    flat_tpfps = torch.cat(tpfps)\n",
    "\n",
    "    scored_preds = list(zip(flat_idxs, flat_bbs, flat_tpfps))\n",
    "    true_bbs = [(int(o[0]), o[1]) for o in scored_preds if o[2]==True]\n",
    "\n",
    "    true_preds = [torch.zeros(4).view(1,4) for i in range(0,bs)]\n",
    "    for i,bb in true_bbs:\n",
    "        if true_preds[i].sum()==0: true_preds[i] = bb.view(1,4)\n",
    "        else: true_preds[i] = torch.cat([true_preds[int(i)], bb.view(1,4)], dim=0)\n",
    "    return true_preds\n",
    "\n",
    "def agg_ten(ten,agg,ifempty=0):\n",
    "    return ifempty if ten.shape[0]==0 else agg(ten).item()\n",
    "\n",
    "def ap_per_cls(batch_preds, batch_gts):\n",
    "    \"\"\"Calculate AP score per class. Returns AP for each class.\"\"\"\n",
    "    avg_precs,preds_out,tally,ntps,nfps = [],[],[],[],[]\n",
    "    for c in range(dls.ncls): # start at 1 to ignore gt\n",
    "        # store preds and gts for current cls\n",
    "        preds = [b for b in batch_preds if b[2]==c]\n",
    "        gts   = [b for b in batch_gts   if b[2]==c]\n",
    "                \n",
    "        # sort preds by conf desc\n",
    "        preds.sort(key=lambda x: x[3], reverse=True)\n",
    "        \n",
    "        # make dict of im_idx:zeros(n_objs)\n",
    "        n_objs = Counter([gt[0] for gt in gts])\n",
    "        for k,v in n_objs.items(): n_objs[k] = torch.zeros(v)\n",
    "        \n",
    "        # init tp: a bool tensor for each im s.t. 1s indicate a pred is a tp\n",
    "        tp = torch.zeros((len(preds))).bool()\n",
    "        total_gt_objs = len(gts)\n",
    "        \n",
    "        for pred_idx, pred in enumerate(preds):\n",
    "            gt_objs = [o for o in gts if o[0] == pred[0]]\n",
    "            n_gt_objs = len(gt_objs)\n",
    "            max_iou = 0\n",
    "            \n",
    "            for idx, gt in enumerate(gt_objs):\n",
    "                iou = get_iou(pred[1].view(1,4), gt[1].view(1,4))\n",
    "                if iou > max_iou: max_iou, idx_of_max = iou, idx\n",
    "                    \n",
    "            # update idx of gt_obj to indicate it's been used\n",
    "            if max_iou > iou_thresh:\n",
    "                if n_objs[pred[0]][idx_of_max]==0:\n",
    "                    tp[pred_idx] = 1\n",
    "                    n_objs[pred[0]][idx_of_max] = 1\n",
    "        \n",
    "        # calc avg_prec and store\n",
    "        tps = torch.cumsum(tp, dim=0)               # 1. tp csum: [0,1,1,0,0] → [0,1,2,2,2]\n",
    "        fps = torch.cumsum(~tp, dim=0)              # (basically same steps for fp)\n",
    "        prec = torch.div(tps, (tps + fps + 1e-6))   # 2. divide each tps item by n_preds\n",
    "        prec = torch.cat((torch.tensor([1]), prec)) # 3. slap on a 1 at the beginning\n",
    "        rec = tps / (total_gt_objs + 1e-6)\n",
    "        rec = torch.cat((torch.tensor([0]), rec))\n",
    "        avg_prec = torch.trapz(prec, rec) # calc AP w/ trap rule\n",
    "        avg_precs.append(avg_prec)        # store AP of this cls in accum\n",
    "        \n",
    "        # store data for reporting\n",
    "        preds_out.append(([preds,tp]))\n",
    "        tally.append(n_objs) #needs processing\n",
    "        ntps.append(agg_ten(tps,max))\n",
    "        nfps.append(agg_ten(fps,max))\n",
    "        \n",
    "    fn_bools = [[~dct[k].bool() for k in dct] for dct in tally]\n",
    "    fns = [sum([agg_ten(t,sum) for t in tl]) for tl in fn_bools]\n",
    "    return avg_precs, preds_out, (ntps,nfps,fns)\n",
    "\n",
    "def get_ap_scores(dls, model, ancs, anc_sz, iou, conf, device='cpu'):\n",
    "    for o in [dls, model, anchors, box_size]: o.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    aps,tps,tallies = [],[],[]\n",
    "    for b in dls.valid:\n",
    "        ap,tp,tally = ap_per_cls(model(b[0]),b,dls.ncls,ancs,anc_sz,iou,conf,device)\n",
    "        aps.append(ap)\n",
    "        tps.append(tp)\n",
    "        tallies.append(tally)\n",
    "    ap_scores = torch.stack([tensor(o) for o in res]).sum(axis=0)/len(res)\n",
    "    ap_scores = ap_scores.numpy().tolist()\n",
    "    return sum(ap_scores)/len(ap_scores), ap_scores\n",
    "\n",
    "class MeanAveragePrecision(Metric):\n",
    "    def __init__(self, func):\n",
    "        self.func = func\n",
    "    def reset(self):\n",
    "        self.res = []\n",
    "    def accumulate(self, learn):\n",
    "        is_last_epoch = learn.epoch==learn.n_epoch-1\n",
    "        if is_last_epoch:\n",
    "            cls_aps,_,_ = self.func(learn.pred,(*learn.xb,*learn.yb),dls.ncls,anchors,box_size)\n",
    "            self.res.append(cls_aps)\n",
    "    @property\n",
    "    def value(self):\n",
    "        if self.res==[]:\n",
    "            return 0\n",
    "        else:\n",
    "            ap_scores = torch.stack([tensor(o) for o in self.res]).sum(axis=0)/len(self.res)\n",
    "            return sum(ap_scores)/len(ap_scores)\n",
    "    @property\n",
    "    def name(self):\n",
    "        return \"mAP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get_fresh_mod\n",
    "# def get_fresh_mod(ncls=dls.ncls, k=k, bias=-4., drop=.4, body='resnet34', device='cuda'):\n",
    "#     test(body, ['resnet34','resnet50'], operator.in_)\n",
    "#     arch = resnet34 if body=='resnet34' else resnet50\n",
    "#     return CustMod(create_body(arch, pretrained=True), SSDHead(ncls, k, bias, drop, body))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lil' helprs\n",
    "def pad_strs(strs):\n",
    "    nchars = [len(s) for s in strs]\n",
    "    maxn = max(nchars)\n",
    "    nspc = [maxn-n for n in nchars]\n",
    "    return [s+' '*n for s,n in zip(strs,nspc)]\n",
    "def print_lists(*lists):\n",
    "    for l in [*lists]: test_eq(len(lists[0]), len(l))        # test lens eq\n",
    "    pstrs = [pad_strs([str(o) for o in l]) for l in lists]   # get list of padding strs\n",
    "    for zpstr in list(zip(*pstrs)): print(' | '.join(zpstr)) # print rows joined with ' | '\n",
    "def batch_info(l):\n",
    "    \"\"\"Print idx, type, shape for items in l (a list of tensors)\"\"\"\n",
    "    idxs = list(range(len(l)))\n",
    "    shapes = apply(lambda t:t.shape, l)\n",
    "    types = apply(type, l)\n",
    "    print_lists(idxs, shapes, types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device='cuda'\n",
    "# subdivs = [4, 2, 1]\n",
    "# zooms   = [0.75, 1.0, 1.3]\n",
    "# ratios  = [(1.,1.), (1.,.5), (.5,1)]\n",
    "# k = len(zooms) * len(ratios)\n",
    "\n",
    "# anchors, box_size = create_anchors(subdivs, zooms, ratios, device)\n",
    "# anchor_boxes = hw2pp(anchors[:,:2], anchors[:,2:])\n",
    "# loss_f = FocalLoss(dls.ncls, device)\n",
    "\n",
    "# body = create_body(resnet34, pretrained=True)\n",
    "# head = SSDHead(k, -4., 0.4, 'resnet34')\n",
    "# mod = CustMod(body, head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learner = Learner(dls, mod, loss_func=ssd_loss).to_fp16()\n",
    "# learner.freeze()\n",
    "# lr_min, lr_steep = learner.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr = (lr_min+lr_steep)/2; print(\"lr:\",round(lr,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learner.fit_one_cycle(7, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learner.save('s1')\n",
    "# learner.export('models/20201215_pascal2007_rory.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learner.export('models/20201210_pascal2007.pkl')          # trained for 10 epochs\n",
    "# learner.export('models/20201210_pascal2007_bad.pkl')      # trained for 5 epochs\n",
    "# learner.export('models/20201211_pascal2007_focalfix.pkl') # corrected weight in focal loss\n",
    "# learner.export('models/20201215_pascal2007_rory.pkl')     # corrected bugs in anchor code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Viz Results ###\n",
    "def show_bb(im, bb=None, lbl=[''], title=None, color='white',\n",
    "            ctx=None, sz=im_sz, figsize=5):\n",
    "    # process empties and nones\n",
    "    if bb.shape[-1]==0 or bb==None: bb  = tensor([[0.,0,0,0]])\n",
    "    if lbl==['']:                   lbl = ['']*bb.shape[0]\n",
    "        \n",
    "    # process tensors to take advantage of fastai show methods\n",
    "    bbox = TensorBBox((bb+1)*sz//2)\n",
    "    labeledbbox = LabeledBBox(bbox,lbl)\n",
    "    \n",
    "    if ctx:     show_image(im, figsize=[figsize,figsize], title=title, ctx=ctx)\n",
    "    else: ctx = show_image(im, figsize=[figsize,figsize], title=title)\n",
    "    \n",
    "    labeledbbox.show(ctx=ctx)       # first, draw white lbl bbs...\n",
    "    bbox.show(ctx=ctx, color=color) # ... then overlay color bbs.\n",
    "    return ctx\n",
    "\n",
    "def get_im_tpfpfns(res):\n",
    "    \"\"\"QnD function to output tpfpfns for each im in a batch given a ResultShower.\"\"\"\n",
    "    # TPs are found by counting \n",
    "    tps = []\n",
    "    for bbs in res.tp_bbs:\n",
    "        if bbs.sum()==0: tps.append(0)\n",
    "        else: tps.append(bbs.shape[0])\n",
    "    # FPs are found by subtracing tps from preds_per_im\n",
    "    preds_per_im = [o.shape[0] for o in res.preds[0]]\n",
    "    fps = [pred-tp for pred,tp in zip(preds_per_im, tps)]\n",
    "    # FNs (relies on res.nobj which is a hack)\n",
    "    # res.nobj is a dict where each key is a cls and the values are some combo of\n",
    "    #  im_idx and a bool list st. the len of the list is the number of gt_objs for\n",
    "    #  that class in that im, and the truth value of the bool represents whether\n",
    "    #  the obj is a TP or FN.\n",
    "    im_dict = defaultdict(lambda: [])\n",
    "    for cls in res.nobj:\n",
    "        for im_idx,tp_tensor in cls.items():\n",
    "            im_dict[im_idx] += ~tp_tensor.bool()\n",
    "    idx_fns = [(k,sum(v).item()) for k,v in im_dict.items()]\n",
    "    idx_fns.sort()\n",
    "    fns = [o[1] for o in idx_fns]\n",
    "    return tps, fps, fns\n",
    "\n",
    "class ResultShower():\n",
    "    def __init__(self, dls, lrn, ancs, anc_sz, iou, conf):\n",
    "        # store init's args\n",
    "        self.dls    = dls\n",
    "        self.mod    = lrn.model.eval().cpu()\n",
    "        self.ancs   = ancs.cpu()\n",
    "        self.anc_sz = anc_sz.cpu()\n",
    "        self.iou    = iou\n",
    "        self.conf   = conf\n",
    "        # compute attrs\n",
    "        self.batch    = next(iter(self.dls.cpu().valid))\n",
    "        self.acts     = [a.data for a in self.mod(self.batch[0])]\n",
    "        self.preds    = get_batch_preds(*self.acts,self.ancs,self.anc_sz,self.iou,self.conf,'cpu')\n",
    "        self.dec_ims  = self.dls.decode(self.batch)[0]\n",
    "        self.bs       = self.dls.bs\n",
    "        self.voc      = self.dls.vocab\n",
    "        self.im_sz    = self.batch[0].shape[-1]\n",
    "        self.last_res = 0\n",
    "        self.fig_sz   = [8,8]\n",
    "        # compute metrics\n",
    "        aps,tp_bbs,nobj = ap_per_cls(self.acts,self.batch,len(self.voc),\n",
    "                                     self.ancs,self.anc_sz,self.iou,self.conf,'cpu')\n",
    "        self.ap_scores  = [o.item() for o in aps]\n",
    "        self.tp_bbs     = tp_bbs\n",
    "        self.nobj       = nobj\n",
    "        self.im_tpfpfns = get_im_tpfpfns(self)\n",
    "        # clean up\n",
    "        self.dls.cuda(); self.mod.cuda()\n",
    "        \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.show_next(*args, **kwargs)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        # get everything to draw\n",
    "        ims             = self.dec_ims\n",
    "        _,bbs,lbls      = self.batch\n",
    "        pbbs,plbls,conf = self.preds\n",
    "        tbbs            = self.tp_bbs\n",
    "        tps,fps,fns     = self.im_tpfpfns\n",
    "        # titles\n",
    "        t_gt = f\"Idx {i} (nobj={(lbls[i] > 0).sum()})\"\n",
    "        t_p  = f\"TPs:{tps[i]} | FPs:{fps[i]} | FNs:{fns[i]}\"\n",
    "        # two ctx: gts and all preds. lime bbs drawn over TP preds.\n",
    "        ctx = get_grid(2, figsize=self.fig_sz)\n",
    "        show_bb(ims[i],bbs[i], self.voc[lbls[i]], t_gt,'white', ctx[0], self.im_sz)\n",
    "        show_bb(ims[i],pbbs[i],self.voc[plbls[i]],t_p, 'magenta',ctx[1],self.im_sz)\n",
    "        show_bb(ims[i],tbbs[i],color='lime',ctx=ctx[1])\n",
    "                 \n",
    "    def show_next(self, n=1):\n",
    "        for i in range(n): self[(i + self.last_res)%self.bs]\n",
    "        self.last_res += n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = ResultShower(dls, learner, anchors, box_size, .5, .30)\n",
    "# res(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_confs(confs):\n",
    "#     for conf in confs:\n",
    "#         final,_ = get_ap_scores(dls, learner.model, anchors.cpu(), box_size.cpu(), .5, conf, 'cpu')\n",
    "#         print(f\"mAP for conf {conf:.2f}: {round(final,3)}\")\n",
    "# test_confs([round(.05*x,2) for x in range(1,11)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # show ap per cls, desc\n",
    "# sum_scores, scores = get_ap_scores(dls,learner.model,anchors.cpu(),box_size.cpu(),.5,.3,'cpu')\n",
    "# print(\"mAP:\",round(sum_scores,3))\n",
    "# pd.DataFrame({'Class':dls.v, 'AP':[round(o,3) for o in scores]}).sort_values('AP',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stepping Through a Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "device='cpu'\n",
    "\n",
    "subdivs = [4, 2, 1]\n",
    "zooms   = [0.75, 1.0, 1.3]\n",
    "ratios  = [(1.,1.), (1.,.5), (.5,1)]\n",
    "k = len(zooms) * len(ratios)\n",
    "anchors, box_size = create_anchors(subdivs, zooms, ratios, device)\n",
    "anchor_boxes = hw2pp(anchors[:,:2], anchors[:,2:])\n",
    "loss_f = FocalLoss(dls.ncls, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get batch, acts, item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get batch, acts\n",
    "# body = create_body(resnet34, pretrained=True)\n",
    "# head = SSDHead(k, -4., 0.4, 'resnet34')\n",
    "# mod = CustMod(body, head)\n",
    "# mod.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = load_learner('models/20201215_pascal2007_rory.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = learner.model.cpu().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dls.cpu().valid))\n",
    "acts = mod(batch[0])\n",
    "bbs, lbls = batch[1], batch[2]\n",
    "\n",
    "\n",
    "# Get acts and targs for a single im\n",
    "b_idx = 0\n",
    "abb,albl,bb,lbl = list(zip(*acts,bbs,lbls))[b_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([189, 4]), torch.Size([189, 21]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abb.shape,albl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.9177, -0.7167,  0.5527,  0.9417],\n",
       "         [-0.5219, -0.7125,  0.2134,  0.1583],\n",
       "         [ 0.0437,  0.1958,  0.5321,  0.7083],\n",
       "         [-0.0026,  0.1167,  0.3728,  0.4792],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000]]),\n",
       " tensor([12, 14, 12, 14,  0,  0,  0,  0,  0,  0,  0,  0]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bb,lbl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_thresh=.5\n",
    "conf_thresh=.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### format inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get batch of preds (pred_bbs, pred_lbls, pred_confs)\n",
    "preds = get_batch_preds(acts[0].data, acts[1].data, anchors, box_size, iou_thresh, conf_thresh, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# out of interest: how many preds?\n",
    "n_preds = torch.cat(preds[0]).shape[0]\n",
    "n_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unp = [remove_padding(b,l,False) for b,l in zip(batch[1],batch[2])]\n",
    "unp = [o[0] for o in unp], [o[1] for o in unp]\n",
    "unp_flat = torch.cat(unp[0]), torch.cat(unp[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of im idxs containing each pred (used in mAP algorithm)\n",
    "p_idxs  = torch.cat([torch.tensor([i]*len(o)) for i,o in enumerate(preds[0])]).numpy().tolist()\n",
    "gt_idxs = torch.cat([torch.tensor([i]*len(o)) for i,o in enumerate(unp[0])]).numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0,\n",
       "  tensor([-0.3964, -0.6585,  0.4472,  0.7216]),\n",
       "  tensor(14),\n",
       "  tensor(0.4489)),\n",
       " (0.0,\n",
       "  tensor([-0.2058, -0.7428,  0.3694,  0.2123]),\n",
       "  tensor(14),\n",
       "  tensor(0.4188)),\n",
       " (1.0,\n",
       "  tensor([-0.8118,  0.0033, -0.5204,  0.3782]),\n",
       "  tensor(14),\n",
       "  tensor(0.3651)),\n",
       " (2.0,\n",
       "  tensor([-0.9212, -1.0978,  0.9875,  0.9936]),\n",
       "  tensor(7),\n",
       "  tensor(0.6503)),\n",
       " (2.0,\n",
       "  tensor([-0.8433, -0.6127,  1.1810,  0.3928]),\n",
       "  tensor(7),\n",
       "  tensor(0.3050)),\n",
       " (3.0,\n",
       "  tensor([-0.1082,  0.2558,  0.5228,  0.9645]),\n",
       "  tensor(8),\n",
       "  tensor(0.4751)),\n",
       " (3.0,\n",
       "  tensor([-0.4371,  0.2209,  0.1259,  1.0206]),\n",
       "  tensor(8),\n",
       "  tensor(0.4260)),\n",
       " (3.0,\n",
       "  tensor([-0.0981, -0.0264,  0.4014,  0.5821]),\n",
       "  tensor(8),\n",
       "  tensor(0.3560)),\n",
       " (3.0,\n",
       "  tensor([-0.3673,  0.0883, -0.0183,  0.6392]),\n",
       "  tensor(8),\n",
       "  tensor(0.3467)),\n",
       " (3.0,\n",
       "  tensor([-0.6955,  0.4008, -0.2635,  0.9906]),\n",
       "  tensor(8),\n",
       "  tensor(0.3260)),\n",
       " (3.0,\n",
       "  tensor([-0.9313,  0.1074, -0.3092,  0.9473]),\n",
       "  tensor(8),\n",
       "  tensor(0.3188)),\n",
       " (3.0, tensor([0.1617, 0.1668, 0.8498, 0.9343]), tensor(8), tensor(0.3150)),\n",
       " (3.0,\n",
       "  tensor([-0.3849,  0.4704, -0.0267,  0.9724]),\n",
       "  tensor(8),\n",
       "  tensor(0.3045)),\n",
       " (3.0, tensor([0.0974, 0.0265, 0.3602, 0.5140]), tensor(8), tensor(0.3005)),\n",
       " (4.0,\n",
       "  tensor([-0.9543, -0.8620,  0.9335,  0.8129]),\n",
       "  tensor(6),\n",
       "  tensor(0.6142)),\n",
       " (4.0,\n",
       "  tensor([-0.9956, -0.7091, -0.1018,  0.4240]),\n",
       "  tensor(6),\n",
       "  tensor(0.3042)),\n",
       " (5.0,\n",
       "  tensor([-0.4751, -0.3217,  0.0940,  0.6141]),\n",
       "  tensor(1),\n",
       "  tensor(0.5194)),\n",
       " (5.0,\n",
       "  tensor([-0.0572, -0.1913,  0.6222,  0.7565]),\n",
       "  tensor(1),\n",
       "  tensor(0.4777)),\n",
       " (5.0,\n",
       "  tensor([-0.7741, -0.6516,  0.8969,  0.8256]),\n",
       "  tensor(13),\n",
       "  tensor(0.4337)),\n",
       " (5.0,\n",
       "  tensor([ 0.5695, -0.1087,  0.9642,  0.6195]),\n",
       "  tensor(1),\n",
       "  tensor(0.3694)),\n",
       " (5.0,\n",
       "  tensor([ 0.0316, -0.5348,  0.9162,  0.9080]),\n",
       "  tensor(1),\n",
       "  tensor(0.3219)),\n",
       " (6.0,\n",
       "  tensor([-0.9572, -0.3367,  0.8040,  0.7867]),\n",
       "  tensor(5),\n",
       "  tensor(0.3303)),\n",
       " (7.0,\n",
       "  tensor([-0.9103, -0.2595, -0.4722,  0.6508]),\n",
       "  tensor(14),\n",
       "  tensor(0.3825)),\n",
       " (7.0,\n",
       "  tensor([-0.8419, -0.2910, -0.0453,  0.8894]),\n",
       "  tensor(14),\n",
       "  tensor(0.3789)),\n",
       " (7.0,\n",
       "  tensor([-0.5388, -0.2644, -0.0428,  0.6742]),\n",
       "  tensor(14),\n",
       "  tensor(0.3559)),\n",
       " (7.0,\n",
       "  tensor([-0.3188, -0.3293,  0.2723,  0.9542]),\n",
       "  tensor(14),\n",
       "  tensor(0.3034)),\n",
       " (8.0,\n",
       "  tensor([-0.9078, -0.8612,  0.8035,  1.0002]),\n",
       "  tensor(14),\n",
       "  tensor(0.4633)),\n",
       " (8.0,\n",
       "  tensor([-0.9104, -1.0009,  0.0597,  0.8024]),\n",
       "  tensor(14),\n",
       "  tensor(0.3775)),\n",
       " (8.0,\n",
       "  tensor([-0.4915, -0.9288,  0.4303,  0.8620]),\n",
       "  tensor(14),\n",
       "  tensor(0.3312)),\n",
       " (10.0,\n",
       "  tensor([ 0.0440, -0.7068,  0.5590,  0.2372]),\n",
       "  tensor(14),\n",
       "  tensor(0.4030)),\n",
       " (10.0,\n",
       "  tensor([-0.2305, -0.5950,  0.5899,  0.8426]),\n",
       "  tensor(14),\n",
       "  tensor(0.3728)),\n",
       " (10.0,\n",
       "  tensor([ 0.0869, -0.5491,  0.8682,  0.9146]),\n",
       "  tensor(14),\n",
       "  tensor(0.3495)),\n",
       " (11.0,\n",
       "  tensor([ 0.1371, -0.4307,  0.8113,  1.2262]),\n",
       "  tensor(14),\n",
       "  tensor(0.4129)),\n",
       " (12.0,\n",
       "  tensor([-0.3038, -0.5538,  0.5481,  0.8057]),\n",
       "  tensor(2),\n",
       "  tensor(0.4281)),\n",
       " (12.0,\n",
       "  tensor([-0.7461, -0.6449,  0.9077,  0.8671]),\n",
       "  tensor(2),\n",
       "  tensor(0.3167)),\n",
       " (13.0,\n",
       "  tensor([-0.3027, -0.4932,  0.4066,  0.8772]),\n",
       "  tensor(14),\n",
       "  tensor(0.4170)),\n",
       " (13.0,\n",
       "  tensor([ 0.1282, -0.3880,  0.8388,  0.9542]),\n",
       "  tensor(14),\n",
       "  tensor(0.3388)),\n",
       " (13.0,\n",
       "  tensor([ 0.5096, -0.1052,  0.9442,  0.8005]),\n",
       "  tensor(14),\n",
       "  tensor(0.3013)),\n",
       " (15.0,\n",
       "  tensor([-0.4892, -0.8816,  0.8601,  0.9756]),\n",
       "  tensor(14),\n",
       "  tensor(0.4762)),\n",
       " (17.0,\n",
       "  tensor([ 0.6002, -0.9997,  0.8184, -0.3295]),\n",
       "  tensor(4),\n",
       "  tensor(0.3160)),\n",
       " (18.0,\n",
       "  tensor([-0.8388, -0.8986,  0.8774,  0.7830]),\n",
       "  tensor(6),\n",
       "  tensor(0.5139)),\n",
       " (19.0,\n",
       "  tensor([-0.6677, -0.8856,  0.9767,  1.0312]),\n",
       "  tensor(7),\n",
       "  tensor(0.3560)),\n",
       " (20.0,\n",
       "  tensor([-0.9760, -0.7225,  0.7191,  0.8497]),\n",
       "  tensor(11),\n",
       "  tensor(0.3078)),\n",
       " (21.0,\n",
       "  tensor([-0.8440, -0.7908,  0.1402,  1.0130]),\n",
       "  tensor(11),\n",
       "  tensor(0.3668)),\n",
       " (21.0,\n",
       "  tensor([-0.3483, -0.5588,  0.8751,  0.9470]),\n",
       "  tensor(14),\n",
       "  tensor(0.3138)),\n",
       " (22.0,\n",
       "  tensor([-0.6507, -0.4179,  0.8086,  0.5104]),\n",
       "  tensor(6),\n",
       "  tensor(0.3521)),\n",
       " (23.0,\n",
       "  tensor([-0.8264, -0.9454,  0.8067,  0.9343]),\n",
       "  tensor(11),\n",
       "  tensor(0.3618)),\n",
       " (24.0,\n",
       "  tensor([-0.8674, -0.6918,  0.8404,  0.6494]),\n",
       "  tensor(6),\n",
       "  tensor(0.3079)),\n",
       " (25.0,\n",
       "  tensor([-0.7684, -0.7043,  0.7870,  0.9350]),\n",
       "  tensor(11),\n",
       "  tensor(0.4200)),\n",
       " (25.0,\n",
       "  tensor([-0.9708, -0.5079,  0.1035,  0.9749]),\n",
       "  tensor(11),\n",
       "  tensor(0.3503)),\n",
       " (26.0,\n",
       "  tensor([-0.6487, -0.7987,  0.7121,  0.9635]),\n",
       "  tensor(14),\n",
       "  tensor(0.5826)),\n",
       " (27.0,\n",
       "  tensor([-0.8812, -0.7724,  0.7081,  1.0246]),\n",
       "  tensor(14),\n",
       "  tensor(0.6229)),\n",
       " (28.0,\n",
       "  tensor([-0.5320, -0.1810,  0.3440,  0.7858]),\n",
       "  tensor(11),\n",
       "  tensor(0.3250)),\n",
       " (28.0,\n",
       "  tensor([-0.6807, -0.5741,  0.6312,  0.9090]),\n",
       "  tensor(11),\n",
       "  tensor(0.3127)),\n",
       " (29.0,\n",
       "  tensor([-0.8960, -0.6794,  1.0608,  0.6669]),\n",
       "  tensor(18),\n",
       "  tensor(0.4427)),\n",
       " (29.0,\n",
       "  tensor([-0.5061, -0.7341,  0.5190,  0.7667]),\n",
       "  tensor(18),\n",
       "  tensor(0.3092)),\n",
       " (29.0,\n",
       "  tensor([-0.3825,  0.4198, -0.2002,  0.9128]),\n",
       "  tensor(14),\n",
       "  tensor(0.3015)),\n",
       " (30.0, tensor([0.1140, 0.2798, 0.3247, 0.6122]), tensor(3), tensor(0.3310)),\n",
       " (31.0,\n",
       "  tensor([-0.8646, -0.9147,  0.9351,  0.9136]),\n",
       "  tensor(6),\n",
       "  tensor(0.4513)),\n",
       " (32.0,\n",
       "  tensor([-0.8963, -0.9074,  0.8654,  0.7773]),\n",
       "  tensor(18),\n",
       "  tensor(0.3906)),\n",
       " (33.0,\n",
       "  tensor([-0.8686, -0.5428,  0.8533,  0.2303]),\n",
       "  tensor(0),\n",
       "  tensor(0.4893)),\n",
       " (34.0,\n",
       "  tensor([-0.8313, -0.8567,  1.0171,  1.0238]),\n",
       "  tensor(14),\n",
       "  tensor(0.4709)),\n",
       " (34.0,\n",
       "  tensor([-0.1555, -0.6054,  1.0566,  0.8848]),\n",
       "  tensor(14),\n",
       "  tensor(0.3280)),\n",
       " (34.0,\n",
       "  tensor([-1.1595, -0.2988,  0.0827,  1.0036]),\n",
       "  tensor(11),\n",
       "  tensor(0.3143)),\n",
       " (36.0,\n",
       "  tensor([-0.9016, -0.9726,  0.8889,  1.0161]),\n",
       "  tensor(14),\n",
       "  tensor(0.5613)),\n",
       " (36.0,\n",
       "  tensor([-0.9632, -0.9090,  0.1497,  0.5448]),\n",
       "  tensor(14),\n",
       "  tensor(0.3157)),\n",
       " (37.0,\n",
       "  tensor([-0.7293, -0.4694,  0.8328,  0.7935]),\n",
       "  tensor(6),\n",
       "  tensor(0.5467)),\n",
       " (38.0,\n",
       "  tensor([ 0.5276, -0.1791,  0.9542,  0.7442]),\n",
       "  tensor(14),\n",
       "  tensor(0.3712)),\n",
       " (38.0,\n",
       "  tensor([-0.1044, -0.3662,  0.4438,  0.1755]),\n",
       "  tensor(14),\n",
       "  tensor(0.3150)),\n",
       " (38.0,\n",
       "  tensor([ 0.0813, -0.4021,  0.3817,  0.0264]),\n",
       "  tensor(14),\n",
       "  tensor(0.3145)),\n",
       " (38.0, tensor([0.5818, 0.2949, 0.9301, 0.9930]), tensor(14), tensor(0.3081)),\n",
       " (39.0,\n",
       "  tensor([-0.8061, -0.9246,  0.9134,  1.0180]),\n",
       "  tensor(14),\n",
       "  tensor(0.5494)),\n",
       " (41.0,\n",
       "  tensor([-0.7643, -0.6267,  0.7572,  0.4803]),\n",
       "  tensor(0),\n",
       "  tensor(0.3551)),\n",
       " (42.0,\n",
       "  tensor([-0.7726, -0.8497,  0.7248,  0.9149]),\n",
       "  tensor(11),\n",
       "  tensor(0.3525)),\n",
       " (43.0,\n",
       "  tensor([-0.6727, -0.9014,  0.7842,  0.9205]),\n",
       "  tensor(7),\n",
       "  tensor(0.3834)),\n",
       " (44.0,\n",
       "  tensor([-0.0569, -0.1806,  0.4451,  0.6487]),\n",
       "  tensor(14),\n",
       "  tensor(0.4129)),\n",
       " (44.0,\n",
       "  tensor([ 0.4274, -0.2017,  0.8566,  0.7219]),\n",
       "  tensor(14),\n",
       "  tensor(0.3922)),\n",
       " (44.0,\n",
       "  tensor([-0.6045,  0.2056,  0.1295,  1.0137]),\n",
       "  tensor(8),\n",
       "  tensor(0.3640)),\n",
       " (44.0,\n",
       "  tensor([-0.9259, -0.0167, -0.2975,  0.9722]),\n",
       "  tensor(8),\n",
       "  tensor(0.3562)),\n",
       " (44.0,\n",
       "  tensor([-0.4280, -0.1633,  0.0331,  0.6020]),\n",
       "  tensor(14),\n",
       "  tensor(0.3494)),\n",
       " (44.0,\n",
       "  tensor([-0.1615,  0.2036,  0.6018,  1.0140]),\n",
       "  tensor(10),\n",
       "  tensor(0.3484)),\n",
       " (44.0,\n",
       "  tensor([-0.6669,  0.1623, -0.2210,  0.9115]),\n",
       "  tensor(8),\n",
       "  tensor(0.3471)),\n",
       " (44.0,\n",
       "  tensor([ 0.1912, -0.0187,  0.8729,  0.9861]),\n",
       "  tensor(14),\n",
       "  tensor(0.3455)),\n",
       " (44.0,\n",
       "  tensor([-0.9786,  0.2608, -0.5621,  0.9827]),\n",
       "  tensor(8),\n",
       "  tensor(0.3294)),\n",
       " (45.0,\n",
       "  tensor([-0.3167, -0.4849,  0.4109,  0.7758]),\n",
       "  tensor(14),\n",
       "  tensor(0.3528)),\n",
       " (45.0,\n",
       "  tensor([ 0.1874, -0.5956,  0.6182,  0.3352]),\n",
       "  tensor(14),\n",
       "  tensor(0.3250)),\n",
       " (46.0,\n",
       "  tensor([-0.2259,  0.2804, -0.1219,  0.5275]),\n",
       "  tensor(14),\n",
       "  tensor(0.4570)),\n",
       " (46.0, tensor([0.2728, 0.2290, 0.3847, 0.4372]), tensor(14), tensor(0.4226)),\n",
       " (46.0,\n",
       "  tensor([-0.7660,  0.1851, -0.6261,  0.5040]),\n",
       "  tensor(14),\n",
       "  tensor(0.4209)),\n",
       " (46.0, tensor([0.0580, 0.1407, 0.3081, 0.3241]), tensor(6), tensor(0.4197)),\n",
       " (46.0,\n",
       "  tensor([-0.4239,  0.2428, -0.2358,  0.4731]),\n",
       "  tensor(14),\n",
       "  tensor(0.4195)),\n",
       " (46.0, tensor([0.0970, 0.1221, 0.4347, 0.4707]), tensor(6), tensor(0.4157)),\n",
       " (46.0,\n",
       "  tensor([-0.8300,  0.1654, -0.5794,  0.5918]),\n",
       "  tensor(14),\n",
       "  tensor(0.4017)),\n",
       " (46.0,\n",
       "  tensor([-0.3424,  0.1806, -0.1363,  0.5511]),\n",
       "  tensor(14),\n",
       "  tensor(0.3919)),\n",
       " (46.0,\n",
       "  tensor([-0.0278,  0.2549,  0.4683,  0.5743]),\n",
       "  tensor(6),\n",
       "  tensor(0.3824)),\n",
       " (46.0,\n",
       "  tensor([-0.4346,  0.1539, -0.1844,  0.3145]),\n",
       "  tensor(14),\n",
       "  tensor(0.3607)),\n",
       " (46.0,\n",
       "  tensor([-0.8912,  0.0895, -0.6403,  0.4130]),\n",
       "  tensor(6),\n",
       "  tensor(0.3466)),\n",
       " (46.0,\n",
       "  tensor([-0.2791,  0.1685,  0.0496,  0.5574]),\n",
       "  tensor(6),\n",
       "  tensor(0.3369)),\n",
       " (46.0,\n",
       "  tensor([-0.5199,  0.2430, -0.2610,  0.7582]),\n",
       "  tensor(14),\n",
       "  tensor(0.3332)),\n",
       " (46.0,\n",
       "  tensor([-0.7729,  0.1270, -0.4328,  0.5089]),\n",
       "  tensor(6),\n",
       "  tensor(0.3208)),\n",
       " (47.0,\n",
       "  tensor([ 0.2397, -0.0425,  0.9590,  0.6721]),\n",
       "  tensor(6),\n",
       "  tensor(0.4865)),\n",
       " (47.0,\n",
       "  tensor([-0.3404,  0.1794,  0.8395,  0.7449]),\n",
       "  tensor(6),\n",
       "  tensor(0.4207)),\n",
       " (47.0, tensor([0.0872, 0.0416, 0.7492, 0.7223]), tensor(6), tensor(0.3646)),\n",
       " (47.0, tensor([0.6025, 0.1340, 1.0047, 0.5169]), tensor(6), tensor(0.3499)),\n",
       " (47.0,\n",
       "  tensor([ 0.4694, -0.0527,  0.8904,  0.3695]),\n",
       "  tensor(6),\n",
       "  tensor(0.3416)),\n",
       " (47.0,\n",
       "  tensor([-0.3155,  0.0454, -0.1357,  0.3416]),\n",
       "  tensor(14),\n",
       "  tensor(0.3149)),\n",
       " (48.0,\n",
       "  tensor([-0.8561, -0.7751,  0.9113,  0.7923]),\n",
       "  tensor(18),\n",
       "  tensor(0.4115)),\n",
       " (48.0,\n",
       "  tensor([-0.8560,  0.5550, -0.4765,  0.9598]),\n",
       "  tensor(6),\n",
       "  tensor(0.3145)),\n",
       " (49.0,\n",
       "  tensor([-0.0516, -0.1118,  0.9764,  0.8994]),\n",
       "  tensor(15),\n",
       "  tensor(0.3646)),\n",
       " (49.0,\n",
       "  tensor([-0.7886, -0.8864,  0.8140,  0.8772]),\n",
       "  tensor(15),\n",
       "  tensor(0.3303)),\n",
       " (49.0,\n",
       "  tensor([-0.8914, -0.2300, -0.1303,  0.8827]),\n",
       "  tensor(15),\n",
       "  tensor(0.3145)),\n",
       " (49.0,\n",
       "  tensor([ 0.0951, -1.1134,  0.9879,  0.3991]),\n",
       "  tensor(15),\n",
       "  tensor(0.3030)),\n",
       " (50.0,\n",
       "  tensor([-1.0657, -0.9585,  0.8268,  0.9759]),\n",
       "  tensor(7),\n",
       "  tensor(0.3577)),\n",
       " (51.0,\n",
       "  tensor([-0.6413, -0.7103,  0.7804,  0.5237]),\n",
       "  tensor(6),\n",
       "  tensor(0.3086)),\n",
       " (52.0,\n",
       "  tensor([-0.3522, -0.4272,  0.5003,  1.0151]),\n",
       "  tensor(14),\n",
       "  tensor(0.5532)),\n",
       " (52.0,\n",
       "  tensor([-0.8100, -0.5845,  0.8833,  1.0448]),\n",
       "  tensor(14),\n",
       "  tensor(0.4281)),\n",
       " (52.0,\n",
       "  tensor([-0.5082,  0.3029,  0.0541,  1.0233]),\n",
       "  tensor(8),\n",
       "  tensor(0.3776)),\n",
       " (52.0,\n",
       "  tensor([-0.0647, -0.2998,  0.9190,  0.9787]),\n",
       "  tensor(14),\n",
       "  tensor(0.3413)),\n",
       " (52.0,\n",
       "  tensor([-0.7171, -0.3250,  0.6552,  0.5317]),\n",
       "  tensor(14),\n",
       "  tensor(0.3253)),\n",
       " (52.0,\n",
       "  tensor([-0.0348,  0.3317,  0.5558,  1.0543]),\n",
       "  tensor(8),\n",
       "  tensor(0.3228)),\n",
       " (52.0,\n",
       "  tensor([-0.4213, -0.0960, -0.0357,  0.7801]),\n",
       "  tensor(8),\n",
       "  tensor(0.3092)),\n",
       " (52.0, tensor([0.4896, 0.3191, 0.9994, 1.0304]), tensor(8), tensor(0.3021)),\n",
       " (53.0,\n",
       "  tensor([-0.5891, -0.8565,  0.7914,  0.8756]),\n",
       "  tensor(12),\n",
       "  tensor(0.3119)),\n",
       " (55.0,\n",
       "  tensor([-0.4721, -0.7593,  0.2888,  0.6586]),\n",
       "  tensor(14),\n",
       "  tensor(0.3950)),\n",
       " (55.0,\n",
       "  tensor([-0.4492, -0.6975,  0.0752,  0.2415]),\n",
       "  tensor(14),\n",
       "  tensor(0.3057)),\n",
       " (56.0,\n",
       "  tensor([-0.9058, -0.8592,  0.8297,  0.9195]),\n",
       "  tensor(13),\n",
       "  tensor(0.4659)),\n",
       " (56.0,\n",
       "  tensor([-0.9001, -0.8623, -0.0524,  0.5588]),\n",
       "  tensor(14),\n",
       "  tensor(0.3192)),\n",
       " (57.0,\n",
       "  tensor([-0.6062, -0.3767,  0.6961,  0.4674]),\n",
       "  tensor(6),\n",
       "  tensor(0.4074)),\n",
       " (57.0, tensor([0.4853, 0.0021, 0.9413, 0.5190]), tensor(6), tensor(0.3521)),\n",
       " (58.0,\n",
       "  tensor([-1.0555, -1.0556,  1.0001,  0.9603]),\n",
       "  tensor(7),\n",
       "  tensor(0.5172)),\n",
       " (58.0,\n",
       "  tensor([-0.9059, -0.5606,  0.9559,  0.4854]),\n",
       "  tensor(7),\n",
       "  tensor(0.3169)),\n",
       " (60.0,\n",
       "  tensor([-0.6368, -0.3326, -0.0519,  0.5868]),\n",
       "  tensor(14),\n",
       "  tensor(0.3071)),\n",
       " (61.0,\n",
       "  tensor([-0.9517, -0.8297,  0.7646,  0.9576]),\n",
       "  tensor(11),\n",
       "  tensor(0.3498)),\n",
       " (62.0,\n",
       "  tensor([-0.6062, -0.7144,  0.6600,  0.9159]),\n",
       "  tensor(11),\n",
       "  tensor(0.3532)),\n",
       " (63.0,\n",
       "  tensor([-0.9269, -0.8982,  0.8305,  0.9468]),\n",
       "  tensor(11),\n",
       "  tensor(0.3684))]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_preds = list(zip(p_idxs, *[torch.cat(o) for o in preds]))\n",
    "batch_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, tensor([-0.9177, -0.7167,  0.5527,  0.9417]), tensor(12)),\n",
       " (0, tensor([-0.5219, -0.7125,  0.2134,  0.1583]), tensor(14)),\n",
       " (0, tensor([0.0437, 0.1958, 0.5321, 0.7083]), tensor(12)),\n",
       " (0, tensor([-0.0026,  0.1167,  0.3728,  0.4792]), tensor(14)),\n",
       " (1, tensor([-0.9000, -0.0190, -0.5960,  0.3688]), tensor(11)),\n",
       " (1, tensor([-0.5640,  0.0570, -0.3640,  0.3840]), tensor(11)),\n",
       " (1, tensor([ 0.0040, -0.0342,  0.2800,  0.3004]), tensor(11)),\n",
       " (1, tensor([-1.0000, -0.3840, -0.8680,  0.2928]), tensor(14)),\n",
       " (2, tensor([-0.4640, -0.9893,  0.9880,  1.0000]), tensor(7)),\n",
       " (3, tensor([0.4720, 0.0301, 0.7440, 0.6024]), tensor(8)),\n",
       " (3, tensor([-0.2480,  0.2831,  0.1080,  1.0000]), tensor(8)),\n",
       " (3, tensor([-0.5960,  0.1446, -0.2760,  0.9337]), tensor(8)),\n",
       " (3, tensor([-0.4960,  0.1988,  0.4480,  0.9277]), tensor(10)),\n",
       " (4, tensor([-0.8040, -0.8187,  1.0000,  0.9147]), tensor(6)),\n",
       " (5, tensor([ 0.0259, -0.4200,  0.7752,  0.7400]), tensor(1)),\n",
       " (5, tensor([-0.7406, -0.3200,  0.0432,  0.6320]), tensor(1)),\n",
       " (5, tensor([-0.1412, -0.8960,  0.8329,  0.4760]), tensor(14)),\n",
       " (5, tensor([-0.8559, -0.8560,  0.1009,  0.5040]), tensor(14)),\n",
       " (6, tensor([-0.8240, -0.0880,  0.6600,  0.6267]), tensor(5)),\n",
       " (6, tensor([-0.9960,  0.3493, -0.8200,  0.4347]), tensor(6)),\n",
       " (7, tensor([0.4880, 0.0671, 0.5920, 0.4760]), tensor(8)),\n",
       " (7, tensor([0.6800, 0.1757, 0.8200, 0.6933]), tensor(8)),\n",
       " (7, tensor([-0.4640, -0.5399,  0.6040,  1.0000]), tensor(14)),\n",
       " (7, tensor([-0.9920, -0.3546, -0.3920,  0.9936]), tensor(14)),\n",
       " (7, tensor([-0.4120, -0.2652, -0.1040,  0.4121]), tensor(14)),\n",
       " (7, tensor([-0.9720, -0.1949, -0.7880,  0.1182]), tensor(14)),\n",
       " (8, tensor([-0.9960, -0.9893,  0.1920,  1.0000]), tensor(14)),\n",
       " (9, tensor([-0.1040, -0.1520,  1.0000,  0.6053]), tensor(17)),\n",
       " (9, tensor([-0.3720,  0.1413,  0.0680,  0.8613]), tensor(11)),\n",
       " (9, tensor([-0.6520, -0.7227, -0.1960,  0.5840]), tensor(14)),\n",
       " (9, tensor([ 0.0440, -0.3600,  0.5080,  0.4560]), tensor(14)),\n",
       " (10, tensor([-0.0240, -0.3227,  0.3960,  1.0000]), tensor(12)),\n",
       " (10, tensor([ 0.3040, -0.7013,  0.9080,  1.0000]), tensor(14)),\n",
       " (10, tensor([ 0.1640, -0.7707,  0.3840, -0.2640]), tensor(14)),\n",
       " (11, tensor([-1.0000, -1.0000,  0.8320,  0.2733]), tensor(3)),\n",
       " (11, tensor([ 0.2720, -0.3453,  0.7920,  0.8619]), tensor(14)),\n",
       " (12, tensor([-0.0800, -0.5303,  0.4800,  0.5157]), tensor(2)),\n",
       " (13, tensor([-0.2800, -0.1733,  0.2960,  0.8400]), tensor(1)),\n",
       " (13, tensor([-0.1760, -0.5467,  0.3080,  0.6267]), tensor(14)),\n",
       " (13, tensor([ 0.5880, -0.4827,  1.0000,  0.1573]), tensor(19)),\n",
       " (14, tensor([-0.0720, -0.1893,  1.0000,  0.7973]), tensor(17)),\n",
       " (15, tensor([-0.4720, -0.4535, -0.2560,  0.7417]), tensor(4)),\n",
       " (15, tensor([-0.5560, -1.0000,  0.7040,  0.9820]), tensor(11)),\n",
       " (15, tensor([-1.0000, -1.0000, -0.6840, -0.0210]), tensor(15)),\n",
       " (16, tensor([0.6120, 0.7173, 1.0000, 0.8720]), tensor(18)),\n",
       " (16, tensor([-1.0000,  0.4667, -0.4880,  0.6640]), tensor(18)),\n",
       " (16, tensor([-0.1760,  0.2587,  0.2240,  0.4560]), tensor(18)),\n",
       " (17, tensor([-0.9960, -0.9947,  1.0000,  1.0000]), tensor(10)),\n",
       " (17, tensor([-1.0000, -1.0000, -0.7560, -0.5147]), tensor(8)),\n",
       " (17, tensor([-0.5640, -1.0000, -0.1040, -0.7173]), tensor(8)),\n",
       " (17, tensor([0.5400, 0.2747, 1.0000, 1.0000]), tensor(8)),\n",
       " (18, tensor([-0.8320, -0.9093,  0.6040,  0.6800]), tensor(6)),\n",
       " (19, tensor([-0.5600, -0.9198,  1.0000,  1.0000]), tensor(7)),\n",
       " (20, tensor([-0.6216, -0.4840,  0.7237,  0.5720]), tensor(11)),\n",
       " (20, tensor([-0.9940, -0.6400,  1.0000,  0.3640]), tensor(11)),\n",
       " (20, tensor([ 0.0390, -0.7840,  0.9039, -0.2560]), tensor(19)),\n",
       " (21, tensor([-0.0680, -0.7658,  0.5320,  1.0000]), tensor(14)),\n",
       " (21, tensor([-1.0000, -0.7477, -0.0040,  0.8318]), tensor(12)),\n",
       " (22, tensor([-0.0920, -0.5307,  1.0000,  0.5147]), tensor(3)),\n",
       " (22, tensor([-0.9360, -0.3600,  0.1840,  0.5093]), tensor(3)),\n",
       " (23, tensor([-0.5600, -0.9840,  0.4400,  0.8507]), tensor(11)),\n",
       " (24, tensor([-0.9960, -0.5680,  1.0000,  0.5893]), tensor(0)),\n",
       " (25, tensor([-0.9960, -0.3067, -0.2600,  0.9893]), tensor(11)),\n",
       " (25, tensor([-0.3360, -0.3813,  0.3520,  0.9893]), tensor(11)),\n",
       " (25, tensor([-0.4480, -0.6053,  0.3160,  0.1893]), tensor(11)),\n",
       " (25, tensor([ 0.1680, -0.4080,  1.0000,  0.7280]), tensor(11)),\n",
       " (26, tensor([-0.7600, -0.0933,  0.5240,  0.9093]), tensor(14)),\n",
       " (26, tensor([-0.3720, -0.9467,  0.4440,  0.7013]), tensor(14)),\n",
       " (27, tensor([-1.0000, -0.6720,  0.7778,  1.0000]), tensor(14)),\n",
       " (27, tensor([0.3153, 0.4960, 0.6517, 0.8880]), tensor(4)),\n",
       " (28, tensor([-0.3480, -0.1317,  0.2800,  0.8932]), tensor(7)),\n",
       " (28, tensor([-1., -1.,  1.,  1.]), tensor(8)),\n",
       " (29, tensor([0.0320, 0.1253, 0.1360, 0.6533]), tensor(14)),\n",
       " (29, tensor([-0.1040,  0.1413,  0.0080,  0.6533]), tensor(14)),\n",
       " (29, tensor([-1.0000, -1.0000,  0.7880,  0.5947]), tensor(0)),\n",
       " (29, tensor([ 0.7680, -0.0240,  0.8720,  0.1360]), tensor(19)),\n",
       " (30, tensor([0.2400, 0.4507, 0.5320, 0.8347]), tensor(2)),\n",
       " (30, tensor([ 0.0200, -0.3067,  0.2360, -0.0880]), tensor(2)),\n",
       " (30, tensor([0.8400, 0.0507, 1.0000, 0.2960]), tensor(2)),\n",
       " (30, tensor([-0.5760, -1.0000, -0.2560, -0.8880]), tensor(2)),\n",
       " (31, tensor([-0.9240, -0.7600,  1.0000,  0.8840]), tensor(18)),\n",
       " (32, tensor([-0.0640, -0.2553,  0.7440,  0.4955]), tensor(0)),\n",
       " (32, tensor([-1.0000, -1.0000,  0.2920,  0.9339]), tensor(0)),\n",
       " (33, tensor([-0.9480, -0.5916,  0.9480,  0.3153]), tensor(0)),\n",
       " (34, tensor([-1.0000,  0.1560, -0.3173,  0.6280]), tensor(11)),\n",
       " (34, tensor([-0.5040, -0.9200,  1.0000,  0.9640]), tensor(14)),\n",
       " (35, tensor([-0.0720, -0.1627,  0.1880,  0.3600]), tensor(14)),\n",
       " (35, tensor([-0.3960, -1.0000, -0.2240, -0.2480]), tensor(14)),\n",
       " (35, tensor([ 0.1960, -0.8187,  0.4040, -0.3760]), tensor(8)),\n",
       " (35, tensor([-0.1880, -0.8080,  0.0520, -0.2533]), tensor(8)),\n",
       " (35, tensor([-0.2680, -0.7600,  0.1360, -0.3173]), tensor(10)),\n",
       " (36, tensor([-1.0000, -1.0000, -0.8040, -0.4560]), tensor(4)),\n",
       " (36, tensor([-0.6240, -0.9680,  0.3600,  0.8187]), tensor(14)),\n",
       " (36, tensor([-0.8240, -1.0000,  0.9960,  0.9947]), tensor(14)),\n",
       " (37, tensor([-0.7960, -0.3874,  1.0000,  0.9459]), tensor(6)),\n",
       " (37, tensor([ 0.6000, -0.6817,  0.7880, -0.5015]), tensor(6)),\n",
       " (37, tensor([ 0.4040, -0.6396,  0.5760, -0.4895]), tensor(6)),\n",
       " (38, tensor([-0.8040, -0.4560,  0.6880,  0.7547]), tensor(6)),\n",
       " (38, tensor([ 0.0960, -0.4987,  0.4720,  0.7867]), tensor(14)),\n",
       " (39, tensor([-1., -1.,  1.,  1.]), tensor(14)),\n",
       " (39, tensor([-1.0000, -0.5893,  0.6640,  1.0000]), tensor(11)),\n",
       " (40, tensor([-0.7040, -0.9680,  0.5480,  0.8827]), tensor(13)),\n",
       " (41, tensor([-0.9840, -0.4639,  0.9880,  0.5663]), tensor(0)),\n",
       " (42, tensor([-0.7080, -0.2107, -0.1760,  0.3973]), tensor(7)),\n",
       " (42, tensor([-0.0800, -0.9253,  0.5760,  0.5840]), tensor(11)),\n",
       " (43, tensor([-0.7734, -0.7720,  0.6209,  0.7120]), tensor(11)),\n",
       " (44, tensor([ 0.4160, -0.1147,  0.9200,  0.9947]), tensor(14)),\n",
       " (44, tensor([ 0.3640, -0.0933,  0.6320,  0.4187]), tensor(14)),\n",
       " (44, tensor([0.0480, 0.0453, 0.4440, 0.9840]), tensor(14)),\n",
       " (44, tensor([-0.4760, -0.0507,  0.0640,  0.9840]), tensor(14)),\n",
       " (44, tensor([-0.8680, -0.0080, -0.4600,  0.8613]), tensor(14)),\n",
       " (44, tensor([-0.0520, -0.1040,  0.1800,  0.3440]), tensor(14)),\n",
       " (44, tensor([-0.4080,  0.1253, -0.2880,  0.3600]), tensor(14)),\n",
       " (44, tensor([-0.6120,  0.2800,  0.6480,  0.9893]), tensor(10)),\n",
       " (44, tensor([0.1960, 0.5947, 0.6160, 0.9947]), tensor(8)),\n",
       " (44, tensor([-0.3560,  0.7333,  0.1320,  1.0000]), tensor(8)),\n",
       " (44, tensor([0.7000, 0.6213, 1.0000, 1.0000]), tensor(8)),\n",
       " (44, tensor([-0.9960,  0.3760, -0.6000,  0.9947]), tensor(8)),\n",
       " (45, tensor([ 0.1280, -0.6506,  0.5560,  0.7410]), tensor(14)),\n",
       " (45, tensor([-0.7040, -0.3614, -0.2400,  0.7530]), tensor(12)),\n",
       " (46, tensor([-0.8960,  0.2480, -0.7520,  0.6000]), tensor(13)),\n",
       " (46, tensor([-0.6160,  0.2160, -0.4800,  0.6160]), tensor(13)),\n",
       " (46, tensor([-0.7480,  0.1467, -0.6520,  0.5680]), tensor(14)),\n",
       " (46, tensor([-0.6840,  0.1093, -0.5640,  0.6000]), tensor(14)),\n",
       " (46, tensor([-0.2080,  0.0720, -0.0640,  0.7013]), tensor(14)),\n",
       " (46, tensor([0.5000, 0.0987, 0.6080, 0.5520]), tensor(14)),\n",
       " (46, tensor([0.7520, 0.1467, 0.8440, 0.3227]), tensor(14)),\n",
       " (46, tensor([-0.3920,  0.2747, -0.2720,  0.6160]), tensor(13)),\n",
       " (46, tensor([-0.3120,  0.2427, -0.1920,  0.5520]), tensor(13)),\n",
       " (46, tensor([0.7400, 0.3173, 0.8600, 0.5893]), tensor(13)),\n",
       " (46, tensor([-0.3080,  0.1573, -0.2440,  0.3280]), tensor(14)),\n",
       " (46, tensor([-0.0440,  0.1840,  0.4240,  0.7280]), tensor(6)),\n",
       " (47, tensor([ 0.0200, -0.2213,  1.0000,  0.4613]), tensor(5)),\n",
       " (47, tensor([0.3600, 0.0987, 1.0000, 1.0000]), tensor(6)),\n",
       " (47, tensor([-0.2200,  0.1893, -0.0680,  0.3387]), tensor(6)),\n",
       " (47, tensor([-0.3000,  0.1893, -0.2160,  0.2853]), tensor(6)),\n",
       " (47, tensor([-0.4480,  0.1840, -0.4000,  0.4187]), tensor(14)),\n",
       " (47, tensor([-0.5240,  0.1840, -0.4600,  0.4133]), tensor(14)),\n",
       " (47, tensor([-0.5880,  0.2000, -0.5240,  0.4187]), tensor(14)),\n",
       " (47, tensor([-0.6800,  0.1627, -0.6000,  0.4027]), tensor(14)),\n",
       " (48, tensor([-0.3000, -0.6157,  1.0000,  0.9929]), tensor(5)),\n",
       " (48, tensor([-0.7840,  0.6797, -0.3800,  0.9359]), tensor(6)),\n",
       " (48, tensor([-0.9840,  0.8221, -0.8200,  0.9217]), tensor(6)),\n",
       " (49, tensor([-0.9960, -1.0000,  1.0000,  1.0000]), tensor(15)),\n",
       " (50, tensor([-0.9160, -0.9947,  0.7040,  0.5520]), tensor(7)),\n",
       " (51, tensor([-0.3880, -0.7305,  0.7600,  0.4790]), tensor(6)),\n",
       " (52, tensor([-0.3441,  0.0760,  0.8712,  0.9960]), tensor(8)),\n",
       " (52, tensor([-0.9557, -0.2920, -0.3119,  0.6760]), tensor(8)),\n",
       " (52, tensor([-0.6298,  0.0440,  1.0000,  1.0000]), tensor(10)),\n",
       " (52, tensor([ 0.6459, -0.2440,  0.8994, -0.0440]), tensor(8)),\n",
       " (52, tensor([-0.5815, -0.5160,  0.0302,  0.3560]), tensor(14)),\n",
       " (52, tensor([-0.3602, -0.1960,  0.7545,  0.9960]), tensor(14)),\n",
       " (52, tensor([ 0.4004, -0.3760,  0.7223,  0.1040]), tensor(14)),\n",
       " (53, tensor([-0.5040, -0.7724,  0.6480,  0.9295]), tensor(11)),\n",
       " (53, tensor([-0.1160, -1.0000,  0.3200,  0.4526]), tensor(14)),\n",
       " (54, tensor([ 0.5840, -0.1064,  0.8000,  0.5234]), tensor(8)),\n",
       " (54, tensor([-0.2480, -0.1234,  0.2360,  0.6085]), tensor(17)),\n",
       " (54, tensor([-0.4640,  0.2426,  0.4120,  1.0000]), tensor(17)),\n",
       " (55, tensor([-0.7669, -0.6840, -0.0613, -0.1600]), tensor(1)),\n",
       " (55, tensor([-0.8589,  0.0080, -0.0798,  0.7160]), tensor(1)),\n",
       " (55, tensor([-0.5767, -0.9480,  0.1104, -0.2600]), tensor(14)),\n",
       " (55, tensor([-0.8773, -0.4560,  0.2822,  0.4600]), tensor(14)),\n",
       " (56, tensor([-0.8600, -0.7917,  0.7720,  0.8281]), tensor(13)),\n",
       " (56, tensor([-0.8440, -0.8438, -0.0440,  0.6771]), tensor(14)),\n",
       " (57, tensor([0.4840, 0.0880, 1.0000, 0.5893]), tensor(6)),\n",
       " (57, tensor([-0.9480, -0.1573, -0.7880, -0.0133]), tensor(6)),\n",
       " (57, tensor([-0.6840, -0.3973,  0.3680,  0.2853]), tensor(5)),\n",
       " (58, tensor([-0.8640, -0.8773,  0.9960,  1.0000]), tensor(7)),\n",
       " (59, tensor([-0.4347,  0.2840,  0.6000,  1.0000]), tensor(8)),\n",
       " (60, tensor([-0.8360, -0.3387, -0.1880,  0.7600]), tensor(11)),\n",
       " (60, tensor([-0.2240, -0.2800,  0.1800,  0.5360]), tensor(11)),\n",
       " (60, tensor([ 0.1280, -0.4027,  0.9840,  0.2853]), tensor(11)),\n",
       " (60, tensor([-1.0000, -0.9947,  1.0000,  0.7973]), tensor(17)),\n",
       " (61, tensor([-0.7760, -0.7143,  0.5960,  0.8935]), tensor(7)),\n",
       " (62, tensor([-0.5195, -0.6360,  0.4294,  0.9440]), tensor(11)),\n",
       " (63, tensor([-0.9288, -0.7919,  0.9847,  0.9364]), tensor(12))]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_gts = list(zip(gt_idxs, *unp_flat))\n",
    "batch_gts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### get AP per cls (one batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1312)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_preds, batch_gts = format_inps(acts, batch, anchors, box_size, iou=.5, conf=.3, device='cpu')\n",
    "aps, pred_bbs, tpfpfns = ap_per_cls(batch_preds, batch_gts)\n",
    "sum(aps) / len(aps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2, 2, 1, 0, 0, 1, 6, 4, 3, 0, 1, 4, 0, 1, 13, 1, 0, 0, 0, 0],\n",
       " [0, 2, 1, 1, 1, 0, 16, 3, 14, 0, 0, 8, 1, 1, 41, 3, 0, 0, 4, 0],\n",
       " [4, 3, 4, 3, 3, 3, 10, 3, 17, 0, 4, 17, 6, 6, 41, 1, 0, 5, 4, 3])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tpfpfns contains three lists: tps, fps, and fns per class\n",
    "tpfpfns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(n_preds, sum(tpfpfns[0])+sum(tpfpfns[1])) # equals the number of preds i found before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "### stopped here ### :3\n",
    "####################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ssd item loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Item Loss\n",
    "#assignments\n",
    "act_bbs, act_lbls, bbs, lbls = abb,albl,bb,lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0411, 0.1417, 0.7763, 0.9708],\n",
       "         [0.2391, 0.1438, 0.6067, 0.5792],\n",
       "         [0.5219, 0.5979, 0.7661, 0.8542],\n",
       "         [0.4987, 0.5583, 0.6864, 0.7396]]),\n",
       " tensor([12, 14, 12, 14]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbs,lbls = remove_padding(bbs,lbls)                      # remove gt padding inserted during training\n",
    "bbs,lbls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([189, 4]),\n",
       " tensor([[ 0.0388,  0.0548,  0.1889,  0.1897],\n",
       "         [ 0.0350,  0.0752,  0.1641,  0.1768],\n",
       "         [ 0.0933,  0.0817,  0.1777,  0.2160],\n",
       "         [ 0.0510,  0.0223,  0.2184,  0.1888],\n",
       "         [ 0.0154,  0.0164,  0.1592,  0.1482],\n",
       "         [ 0.0404,  0.0571,  0.1333,  0.2073],\n",
       "         [ 0.0240,  0.0167,  0.2563,  0.2961],\n",
       "         [ 0.0206,  0.0685,  0.2763,  0.1975],\n",
       "         [ 0.0332,  0.0305,  0.1772,  0.2297],\n",
       "         [ 0.2547,  0.0517,  0.4217,  0.2185],\n",
       "         [ 0.2628,  0.0641,  0.3889,  0.1662],\n",
       "         [ 0.3562,  0.0807,  0.4213,  0.2188],\n",
       "         [ 0.3027,  0.0767,  0.4978,  0.3198],\n",
       "         [ 0.2346,  0.0788,  0.3736,  0.1637],\n",
       "         [ 0.2768,  0.0581,  0.4048,  0.2178],\n",
       "         [ 0.3098,  0.0982,  0.5987,  0.3073],\n",
       "         [ 0.3044,  0.0633,  0.4876,  0.1915],\n",
       "         [ 0.3120,  0.0664,  0.4563,  0.2837],\n",
       "         [ 0.5207,  0.0499,  0.6875,  0.2086],\n",
       "         [ 0.5446,  0.0614,  0.6481,  0.1699],\n",
       "         [ 0.5673,  0.0866,  0.6437,  0.1994],\n",
       "         [ 0.5019,  0.0709,  0.6905,  0.3065],\n",
       "         [ 0.5589,  0.0361,  0.7089,  0.1462],\n",
       "         [ 0.5982,  0.0624,  0.7092,  0.2008],\n",
       "         [ 0.4311,  0.0762,  0.7122,  0.3049],\n",
       "         [ 0.4661,  0.0896,  0.6357,  0.1946],\n",
       "         [ 0.5446,  0.0949,  0.7208,  0.3134],\n",
       "         [ 0.8033,  0.0691,  0.9454,  0.2047],\n",
       "         [ 0.7919,  0.0862,  0.9143,  0.2057],\n",
       "         [ 0.8172,  0.0658,  0.9012,  0.1898],\n",
       "         [ 0.7473,  0.0219,  0.9430,  0.2222],\n",
       "         [ 0.7913,  0.0487,  0.9788,  0.1634],\n",
       "         [ 0.8397,  0.0504,  0.9419,  0.2113],\n",
       "         [ 0.7671,  0.0146,  1.0219,  0.2824],\n",
       "         [ 0.7340,  0.0779,  0.9361,  0.1958],\n",
       "         [ 0.7813,  0.0659,  0.9407,  0.2742],\n",
       "         [ 0.0127,  0.1886,  0.1819,  0.4079],\n",
       "         [ 0.0012,  0.3261,  0.1622,  0.4419],\n",
       "         [ 0.0932,  0.2686,  0.1560,  0.4995],\n",
       "         [ 0.0232,  0.2063,  0.1885,  0.5022],\n",
       "         [ 0.0144,  0.2883,  0.1570,  0.4613],\n",
       "         [ 0.0736,  0.2378,  0.1620,  0.4735],\n",
       "         [ 0.0321,  0.2135,  0.3003,  0.5748],\n",
       "         [ 0.0296,  0.2650,  0.3164,  0.4815],\n",
       "         [ 0.0225,  0.2118,  0.1729,  0.5082],\n",
       "         [ 0.2237,  0.1854,  0.3964,  0.4633],\n",
       "         [ 0.2497,  0.2551,  0.4359,  0.3900],\n",
       "         [ 0.3525,  0.2215,  0.4180,  0.4825],\n",
       "         [ 0.3030,  0.1923,  0.5865,  0.5584],\n",
       "         [ 0.2604,  0.2759,  0.3994,  0.4559],\n",
       "         [ 0.2681,  0.1316,  0.4353,  0.4910],\n",
       "         [ 0.2586,  0.1340,  0.6677,  0.5977],\n",
       "         [ 0.2728,  0.2454,  0.5700,  0.4881],\n",
       "         [ 0.2834,  0.0731,  0.4998,  0.5188],\n",
       "         [ 0.5143,  0.2014,  0.7167,  0.4733],\n",
       "         [ 0.5539,  0.2849,  0.6873,  0.4236],\n",
       "         [ 0.5693,  0.1938,  0.6872,  0.4484],\n",
       "         [ 0.4515,  0.1978,  0.7501,  0.5676],\n",
       "         [ 0.5605,  0.2885,  0.7010,  0.4750],\n",
       "         [ 0.5295,  0.1488,  0.6960,  0.4968],\n",
       "         [ 0.3473,  0.1449,  0.7582,  0.6104],\n",
       "         [ 0.4169,  0.2598,  0.6197,  0.5013],\n",
       "         [ 0.4548,  0.0939,  0.6863,  0.5299],\n",
       "         [ 0.7922,  0.2416,  0.9647,  0.4335],\n",
       "         [ 0.8474,  0.3162,  0.9613,  0.4513],\n",
       "         [ 0.8023,  0.2257,  0.9121,  0.4212],\n",
       "         [ 0.7795,  0.2209,  0.9732,  0.5016],\n",
       "         [ 0.7994,  0.2795,  0.9405,  0.4568],\n",
       "         [ 0.8207,  0.2209,  0.9326,  0.4961],\n",
       "         [ 0.7121,  0.1734,  0.9864,  0.5724],\n",
       "         [ 0.6744,  0.2813,  0.8901,  0.4826],\n",
       "         [ 0.7670,  0.2051,  0.9606,  0.4886],\n",
       "         [ 0.0022,  0.4760,  0.2136,  0.7257],\n",
       "         [-0.0159,  0.6075,  0.1870,  0.6946],\n",
       "         [ 0.0693,  0.5237,  0.1407,  0.7682],\n",
       "         [-0.0114,  0.4424,  0.2083,  0.7362],\n",
       "         [ 0.0522,  0.5962,  0.2336,  0.7702],\n",
       "         [ 0.1007,  0.5530,  0.2254,  0.7591],\n",
       "         [-0.0136,  0.3794,  0.2959,  0.7623],\n",
       "         [-0.0249,  0.4994,  0.2389,  0.7113],\n",
       "         [ 0.0259,  0.4483,  0.1854,  0.7612],\n",
       "         [ 0.2401,  0.4674,  0.4850,  0.7449],\n",
       "         [ 0.2219,  0.5537,  0.4862,  0.6607],\n",
       "         [ 0.3496,  0.4991,  0.4214,  0.7679],\n",
       "         [ 0.1986,  0.3819,  0.5298,  0.7240],\n",
       "         [ 0.3096,  0.5944,  0.4862,  0.7694],\n",
       "         [ 0.3068,  0.4449,  0.4789,  0.7855],\n",
       "         [ 0.1795,  0.3173,  0.6417,  0.7603],\n",
       "         [ 0.1899,  0.4981,  0.5663,  0.7376],\n",
       "         [ 0.2669,  0.3048,  0.4829,  0.7391],\n",
       "         [ 0.5036,  0.4411,  0.7520,  0.7151],\n",
       "         [ 0.5072,  0.5370,  0.7583,  0.6481],\n",
       "         [ 0.6343,  0.4474,  0.7487,  0.7041],\n",
       "         [ 0.4178,  0.3885,  0.7434,  0.7549],\n",
       "         [ 0.5812,  0.5631,  0.7833,  0.7494],\n",
       "         [ 0.5423,  0.3955,  0.7100,  0.7282],\n",
       "         [ 0.3727,  0.3092,  0.8404,  0.7726],\n",
       "         [ 0.4058,  0.5082,  0.7003,  0.7497],\n",
       "         [ 0.5119,  0.3219,  0.7382,  0.7504],\n",
       "         [ 0.7298,  0.4557,  0.9612,  0.6762],\n",
       "         [ 0.7834,  0.5396,  0.9873,  0.6515],\n",
       "         [ 0.8495,  0.4775,  0.9519,  0.6860],\n",
       "         [ 0.7853,  0.4325,  1.0066,  0.7604],\n",
       "         [ 0.7448,  0.5752,  0.9432,  0.7567],\n",
       "         [ 0.8225,  0.4310,  0.9281,  0.7121],\n",
       "         [ 0.6628,  0.3625,  1.0100,  0.7594],\n",
       "         [ 0.6930,  0.5225,  0.9497,  0.7379],\n",
       "         [ 0.7655,  0.4234,  0.9452,  0.7409],\n",
       "         [ 0.0100,  0.8115,  0.2081,  0.9788],\n",
       "         [ 0.0470,  0.8403,  0.2079,  0.9253],\n",
       "         [ 0.0734,  0.8171,  0.1566,  0.9773],\n",
       "         [-0.0156,  0.7228,  0.2079,  0.9476],\n",
       "         [-0.0028,  0.8209,  0.2038,  0.9306],\n",
       "         [ 0.0687,  0.7929,  0.1906,  0.9693],\n",
       "         [-0.0140,  0.7159,  0.2669,  0.9942],\n",
       "         [-0.0429,  0.8121,  0.2354,  0.9452],\n",
       "         [ 0.0167,  0.7495,  0.1660,  0.9712],\n",
       "         [ 0.2443,  0.8513,  0.4558,  1.0214],\n",
       "         [ 0.2416,  0.8294,  0.4627,  0.8932],\n",
       "         [ 0.3362,  0.8314,  0.4007,  0.9961],\n",
       "         [ 0.1985,  0.7671,  0.4816,  0.9325],\n",
       "         [ 0.2369,  0.8632,  0.4972,  0.9455],\n",
       "         [ 0.3404,  0.7828,  0.4775,  0.9844],\n",
       "         [ 0.1674,  0.7072,  0.5892,  0.9950],\n",
       "         [ 0.1283,  0.8547,  0.5465,  0.9740],\n",
       "         [ 0.2995,  0.7260,  0.4251,  0.9489],\n",
       "         [ 0.5179,  0.8105,  0.7509,  0.9970],\n",
       "         [ 0.4918,  0.8453,  0.6995,  0.9120],\n",
       "         [ 0.6168,  0.8465,  0.6964,  0.9957],\n",
       "         [ 0.4590,  0.7457,  0.7520,  0.9666],\n",
       "         [ 0.4970,  0.8494,  0.7791,  0.9598],\n",
       "         [ 0.6002,  0.7832,  0.7247,  0.9748],\n",
       "         [ 0.4173,  0.6652,  0.8536,  0.9888],\n",
       "         [ 0.4628,  0.8412,  0.8802,  0.9623],\n",
       "         [ 0.5333,  0.7380,  0.6887,  1.0218],\n",
       "         [ 0.7752,  0.7847,  0.9802,  0.9610],\n",
       "         [ 0.7741,  0.8599,  0.9559,  0.9436],\n",
       "         [ 0.8581,  0.8175,  0.9477,  0.9702],\n",
       "         [ 0.7799,  0.7833,  1.0227,  0.9980],\n",
       "         [ 0.7475,  0.8285,  1.0076,  0.9487],\n",
       "         [ 0.8498,  0.8013,  0.9645,  1.0067],\n",
       "         [ 0.7276,  0.6933,  1.0484,  0.9973],\n",
       "         [ 0.7497,  0.8142,  1.1062,  0.9602],\n",
       "         [ 0.7891,  0.7576,  0.9534,  1.0064],\n",
       "         [ 0.1282,  0.0863,  0.5074,  0.6079],\n",
       "         [ 0.1581,  0.1281,  0.4034,  0.3603],\n",
       "         [ 0.1691,  0.1730,  0.2816,  0.4409],\n",
       "         [ 0.1223,  0.0361,  0.6066,  0.6742],\n",
       "         [ 0.1406,  0.1164,  0.4966,  0.4614],\n",
       "         [ 0.1382,  0.0698,  0.4033,  0.5754],\n",
       "         [ 0.0279,  0.0319,  0.6322,  0.8563],\n",
       "         [ 0.1130,  0.0606,  0.6313,  0.5054],\n",
       "         [ 0.1938,  0.0304,  0.5512,  0.6287],\n",
       "         [ 0.5838,  0.1177,  0.9371,  0.5954],\n",
       "         [ 0.5371,  0.1014,  0.8249,  0.3223],\n",
       "         [ 0.6572,  0.1499,  0.8064,  0.5389],\n",
       "         [ 0.4392,  0.0107,  0.9248,  0.6788],\n",
       "         [ 0.4944,  0.0894,  0.8145,  0.4010],\n",
       "         [ 0.5785,  0.1072,  0.8446,  0.5813],\n",
       "         [ 0.2924,  0.1239,  1.0111,  0.7559],\n",
       "         [ 0.3244,  0.0737,  0.9028,  0.5302],\n",
       "         [ 0.5081,  0.0670,  0.8731,  0.6680],\n",
       "         [ 0.1015,  0.4148,  0.4430,  0.9230],\n",
       "         [ 0.2018,  0.6372,  0.4391,  0.8898],\n",
       "         [ 0.1419,  0.4961,  0.3015,  0.9389],\n",
       "         [ 0.0171,  0.2549,  0.6502,  0.9470],\n",
       "         [ 0.1065,  0.5192,  0.4859,  0.8822],\n",
       "         [ 0.1585,  0.3107,  0.4279,  0.8880],\n",
       "         [ 0.0293,  0.3144,  0.7337,  0.9836],\n",
       "         [ 0.1057,  0.4535,  0.6144,  0.9307],\n",
       "         [ 0.1098,  0.2563,  0.5227,  0.8886],\n",
       "         [ 0.4807,  0.4687,  0.9527,  0.9575],\n",
       "         [ 0.5600,  0.6554,  0.7968,  0.8677],\n",
       "         [ 0.6531,  0.5706,  0.8743,  0.9641],\n",
       "         [ 0.2980,  0.3210,  0.9530,  0.9889],\n",
       "         [ 0.5432,  0.5502,  1.0482,  0.9035],\n",
       "         [ 0.6543,  0.2687,  0.8968,  0.9091],\n",
       "         [ 0.2314,  0.3258,  0.9369,  1.0441],\n",
       "         [ 0.4257,  0.4230,  0.9889,  0.9073],\n",
       "         [ 0.5309,  0.2381,  0.9585,  0.9061],\n",
       "         [ 0.2316,  0.1569,  0.7753,  0.8916],\n",
       "         [ 0.2387,  0.2643,  0.7995,  0.8124],\n",
       "         [ 0.2832,  0.1187,  0.7130,  0.8487],\n",
       "         [ 0.1758,  0.1466,  0.8435,  0.9411],\n",
       "         [ 0.1773,  0.1562,  0.8298,  0.8710],\n",
       "         [ 0.2212,  0.1469,  0.7511,  0.8914],\n",
       "         [ 0.0297,  0.0615,  0.9400,  1.0003],\n",
       "         [ 0.1440,  0.1617,  0.9071,  0.9072],\n",
       "         [ 0.2168,  0.1404,  0.7985,  0.9413]], grad_fn=<CatBackward>))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_bbs = get_pred_bbs(act_bbs, anchors, box_size, device) # make 196 pred bbs from acts and ancs\n",
    "pred_bbs.shape, pred_bbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 189]),\n",
       " tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 1.6586e-04, 0.0000e+00, 0.0000e+00,\n",
       "          4.6010e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 4.1484e-05, 0.0000e+00, 0.0000e+00,\n",
       "          1.1503e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0372e-05, 0.0000e+00, 0.0000e+00,\n",
       "          2.8756e-05, 0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0771e-05, 0.0000e+00, 0.0000e+00,\n",
       "          1.2081e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 2.6930e-06, 0.0000e+00, 0.0000e+00,\n",
       "          3.0202e-05, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 6.7329e-07, 0.0000e+00, 0.0000e+00,\n",
       "          7.5504e-06, 0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 7.9822e-05, 0.0000e+00, 0.0000e+00,\n",
       "          4.7232e-05, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 1.9955e-05, 0.0000e+00, 0.0000e+00,\n",
       "          1.1808e-05, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 4.9889e-06, 0.0000e+00, 0.0000e+00,\n",
       "          2.9520e-06, 0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 4.3087e-05, 0.0000e+00, 0.0000e+00,\n",
       "          2.5671e-05, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0772e-05, 0.0000e+00, 0.0000e+00,\n",
       "          6.4178e-06, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 2.6930e-06, 0.0000e+00, 0.0000e+00,\n",
       "          1.6045e-06, 0.0000e+00, 0.0000e+00]]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map gt to preds\n",
    "iou_gt_grid = get_iou(bbs.data, anchor_boxes.data)       # get iou(gt_bbs,anc_bbs); used to map gt → ancs\n",
    "iou_gt_grid.shape, iou_gt_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 1.9900e+00, 0.0000e+00, 0.0000e+00,\n",
       "         1.9900e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 4.1484e-05, 0.0000e+00, 0.0000e+00,\n",
       "         1.1503e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0372e-05, 0.0000e+00, 0.0000e+00,\n",
       "         2.8756e-05, 0.0000e+00, 0.0000e+00]),\n",
       " tensor([0, 0, 0, 3, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iou_gt_preds, mapped_gt_idx = map_to_gt(iou_gt_grid)     # assign each pred an index of a gt object\n",
    "iou_gt_preds, mapped_gt_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([189]),\n",
       " tensor([12, 12, 12, 14, 12, 12, 14, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "         12, 12, 12, 12, 12, 12, 12, 12, 12]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapped_lbls = lbls[mapped_gt_idx]                        # project gt lbls into pred space\n",
    "mapped_lbls.shape, mapped_lbls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False,  True, False, False,  True, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove low-iou bb preds & set mapped lbl to bg\n",
    "high_iou = iou_gt_preds > 0.4                            # only include bb preds that overlap w/a gt obj and\n",
    "high_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 6])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incl = torch.nonzero(high_iou)[:,0]                      #  are not predicting background\n",
    "incl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "        20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "        20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 14, 20, 20,\n",
       "        20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "        20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "        20, 20, 20, 14, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "        20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "        20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "        20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "        20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "        20, 20, 20, 20, 20, 20, 20, 20, 20])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapped_lbls[~high_iou] = dls.ncls                        # assign gt class of bg to preds w/ low max gt iou\n",
    "mapped_lbls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([189, 4]),\n",
       " tensor([[-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-1.1476e-05,  5.2083e-04,  1.6641e-03,  2.1391e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-2.3297e-03, -3.1808e-03,  9.5253e-04,  7.0685e-04],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03],\n",
       "         [-4.0970e-03, -3.1994e-03,  2.4674e-03,  4.2039e-03]]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapped_bbs  = bbs[mapped_gt_idx]                         # project gt bbs into pred space\n",
    "mapped_bbs.shape, mapped_bbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1364, grad_fn=<L1LossBackward>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute loss\n",
    "bb_res  = F.l1_loss(pred_bbs[incl], mapped_bbs[incl])\n",
    "bb_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.1986, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbl_res = loss_f(act_lbls.cpu(), mapped_lbls.cpu())\n",
    "lbl_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lbl loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lbl_loss\n",
    "#assignments\n",
    "acts, targs = act_lbls.cpu(), mapped_lbls.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([189, 21]),\n",
       " tensor([[0., 0., 0.,  ..., 0., 0., 1.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 1.]]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = one_hot_embedding(targs, dls.ncls+1, device)\n",
    "t.shape, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([189, 20]),\n",
       " tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tensor(t[:,:-1].contiguous())\n",
    "t.shape, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([189, 20]),\n",
       " tensor([[-3.9891, -4.1691, -4.0815,  ..., -4.1617, -3.9448, -4.2141],\n",
       "         [-3.9118, -3.8588, -4.1509,  ..., -3.9471, -4.1068, -4.4997],\n",
       "         [-3.9061, -4.0771, -4.0870,  ..., -4.1831, -3.9755, -3.9879],\n",
       "         ...,\n",
       "         [-3.9870, -4.0060, -3.9948,  ..., -3.9976, -4.0073, -3.9924],\n",
       "         [-3.9988, -3.9908, -4.0037,  ..., -4.0180, -4.0328, -3.9996],\n",
       "         [-4.0175, -4.0025, -4.0115,  ..., -3.9883, -4.0004, -3.9869]],\n",
       "        grad_fn=<SliceBackward>))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = acts[:,:-1]\n",
    "a.shape, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weight(a, t):\n",
    "    alpha, gamma = 0.25, 2.0 # vals from paper\n",
    "    p = a.sigmoid()\n",
    "    pt = p*t + (1-p)*(1-t)\n",
    "    w = alpha*t + (1-alpha)*(1-t)\n",
    "    return w * (1-pt).pow(gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([189, 20]),\n",
       " tensor([[2.4787e-04, 1.7398e-04, 2.0673e-04,  ..., 1.7655e-04, 2.7037e-04,\n",
       "          1.5923e-04],\n",
       "         [2.8849e-04, 3.2002e-04, 1.8033e-04,  ..., 2.6919e-04, 1.9670e-04,\n",
       "          9.0596e-05],\n",
       "         [2.9171e-04, 2.0851e-04, 2.0448e-04,  ..., 1.6924e-04, 2.5457e-04,\n",
       "          2.4844e-04],\n",
       "         ...,\n",
       "         [2.4890e-04, 2.3979e-04, 2.4511e-04,  ..., 2.4376e-04, 2.3919e-04,\n",
       "          2.4626e-04],\n",
       "         [2.4322e-04, 2.4704e-04, 2.4087e-04,  ..., 2.3420e-04, 2.2750e-04,\n",
       "          2.4284e-04],\n",
       "         [2.3443e-04, 2.4144e-04, 2.3722e-04,  ..., 2.4827e-04, 2.4243e-04,\n",
       "          2.4896e-04]]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = get_weight(a,t).detach()\n",
    "w.shape, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

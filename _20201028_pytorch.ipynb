{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Tensors\" data-toc-modified-id=\"Tensors-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Tensors</a></span><ul class=\"toc-item\"><li><span><a href=\"#Multi-dimensional-matricies,-single-data-type\" data-toc-modified-id=\"Multi-dimensional-matricies,-single-data-type-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Multi-dimensional matricies, single data type</a></span></li><li><span><a href=\"#shape-is-a-tensor's-most-important-attribute\" data-toc-modified-id=\"shape-is-a-tensor's-most-important-attribute-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span><code>shape</code> is a tensor's most important attribute</a></span></li><li><span><a href=\"#stride-tells-us-where-a-tensor's-data-is-stored\" data-toc-modified-id=\"stride-tells-us-where-a-tensor's-data-is-stored-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span><code>stride</code> tells us where a tensor's <code>data</code> is stored</a></span></li><li><span><a href=\"#views-are-tensors-that-are-paired-to-another-tensor's-data\" data-toc-modified-id=\"views-are-tensors-that-are-paired-to-another-tensor's-data-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span><code>view</code>s are tensors that are paired to another tensor's <code>data</code></a></span></li><li><span><a href=\"#Looking-at-tensor-attributes-in-PyTorch\" data-toc-modified-id=\"Looking-at-tensor-attributes-in-PyTorch-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Looking at tensor attributes in PyTorch</a></span></li></ul></li><li><span><a href=\"#Tensor-Manipulation\" data-toc-modified-id=\"Tensor-Manipulation-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Tensor Manipulation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Creating-Tensors\" data-toc-modified-id=\"Creating-Tensors-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Creating Tensors</a></span><ul class=\"toc-item\"><li><span><a href=\"#torch.tensor()-and-torch.Tensor()\" data-toc-modified-id=\"torch.tensor()-and-torch.Tensor()-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span><code>torch.tensor()</code> and <code>torch.Tensor()</code></a></span></li><li><span><a href=\"#torch.empty(shape)\" data-toc-modified-id=\"torch.empty(shape)-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span><code>torch.empty(shape)</code></a></span></li><li><span><a href=\"#torch.zeros(sh)-/-.ones(sh)\" data-toc-modified-id=\"torch.zeros(sh)-/-.ones(sh)-2.1.3\"><span class=\"toc-item-num\">2.1.3&nbsp;&nbsp;</span><code>torch.zeros(sh)</code> / <code>.ones(sh)</code></a></span></li><li><span><a href=\"#torch.arange(num)\" data-toc-modified-id=\"torch.arange(num)-2.1.4\"><span class=\"toc-item-num\">2.1.4&nbsp;&nbsp;</span><code>torch.arange(num)</code></a></span></li><li><span><a href=\"#torch.rand(sh)-/-.randn(sh)-/-.randint(sh)\" data-toc-modified-id=\"torch.rand(sh)-/-.randn(sh)-/-.randint(sh)-2.1.5\"><span class=\"toc-item-num\">2.1.5&nbsp;&nbsp;</span><code>torch.rand(sh)</code> / <code>.randn(sh)</code> / <code>.randint(sh)</code></a></span></li></ul></li><li><span><a href=\"#Viewing-and-Slicing\" data-toc-modified-id=\"Viewing-and-Slicing-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Viewing and Slicing</a></span><ul class=\"toc-item\"><li><span><a href=\"#t[:,-:,-:]\" data-toc-modified-id=\"t[:,-:,-:]-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span><code>t[:, :, :]</code></a></span></li><li><span><a href=\"#t1[t2]\" data-toc-modified-id=\"t1[t2]-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span><code>t1[t2]</code></a></span></li><li><span><a href=\"#t.view(shape)\" data-toc-modified-id=\"t.view(shape)-2.2.3\"><span class=\"toc-item-num\">2.2.3&nbsp;&nbsp;</span><code>t.view(shape)</code></a></span></li></ul></li><li><span><a href=\"#Adding/enlarging-dimensions\" data-toc-modified-id=\"Adding/enlarging-dimensions-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Adding/enlarging dimensions</a></span><ul class=\"toc-item\"><li><span><a href=\"#torch.cat\" data-toc-modified-id=\"torch.cat-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span><code>torch.cat</code></a></span></li><li><span><a href=\"#torch.stack\" data-toc-modified-id=\"torch.stack-2.3.2\"><span class=\"toc-item-num\">2.3.2&nbsp;&nbsp;</span><code>torch.stack</code></a></span></li><li><span><a href=\"#t.repeat(nreps,nreps,nreps,...)\" data-toc-modified-id=\"t.repeat(nreps,nreps,nreps,...)-2.3.3\"><span class=\"toc-item-num\">2.3.3&nbsp;&nbsp;</span><code>t.repeat(nreps,nreps,nreps,...)</code></a></span></li></ul></li><li><span><a href=\"#Removing/shrinking-dimensions\" data-toc-modified-id=\"Removing/shrinking-dimensions-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Removing/shrinking dimensions</a></span><ul class=\"toc-item\"><li><span><a href=\"#torch.squeeze(t)\" data-toc-modified-id=\"torch.squeeze(t)-2.4.1\"><span class=\"toc-item-num\">2.4.1&nbsp;&nbsp;</span><code>torch.squeeze(t)</code></a></span></li><li><span><a href=\"#t.flatten()\" data-toc-modified-id=\"t.flatten()-2.4.2\"><span class=\"toc-item-num\">2.4.2&nbsp;&nbsp;</span><code>t.flatten()</code></a></span></li><li><span><a href=\"#torch.split(t,n,dim=0)\" data-toc-modified-id=\"torch.split(t,n,dim=0)-2.4.3\"><span class=\"toc-item-num\">2.4.3&nbsp;&nbsp;</span><code>torch.split(t,n,dim=0)</code></a></span></li></ul></li></ul></li><li><span><a href=\"#Broadcasting\" data-toc-modified-id=\"Broadcasting-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Broadcasting</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#No-broadcasting-needed-for-same-shape\" data-toc-modified-id=\"No-broadcasting-needed-for-same-shape-3.0.1\"><span class=\"toc-item-num\">3.0.1&nbsp;&nbsp;</span>No broadcasting needed for same shape</a></span></li><li><span><a href=\"#dimsize-pair-has-one-1\" data-toc-modified-id=\"dimsize-pair-has-one-1-3.0.2\"><span class=\"toc-item-num\">3.0.2&nbsp;&nbsp;</span>dimsize pair has one 1</a></span></li><li><span><a href=\"#dimsize-pair-has-one-element\" data-toc-modified-id=\"dimsize-pair-has-one-element-3.0.3\"><span class=\"toc-item-num\">3.0.3&nbsp;&nbsp;</span>dimsize pair has one element</a></span></li><li><span><a href=\"#Example\" data-toc-modified-id=\"Example-3.0.4\"><span class=\"toc-item-num\">3.0.4&nbsp;&nbsp;</span>Example</a></span></li></ul></li></ul></li><li><span><a href=\"#Playground\" data-toc-modified-id=\"Playground-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Playground</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pnl(*args, ns=1): print(*args,sep='\\n'*ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import tensor\n",
    "def T(*args):\n",
    "    e = tensor(args).prod()\n",
    "    return torch.arange(e).view(*args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-dimensional matricies, single data type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Official docs def: \"A `torch.Tensor` is a multi-dimensional matrix containing elements of a single data type.\"\n",
    "([link](https://pytorch.org/docs/stable/tensors.html))\n",
    "\n",
    "Torch defines 10 tensor types with CPU and GPU variants for all (CPU constructor):\n",
    "- 16, 32, 64 bit floats (`torch.float##`, ## is bit size)\n",
    "- 16 bit bfloats (`.bfloat16`)\n",
    "- 32, 64, 128 bit complex (special case re: XPU variants) (`.complex##`)\n",
    "- 8, 16, 32, 64 bit signed ints (`.int##`)\n",
    "- 8 bit unsigned ints (`.uint8`)\n",
    "- boolean (`.bool`)\n",
    "\n",
    "*GPU tensors constructed as follows: `.cuda.type##`*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `shape` is a tensor's most important attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors have shapes that look like tuples, lists, vectors, etc. Ex: (5,2,3). The tuple represents a list of dimension sizes. The length of the tuple is the total number of dimensions (aka the tensor's rank).\n",
    "\n",
    "To visualize the shape of a tensor, I basically visualize a spreadsheet workbook. I start at the \"trailing\" dimension (the right-most one) – 3 in the example above. This is the number of \"columns\" on one page. The next number is the number of rows. The next number is the number of sheets. And, if there was a number above that one, I'd visualize that as the number of workbooks. (If there was one above that, I'd visualize it as the number of folders; from there it's nested folders all the way down).\n",
    "\n",
    "**Ex: visualizing the tensor T(2,2,2,3,4).**\n",
    "\n",
    "From right to left: (4 cols, 3 rows, 2 sheets, 2 workbooks, 2 folders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`\n",
    " \n",
    "               end of first col (4 per row)\n",
    "               ↓ \n",
    "       [[[[[ 0.,  1.,  2.,  3.],     ← end of first row (3 per sheet)\n",
    "           [ 4.,  5.,  6.,  7.],\n",
    "           [ 8.,  9., 10., 11.]],    ← end of sheet (2 per wb)\n",
    "\n",
    "          [[12., 13., 14., 15.],\n",
    "           [16., 17., 18., 19.],\n",
    "           [20., 21., 22., 23.]]],   ← end of wb (2 per folder)\n",
    "\n",
    "\n",
    "         [[[24., 25., 26., 27.],\n",
    "           [28., 29., 30., 31.],\n",
    "           [32., 33., 34., 35.]],\n",
    "\n",
    "          [[36., 37., 38., 39.],\n",
    "           [40., 41., 42., 43.],\n",
    "           [44., 45., 46., 47.]]]],  ← end of folder (2 per tensor)\n",
    "\n",
    "\n",
    "\n",
    "        [[[[48., 49., 50., 51.],\n",
    "           [52., 53., 54., 55.],\n",
    "           [56., 57., 58., 59.]],\n",
    "\n",
    "          [[60., 61., 62., 63.],\n",
    "           [64., 65., 66., 67.],\n",
    "           [68., 69., 70., 71.]]],\n",
    "\n",
    "\n",
    "         [[[72., 73., 74., 75.],\n",
    "           [76., 77., 78., 79.],\n",
    "           [80., 81., 82., 83.]],\n",
    "\n",
    "          [[84., 85., 86., 87.],\n",
    "           [88., 89., 90., 91.],\n",
    "           [92., 93., 94., 95.]]]]]  ← end of tensor\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `stride` tells us where a tensor's `data` is stored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Great writeup](http://blog.ezyang.com/2019/05/pytorch-internals/) and the [Stride Visualizer](https://ezyang.github.io/stride-visualizer/index.html), both by ezyang."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors are logically represented by their shape, but they need to exist in physical realty (especially if they contain data). Writing each row to memory sequentially yields a **contiguous** tensor. A contiguous tensor of 32-bit ints will have each int spaced out 4 bytes in memory (32/8=4). If we index the starting byte of every 32-bit int in memory (the first of every four bytes), then we have to increment the index by 1 to get to the next value in the tensor. The value we increment the index by to get the next value is the **stride**.\n",
    "\n",
    "For example, to locate activation `t[i,j]` in memory given `t.stride=[u,v]`, the index of its location is found with: `i*s + j*t`.\n",
    "\n",
    "Generally, given a tensor of shape `(d, c, b, a)`, it is contiguous if it has strides `(cba, ba, a, 1)`. Generally, the right-most stride must be 1, and every other stride must be the product of the dimension sizes before it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some view ops (see below) may produce non-contiguous tensors. Tensors that are not contiguous may not be able to undergo some operations that depend on sequential indexing, and may also suffer performance issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strides are the fundamental basis of how PyTorch provides views to users. For example, a view created by taking a slice of an index will point to the same physical as the original tensor, but the stride metadata will be different. Using `t` with stride `[u,v]` from above: `v = t[,0]` will have `v.stride=[u]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparse tensors are not strided. They consist of two tensors: one for indeces, one for data. MKL-DNN tensors are an even more exotic creatures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `view`s are tensors that are paired to another tensor's `data`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch allows a tensor to be a `view` of an existing tensor. A view tensor shares the same underlying data with its base tensor, avoiding explicit data copy.\n",
    "\n",
    "Typically, a pytorch operation will return a new tensor as an output, but special **view ops** return a view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View ops\n",
    "([full list here](https://pytorch.org/docs/stable/tensor_view.html)):\n",
    "- view, as seen above\n",
    "- slicing and indexing\n",
    "- squeeze & unsqueeze\n",
    "- transpose\n",
    "- expand\n",
    "- split\n",
    "- *might create views: reshape, reshape_as, flatten*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at tensor attributes in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a tensor's attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 1.],\n",
       "         [2., 3.]]),\n",
       " torch.Size([2, 2]),\n",
       " tensor([[0., 1.],\n",
       "         [2., 3.]]),\n",
       " (2, 1),\n",
       " 94706318980096)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = T(2,2)\n",
    "t, t.shape, t.data, t.stride(), t.storage().data_ptr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a view of that tensor and look at its attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2., 3.]),\n",
       " torch.Size([4]),\n",
       " tensor([0., 1., 2., 3.]),\n",
       " (1,),\n",
       " 94706318980096)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = t.view(4)\n",
    "v, v.shape, v.data, v.stride(), v.storage().data_ptr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that both tensors have the same data pointer for their storage (data-holding containers):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.storage().data_ptr() == v.storage().data_ptr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we created a new tensor using an operation that isn't a view-op, you can see that the new tensor has its own data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = t + T(2,2)\n",
    "t.storage().data_ptr() == t2.storage().data_ptr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `torch.tensor()` and `torch.Tensor()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.tensor()` is the recommended way of creating tensors. It creates a tensor given an input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.Tensor()` is different and has lots of hidden behavior:\n",
    "- `Tensor([1,2,3])` creates a float tensor\n",
    "- `Tensor(1,2,3)` performs `torch.empty(1,2,3)`\n",
    "- `Tensor()` yeilds the null tensor with no allocated mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-4.1637e+24,  4.5614e-41, -4.1637e+24],\n",
       "         [ 4.5614e-41,  4.4842e-44,  0.0000e+00]]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(1,2,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `torch.empty(shape)`\n",
    "\n",
    "Create a tensor of `shape` using random memory; will be filled with garbage, but it's the fastest way to create a tensor. Note that this tensor is NOT a floattensor, but all other tensors created in this section are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3740e+18,  4.5583e-41, -1.2442e+23],\n",
       "        [ 3.0963e-41,  4.4842e-44,  0.0000e+00]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.empty((2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `torch.zeros(sh)` / `.ones(sh)`\n",
    "\n",
    "Create float tensor of shape with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `torch.arange(num)`\n",
    "Create a tensor containing float ints from 0 to num-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(4.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `torch.rand(sh)` / `.randn(sh)` / `.randint(sh)`\n",
    "- Uni-dist floats [0,1]\n",
    "- Norm-dist floats [-1,1]\n",
    "- Uni-dist ints [0,n-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8196, 0.3945, 0.2371, 0.1140])\n",
      "tensor([ 0.7954, -1.0214, -1.8045, -1.4719])\n",
      "tensor([0, 2, 2, 2, 1, 2, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "pnl( torch.rand(4)         ,\n",
    "     torch.randn(4)        ,\n",
    "     torch.randint(3,(8,)) ) # shape must be tuple, even if rank one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing and Slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `t[:, :, :]` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follows the (dir,sheet,row,col) shape structure order!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2., 3.],\n",
      "        [4., 5., 6., 7.]])\n",
      "tensor([4., 5., 6., 7.])\n",
      "tensor(4.)\n",
      "tensor([ 7., 15.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.]],\n",
       "\n",
       "        [[ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.]]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = T(2,2,4)          # 2 sheets, 2 rows, 4 cols\n",
    "\n",
    "pnl( t[0]       ,     # first sheet\n",
    "     t[0,-1]    ,     # first sheet's last row\n",
    "     t[0,-1,0]  ,     # first sheet's last row's first col\n",
    "     t[:,-1,-1] )     # all   sheets' last row's last  col\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `t1[t2]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**t1[t2]**\n",
    "\n",
    "Grab the first and last rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4],\n",
       "        [45, 46, 47, 48, 49]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = T(10,5)\n",
    "idx = tensor([0,-1])\n",
    "t[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**t1[:, t2]**\n",
    "\n",
    "Grab the first and last cols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  4],\n",
       "        [ 5,  9],\n",
       "        [10, 14],\n",
       "        [15, 19],\n",
       "        [20, 24],\n",
       "        [25, 29],\n",
       "        [30, 34],\n",
       "        [35, 39],\n",
       "        [40, 44],\n",
       "        [45, 49]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[:, idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**t1[t2][:,t2]**\n",
    "\n",
    "Grab the first and last cols from the first and last rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  4],\n",
       "        [45, 49]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[idx][:,idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**t1[t2,t2]**\n",
    "\n",
    "Double-tensor indexing works a little differently. The second tensor's indeces correspond the first tensor's.\n",
    "\n",
    "Grab the first column from the first row and the last column from the last row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0, 49])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[idx,idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `t.view(shape)`\n",
    "\n",
    "Create a view of a tensor. View object shares data (and memory) with underlying tensor; avoids extra data copying for more efficient operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T(6).view(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[0, 1],\n",
       "           [2, 3],\n",
       "           [4, 5]]]]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T(6).view(1,1,1,3,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding/enlarging dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `torch.cat`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smoosh tensors together into a single tensor.\n",
    "\n",
    "Concatenates to trailing (last) dimension. All dimensions except the first dim must have the same sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [2, 3],\n",
       "        [0, 1],\n",
       "        [2, 3],\n",
       "        [4, 5],\n",
       "        [6, 7]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.cat((T(2,2), T(4,2)))\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `torch.stack`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a tensor of tensors (like a list of lists).\n",
    "\n",
    "Create a new dimension by stacking tensors of the same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1],\n",
       "         [2, 3]],\n",
       "\n",
       "        [[0, 1],\n",
       "         [2, 3]]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.stack((T(2,2),T(2,2)))\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [2, 3]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the difference between cat and stack?\n",
    "- **cat** increases the size of the final dimension\n",
    "    - `torch.cat((T(2,2),T(2,2))).shape == [4,2]`\n",
    "- **stack** creates a new dimension\n",
    "    - `torch.stack ((T(2,2),T(2,2))).shape == [2,2,2]`\n",
    "- use **cat** when you want to smoosh tensors together\n",
    "- use **stack** to create a tensor of tensors\n",
    "    - `t3 = torch.stack(t1, t2)` is like creating a list of lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `t.repeat(nreps,nreps,nreps,...)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new tensor from an existing one. The args passed are the number of repitions of t per dimension. Note that an arg of 1 is NOT trivial (unless all args are 1 and the length of the args = the rank of the original tensor).\n",
    "\n",
    "Trivial case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1.],\n",
       "        [2., 3.]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = T(2,2)\n",
    "t.repeat(1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about the usual spreadsheet metaphor: \"I'd like 2 sheets of 1\\*rows and 1\\*cols\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 1.],\n",
       "         [2., 3.]],\n",
       "\n",
       "        [[0., 1.],\n",
       "         [2., 3.]]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.repeat(2,1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dim sizes of the new tensor are the dim sizes of the original tensor times how many times it is repeated. Below, (3,1,4) is multiplied by (1,2,2) to find the new tensor's shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 8])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T(2,2).repeat(3,1,4).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing/shrinking dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `torch.squeeze(t)`\n",
    "\n",
    "Remove dims of size 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.squeeze(T(1,8,1,8,1)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `t.flatten()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit elements of tensor into specified end dimension (usually the last for a dim 1 tensor, but not always).\n",
    "\n",
    "`T(c,b,a).flatten()` → `T(c*b*a)`\n",
    "\n",
    "`T(c,b,a).flatten(end_dim=-2)` → `T(c*b,a)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T(2,2,3).flatten() # same as end_dim=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.],\n",
       "        [ 3.,  4.,  5.],\n",
       "        [ 6.,  7.,  8.],\n",
       "        [ 9., 10., 11.]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T(2,2,3).flatten(end_dim=-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `torch.split(t,n,dim=0)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split t into dimsize/n new tensors. Does not repeat. Can result in unevenly sized new tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 1., 2.]]),\n",
       " tensor([[3., 4., 5.]]),\n",
       " tensor([[6., 7., 8.]]),\n",
       " tensor([[ 9., 10., 11.]]),\n",
       " tensor([[12., 13., 14.]]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.split(T(5,3), 1) # 5/1 = 5 new tensors, each of shape (1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.],\n",
       "         [ 3.,  4.],\n",
       "         [ 6.,  7.],\n",
       "         [ 9., 10.],\n",
       "         [12., 13.]]),\n",
       " tensor([[ 2.],\n",
       "         [ 5.],\n",
       "         [ 8.],\n",
       "         [11.],\n",
       "         [14.]]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.split(T(5,3), 2, dim=1)  # 3/2 = 2 new tensors of diff shapes: (5,2) and (5,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "- [NumPy broadcasting](https://numpy.org/doc/stable/user/basics.broadcasting.html#module-numpy.doc.broadcasting)\n",
    "- [PyTorch broadcasting](https://pytorch.org/docs/stable/notes/broadcasting.html) (mostly from this)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at what happens when you add a 1x3 vector to a 3x1 vector:\n",
    "\n",
    "`ex:\n",
    "          |1|   |1+1 1+2 1+3|   |2 3 4|\n",
    "|1 2 3| + |2| = |1+2 2+2 3+2| = |3 4 5|\n",
    "          |3|   |1+3 2+3 3+3|   |4 5 6|`\n",
    "\n",
    "Notice how each of the input vectors had to be copied three times to perform all of the element-wise operations. That copy-creating operation is called **broadcasting.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From PyTorch docs: \"If a PyTorch operation supports broadcasting, then its Tensor args can be automatically expanded to be of equal sizes *(without making copies of the data)*.\" (my emphasis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Being able to do tensor maths with massive tensors and without having to make copies when broadcasting is a big deal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to Pytorch, two tensors are broadcastable if:\n",
    "- Neither has a shape of [0]\n",
    "- When iterating over the dimension sizes, starting at the trailing dimensison, the dimension sizes must be equal, one of them is 1, or one of them does not exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def broadcastable(t1,t2):\n",
    "    dim_sizes1, dim_sizes2 = t1.size, t2.size\n",
    "    \n",
    "    for p,q in zip(dim_sizes1, dim_sizes2):\n",
    "        if p+1 == 0: continue\n",
    "        if p\n",
    "\n",
    "grab e1\n",
    "e1+0==1           --> pass and break\n",
    "e2 does not exist --> pass and break\n",
    "grab e2\n",
    "e1==e2            --> pass and break\n",
    "else              --> fail and break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How I think about it:\n",
    "\n",
    "For every dimsize pair:\n",
    "- one or both == 1; e1+0==1 -> pass\n",
    "- pair has len 1    e2 does not exist -> pass\n",
    "- values are equal; e1==e2 -> pass\n",
    "- else fail\n",
    "\n",
    "*Again:*\n",
    "\n",
    "For every dimsize-pair: if one element equals x st x in range [2-9], the other must be x, 1, or null."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No broadcasting needed for same shape\n",
    "No element is referenced any more than once --> no broadcasting takes place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6.7333e+22, 1.7591e+22],\n",
       "        [1.7184e+25, 4.3222e+27]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T(2,2) + T(2,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dimsize pair has one 1\n",
    "- A folder containing nothing but one other folder can always be added or deleted.\n",
    "- Like my junior-high English teacher said: \"no lists with only one bullet!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.5501e-09,  3.5505e-09,  3.5501e-09],\n",
       "        [-3.5675e-13,  7.6170e-41, -3.5282e-13],\n",
       "        [ 3.5501e-09,  3.5505e-09,  3.5501e-09]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T(1,2) + T(3,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dimsize pair has one element\n",
    "- Happens when tensors are different ranks\n",
    "- The outermost dimension(s) of the higher-rank tensor will not have a counterpart from the lower-ranked tensor\n",
    "- Lower-rank tensor is padded with dims of size 1 until it matches the other tensor's rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.7568e+13,  7.6556e-41, -4.3588e-01],\n",
       "         [-2.7568e+13,  3.1007e-41, -2.1794e-01]],\n",
       "\n",
       "        [[-2.7568e+13,  3.0962e-41, -2.7677e+13],\n",
       "         [-2.7568e+13, -2.7677e+13, -2.1794e-01]]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T(2,2,3) + T(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `(4,4)`: equal\n",
    "- `(1,2)`: one is 1\n",
    "- `(3, )`: len is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  2.,  4.,  6.],\n",
       "         [ 4.,  6.,  8., 10.]],\n",
       "\n",
       "        [[ 4.,  6.,  8., 10.],\n",
       "         [ 8., 10., 12., 14.]],\n",
       "\n",
       "        [[ 8., 10., 12., 14.],\n",
       "         [12., 14., 16., 18.]]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T(3,1,4) + T(2,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

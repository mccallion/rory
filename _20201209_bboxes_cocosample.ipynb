{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Notes\" data-toc-modified-id=\"Notes-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Notes</a></span><ul class=\"toc-item\"><li><span><a href=\"#Journal\" data-toc-modified-id=\"Journal-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Journal</a></span></li><li><span><a href=\"#To-Do\" data-toc-modified-id=\"To-Do-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>To Do</a></span></li><li><span><a href=\"#Musings\" data-toc-modified-id=\"Musings-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Musings</a></span></li></ul></li><li><span><a href=\"#Data\" data-toc-modified-id=\"Data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data</a></span></li><li><span><a href=\"#Model\" data-toc-modified-id=\"Model-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Model</a></span></li><li><span><a href=\"#Train\" data-toc-modified-id=\"Train-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Train</a></span><ul class=\"toc-item\"><li><span><a href=\"#Init\" data-toc-modified-id=\"Init-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Init</a></span></li><li><span><a href=\"#Training-sessions\" data-toc-modified-id=\"Training-sessions-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Training sessions</a></span><ul class=\"toc-item\"><li><span><a href=\"#What-worked\" data-toc-modified-id=\"What-worked-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>What worked</a></span></li><li><span><a href=\"#What-didn't-work\" data-toc-modified-id=\"What-didn't-work-4.2.2\"><span class=\"toc-item-num\">4.2.2&nbsp;&nbsp;</span>What didn't work</a></span><ul class=\"toc-item\"><li><span><a href=\"#Overfit-fine-tune\" data-toc-modified-id=\"Overfit-fine-tune-4.2.2.1\"><span class=\"toc-item-num\">4.2.2.1&nbsp;&nbsp;</span>Overfit fine tune</a></span></li><li><span><a href=\"#Test-fine_tune-method\" data-toc-modified-id=\"Test-fine_tune-method-4.2.2.2\"><span class=\"toc-item-num\">4.2.2.2&nbsp;&nbsp;</span>Test fine_tune method</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Results-and-Metrics\" data-toc-modified-id=\"Results-and-Metrics-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Results and Metrics</a></span><ul class=\"toc-item\"><li><span><a href=\"#Viz-results\" data-toc-modified-id=\"Viz-results-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Viz results</a></span></li><li><span><a href=\"#Investigating-low-performance\" data-toc-modified-id=\"Investigating-low-performance-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Investigating low performance</a></span><ul class=\"toc-item\"><li><span><a href=\"#Deeper-architecture?\" data-toc-modified-id=\"Deeper-architecture?-5.2.1\"><span class=\"toc-item-num\">5.2.1&nbsp;&nbsp;</span>Deeper architecture?</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to learn how to fine-tune my learner with fastai's built-in methods. I'll also be moving to the COCO_SAMPLE dataset so I can speed up the training loops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Journal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2020-12-09 (Wed):\n",
    "    - Created nb\n",
    "    - Accomplishment: I figured out how to use a resnet50 backbone\n",
    "    - I'm concerned that the results are pretty poor. I made a note to figure out why the predictions for remote are especially bad. Is it just that this dataset doesn't have people..?\n",
    "    - The fastai 2018 course implementation of SSD achieved **.30** mAP on the PASCAL VOC 2007 dataset ([link here](https://forums.fast.ai/t/mean-average-precision-map/14345/16?u=mccallionr)). This makes me think I should aim for an mAP of .5 on the same dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Main things**\n",
    "- ðŸŸ¢ Move to COCO_SAMPLE in order to speed up training\n",
    "- ðŸŸ¡ Train & fine-tune resnet34 to the best of my ability using only fastai's train methods\n",
    "    - Review fastbook and course to figure out how to squeeze out some extra performance\n",
    "    - ðŸŸ¡ because I did a lot of testing but wasn't able to improve on fit_one_cycle.\n",
    "    - I can still try fine_tune(5, frozen_epochs=2, base_lr=lr), but I do not think I'll get better results.\n",
    "- ðŸŸ¢ Unexpected: I was able to figure out how to use a resnet50 base instead of a resnet34!!!\n",
    "- Next up: train data on pascal2007 so I can compare my results to the 2018 fastai class's.\n",
    "    \n",
    "\n",
    "**Laundry**\n",
    "- Refactor ap_per_cls for use in metric and in ResultShower\n",
    "- Consider refactoring SSD loss to use acts_to_preds\n",
    "- show_bb should output colored text\n",
    "- Set all devices in one go\n",
    "- Change dls.vocab to be voc or v in places"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Musings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The anchor box approach**\n",
    "\n",
    "The idea is to subdivide the image into chunks so we can essentially do many single bbox learners instead of one big multibox. To do this, each image must be subdivided into a grid (or multiple grids), and the labels must be remapped to individual grid cells.\n",
    "\n",
    "Subdivide the image into cells with a grid. For each cell, generate any number of boxes from it so long as those boxes do not have a higher IOU between them and any other grid cell â€“ the generated boxes must be able to be traced back to their grid cell via IOU. For each box generated, have it perform a 'single labeled bbox' task. Each box generated will produce one result. Keep only the most confident results & prune out any that overlap excessively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "\n",
    "### Params ###\n",
    "im_sz   = 224\n",
    "bs      = 64\n",
    "val_pct = .1\n",
    "sub_pct = .2\n",
    "\n",
    "path = untar_data(URLs.COCO_SAMPLE)\n",
    "fns, annos = get_annotations(path/'annotations/train_sample.json')\n",
    "fn2anno = {f:a for f,a in zip(fns,annos)}\n",
    "def get_im(f):   return Path(path/'train_sample'/f)\n",
    "def get_bbox(f): return fn2anno[f][0]\n",
    "def get_lbl(f):  return fn2anno[f][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#7) ['#na#','book','chair','couch','remote','tv','vase']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### DataBlock & DataLoaders ###\n",
    "db = DataBlock(\n",
    "    blocks=[ImageBlock, BBoxBlock, BBoxLblBlock],\n",
    "    get_x=get_im, get_y=[get_bbox, get_lbl],\n",
    "    splitter=RandomSplitter(val_pct),\n",
    "    item_tfms=Resize(im_sz, method='squish'),\n",
    "    batch_tfms=Normalize.from_stats(*imagenet_stats), n_inp=1)\n",
    "subset = L(fns).shuffle()[0:int(len(fns)*sub_pct)]\n",
    "dls = db.dataloaders(subset, bs=bs)\n",
    "voc = dls.vocab\n",
    "n_cls = len(voc)\n",
    "voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train data: 3931\n",
      "Size of valid data: 436\n",
      "batch[0]: \t torch.float32 \t torch.Size([64, 3, 224, 224])\n",
      "batch[1]: \t torch.float32 \t torch.Size([64, 20, 4])\n",
      "batch[2]: \t torch.int64 \t torch.Size([64, 20])\n"
     ]
    }
   ],
   "source": [
    "### Inspection (IMPORTANT) ###\n",
    "print(\"Size of train data:\",len(dls.train.items))\n",
    "print(\"Size of valid data:\",len(dls.valid.items))\n",
    "for i,t in enumerate(dls.one_batch()):\n",
    "    print(f\"batch[{i}]:\",'\\t',t.dtype,'\\t',t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation of tensor shapes:\n",
    "- torch.Size([128, 3, 224, 224]): bs, channels (rgb), im_sz, im_sz\n",
    "- torch.Size([128, 20, 4]): bs, max objs for a single im in batch, bb coords\n",
    "- torch.Size([128, 20]): bs, max objs for a single im in batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Architecture ###\n",
    "def perm(x,k):\n",
    "    return x.permute(0,3,2,1).contiguous().view(x.size(0),x.size(2)*x.size(3)*k,x.size(1)//k)\n",
    "class StdConv(Module):\n",
    "    \"\"\"(42:00) Wraps together the standard conv2d â†’ batchnorm â†’ dropout.\"\"\"\n",
    "    def __init__(self, nin, nout, stride=2, drop=0.1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(nin, nout, 3, stride=stride, padding=1)\n",
    "        self.bn = nn.BatchNorm2d(nout)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x): return self.drop(self.bn(F.relu(self.conv(x))))\n",
    "class OutConv(Module):\n",
    "    \"\"\"Takes nin inputs, outputs two layers: one for bbs, one for lbls.\"\"\"\n",
    "    def __init__(self, k, nin, bias):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "        self.bb_acts  = nn.Conv2d(nin, 4*k, 3, padding=1)             \n",
    "        self.lbl_acts = nn.Conv2d(nin, len(dls.vocab)*k, 3, padding=1)\n",
    "        self.lbl_acts.bias.data.zero_().add_(bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return [perm(self.bb_acts(x), self.k),\n",
    "                perm(self.lbl_acts(x),self.k)]         \n",
    "class SSDHead(Module):\n",
    "    \"\"\"Wraps StdConv and OutConv into a head module.\"\"\"\n",
    "    def __init__(self, k, bias, drop):\n",
    "        super().__init__()\n",
    "        self.drop  = nn.Dropout(drop)\n",
    "        self.conv0 = StdConv(512,256, drop=drop) # most grid cells (4x4 for sz=224)\n",
    "        self.out0  = OutConv(k,  256, bias)\n",
    "        self.conv1 = StdConv(256,256, drop=drop)\n",
    "        self.out1  = OutConv(k,  256, bias)\n",
    "        self.conv2 = StdConv(256,256, drop=drop) # fewest grid cells (1x1 for sz=224)\n",
    "        self.out2  = OutConv(k,  256, bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.drop(F.relu(x))\n",
    "        x = self.conv0(x)\n",
    "        bb0,lbl0 = self.out0(x)\n",
    "        x = self.conv1(x)\n",
    "        bb1,lbl1 = self.out1(x)\n",
    "        x = self.conv2(x)\n",
    "        bb2,lbl2 = self.out2(x)\n",
    "        return [torch.cat([ bb0, bb1, bb2], dim=1),\n",
    "                torch.cat([lbl0,lbl1,lbl2], dim=1)]\n",
    "class CustomModule(Module):\n",
    "    \"\"\"Simple class for joining two modules (a 'head' and a 'body') into one.\"\"\"\n",
    "    def __init__(self, body, head):\n",
    "        self.body, self.head = body, head\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.head(self.body(x))\n",
    "\n",
    "\n",
    "### Create Anchors ###\n",
    "def create_anchors(subdivs, zooms, ratios, device='cuda'):\n",
    "    # create list of permutations per default anchor box\n",
    "    perms = [(z*r1,z*r2) for z in zooms for (r1,r2) in ratios]\n",
    "    k = len(perms)\n",
    "    offsets = [1/(sd*2) for sd in subdivs]\n",
    "    xs = np.concatenate([np.repeat(np.linspace(o, 1-o, sd), sd) for o,sd in zip(offsets, subdivs)])\n",
    "    ys = np.concatenate([np.tile  (np.linspace(o, 1-o, sd), sd) for o,sd in zip(offsets, subdivs)])\n",
    "    ctrs = np.repeat(np.stack([xs,ys], axis=1), k, axis=0)\n",
    "    hws = np.concatenate([np.array([[o/sd,p/sd] for i in range(sd*sd) for o,p in perms]) for sd in subdivs])\n",
    "    box_sizes = tensor(np.concatenate([np.array([1/sd for i in range(sd*sd) for o,p in perms])\n",
    "                                      for sd in subdivs]), requires_grad=False).unsqueeze(1)\n",
    "    anchors = tensor(np.concatenate([ctrs, hws], axis=1), requires_grad=False).float()\n",
    "    return anchors.to(device), box_sizes.to(device)\n",
    "def create_anchor_boxes(ctr, hw):\n",
    "    return 2*torch.cat((tensor(ctr-hw/2), tensor(ctr+hw/2)), axis=1)-1\n",
    "\n",
    "\n",
    "### mae, BCE_Loss, and FocalLoss ###\n",
    "def mae(t1, t2):\n",
    "    return ((t1 - t2).abs()).mean()\n",
    "def one_hot_embedding(lbls, n_cls):\n",
    "    return torch.eye(n_cls)[lbls.data.long()].cuda()\n",
    "class BCELoss(nn.Module):\n",
    "    def __init__(self, n_cls):\n",
    "        super().__init__()\n",
    "        self.n_cls = n_cls\n",
    "        \n",
    "    def forward(self, act_lbl, targ_lbl):\n",
    "        a = act_lbl[:,1:]\n",
    "        t = one_hot_embedding(targ_lbl, self.n_cls)\n",
    "        t = tensor(t[:,1:].contiguous())\n",
    "        w = self.get_weight(a,t)\n",
    "        return F.binary_cross_entropy_with_logits(a,t,None,reduction='sum')/self.n_cls\n",
    "    \n",
    "    def get_weight(self,a,t): return None \n",
    "class FocalLoss(BCELoss):\n",
    "    def get_weight(self, a, t):\n",
    "        alpha, gamma = 0.25, 2.0 # vals from paper\n",
    "        p = a.sigmoid()\n",
    "        pt = p*t + (1-p)*(1-t)\n",
    "        w = alpha*t + (1-alpha)*(1-t)\n",
    "        return w * (1-pt).pow(gamma)\n",
    "    \n",
    "\n",
    "### iou ###\n",
    "def intersxn(b1,b2):\n",
    "    x1 = torch.max((b1)[:,None,0], (b2)[None,:,0])\n",
    "    y1 = torch.max((b1)[:,None,1], (b2)[None,:,1])\n",
    "    x2 = torch.min((b1)[:,None,2], (b2)[None,:,2])\n",
    "    y2 = torch.min((b1)[:,None,3], (b2)[None,:,3])\n",
    "    return torch.clamp((x2-x1), min=0) * torch.clamp((y2-y1), min=0)\n",
    "def area(b):\n",
    "    return (b[:,2]-b[:,0]) * (b[:,3]-b[:,1])\n",
    "def get_iou(b1, b2):\n",
    "    inter = intersxn(b1,b2)\n",
    "    union = area(b1).unsqueeze(1) + area(b2).unsqueeze(0) - inter\n",
    "    return inter / union\n",
    "\n",
    "\n",
    "### ssd_loss ###\n",
    "def remove_padding(bb, lbl):\n",
    "    bb = bb.view(-1,4)\n",
    "    padding = (bb[:,2]-bb[:,0])==0\n",
    "    return bb[~padding],lbl[~padding]\n",
    "def get_pred_bbs(act_bb, ancs, anc_sz, device):\n",
    "    ancs.to(device); anc_sz.to(device)\n",
    "    acts = torch.tanh(act_bb)                   # make acts between -1 and 1\n",
    "    ctrs = ancs[:,:2] + (acts[:,:2]/2 * anc_sz) # move ctrs by up to box_size/2\n",
    "    hws  = ancs[:,2:] * (acts[:,2:]/2+1)        # adjust hw up to 1/2\n",
    "    return create_anchor_boxes(ctrs, hws)\n",
    "def map_to_gt(ious):\n",
    "    max_iou_per_bb, anc_idxs = ious.max(1)\n",
    "    max_iou_per_anc, bb_idxs = ious.max(0)\n",
    "    max_iou_per_anc[anc_idxs] = 1.99\n",
    "    for i,iou in enumerate(anc_idxs): bb_idxs[iou] = i\n",
    "    return bb_idxs, max_iou_per_anc\n",
    "def ssd_item_loss(act_bbs, act_lbls, bbs, lbls):\n",
    "    \"\"\"SSD item loss takes single items from a minibatch, creates hundreds of preds, maps gt\n",
    "       to the preds, prunes the preds, then calcs & returns the bb and lbl loss for that item.\n",
    "       bb_loss and lbl_loss must both be instantiated before this is.\"\"\"\n",
    "    # prep\n",
    "    bbs,lbls = remove_padding(bbs,lbls)                  # remove gt padding inserted during training\n",
    "    pred_bbs = get_pred_bbs(act_bbs, anchors, box_size, 'cuda') # assemble the ~200 pred bbs from acts and ancs\n",
    "    # map gt to preds\n",
    "    iou_gt_grid = get_iou(bbs.data, anchor_boxes.data)   # get iou(gt_bbs,anc_bbs); used to map gt â†’ ancs\n",
    "    mapped_gt_idx, iou_gt_preds = map_to_gt(iou_gt_grid) # assign each pred an index of a gt object\n",
    "    mapped_bbs  = bbs[mapped_gt_idx]                     # project gt bbs into pred space\n",
    "    mapped_lbls = lbls[mapped_gt_idx]                    # project gt lbls into pred space\n",
    "    # remove low-iou bb preds & set mapped lbl to bg\n",
    "    high_iou = iou_gt_preds > 0.4                        # only include bb preds that overlap w/a gt obj and\n",
    "    incl = torch.nonzero(high_iou)[:,0]                  #  are not predicting background\n",
    "    mapped_lbls[~high_iou] = 0                           # assign gt class of bg to preds w/ low max gt iou\n",
    "    # compute loss\n",
    "    bb_loss_val  = bb_loss(pred_bbs[incl], mapped_bbs[incl])\n",
    "    lbl_loss_val = lbl_loss(act_lbls, mapped_lbls)\n",
    "    return bb_loss_val, lbl_loss_val\n",
    "def ssd_loss(b_acts, b_bbs, b_lbls):\n",
    "    sum_bb_loss, sum_lbl_loss = 0., 0.\n",
    "    for o in zip(*b_acts, b_bbs, b_lbls):\n",
    "        bb_loss, lbl_loss = ssd_item_loss(*o)\n",
    "        sum_bb_loss  += bb_loss\n",
    "        sum_lbl_loss += lbl_loss\n",
    "    return sum_bb_loss + sum_lbl_loss\n",
    "\n",
    "\n",
    "### Utils for converting acts to preds ###\n",
    "def nms(boxes, scores, min_iou=0.5, top_k=100):\n",
    "    keep = scores.new(scores.size(0)).zero_().long()\n",
    "    if boxes.numel() == 0: return keep\n",
    "    x1 = boxes[:, 0]\n",
    "    y1 = boxes[:, 1]\n",
    "    x2 = boxes[:, 2]\n",
    "    y2 = boxes[:, 3]\n",
    "    area = torch.mul(x2 - x1, y2 - y1)\n",
    "    v, idx = scores.sort(0)  # sort asc\n",
    "    idx = idx[-top_k:]       # indices of k largest vals\n",
    "    xx1 = boxes.new()\n",
    "    yy1 = boxes.new()\n",
    "    xx2 = boxes.new()\n",
    "    yy2 = boxes.new()\n",
    "    w = boxes.new()\n",
    "    h = boxes.new()\n",
    "\n",
    "    count = 0\n",
    "    while idx.numel() > 0:\n",
    "        i = idx[-1]  # index of current largest val\n",
    "        keep[count] = i\n",
    "        count += 1\n",
    "        if idx.size(0) == 1: break\n",
    "        idx = idx[:-1]  # remove kept element from view\n",
    "        # load bboxes of next highest vals\n",
    "        torch.index_select(x1, 0, idx, out=xx1)\n",
    "        torch.index_select(y1, 0, idx, out=yy1)\n",
    "        torch.index_select(x2, 0, idx, out=xx2)\n",
    "        torch.index_select(y2, 0, idx, out=yy2)\n",
    "        # store element-wise max with next highest score\n",
    "        xx1 = torch.clamp(xx1, min=x1[i])\n",
    "        yy1 = torch.clamp(yy1, min=y1[i])\n",
    "        xx2 = torch.clamp(xx2, max=x2[i])\n",
    "        yy2 = torch.clamp(yy2, max=y2[i])\n",
    "        w.resize_as_(xx2)\n",
    "        h.resize_as_(yy2)\n",
    "        w = xx2 - xx1\n",
    "        h = yy2 - yy1\n",
    "        # check sizes of xx1 and xx2.. after each iteration\n",
    "        w = torch.clamp(w, min=0.0)\n",
    "        h = torch.clamp(h, min=0.0)\n",
    "        inter = w*h\n",
    "        # IoU = i / (area(a) + area(b) - i)\n",
    "        rem_areas = torch.index_select(area, 0, idx)  # load remaining areas)\n",
    "        union = (rem_areas - inter) + area[i]\n",
    "        IoU = inter/union  # store result in iou\n",
    "        # keep only elements with an IoU <= min_iou\n",
    "        idx = idx[IoU.le(min_iou)]\n",
    "    return keep, count\n",
    "def acts_to_preds(abb, albl, ancs, anc_sz, min_iou=.5, min_conf=.2, device='cuda'):\n",
    "    \"\"\"Turn model acts into preds: abbs use get_pred_bbs, and albls use sigmoid().max().\n",
    "       Used in ResultShower and mAP (and could possibly be used in loss fxn).\"\"\"\n",
    "     # convert acts to preds\n",
    "    pbb = get_pred_bbs(abb, ancs, anc_sz, device)\n",
    "    conf, plbl = albl.sigmoid().max(1)\n",
    "    # filter out preds w/ nms\n",
    "    nms_idxs, nms_n = nms(pbb.data, conf, min_iou)\n",
    "    nms_idxs = nms_idxs[:nms_n]\n",
    "    pbb  = pbb[nms_idxs]\n",
    "    plbl = plbl[nms_idxs]\n",
    "    conf = conf[nms_idxs]\n",
    "    # filter out bg and low-conf preds\n",
    "    is_not_bg = (plbl!=0)\n",
    "    is_confident = conf > min_conf\n",
    "    mask = is_not_bg & is_confident\n",
    "    return pbb[mask], plbl[mask], conf[mask]\n",
    "def get_batch_preds(abb, albl, ancs, anc_sz, device='cuda'):\n",
    "    \"\"\"Loop through a batch and of activations and turn them into predictions.\"\"\"\n",
    "    ancs.to(device); anc_sz.to(device)\n",
    "    pbbs, plbls, confs = [], [], []\n",
    "    for abb, albl in zip(abb, albl):\n",
    "        pbb, plbl, conf = acts_to_preds(abb, albl, ancs, anc_sz, device=device)\n",
    "        pbbs  += [pbb]\n",
    "        plbls += [plbl]\n",
    "        confs += [conf]\n",
    "    return pbbs, plbls, confs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Metric ###\n",
    "def _format_inps(acts, batch, anchors, box_size):\n",
    "    \"\"\"Format acts and targs for AP score calc. Input expects learner.acts & learner.batch,\n",
    "       output format: (im_idx, pred_bbs, pred_cls, cls_conf) and (im_idx, bbs, cls).\n",
    "       Ex: (46.0, tensor([0.1, 0.2, 0.9, 0.9]), tensor(3), tensor(0.78))\"\"\"\n",
    "    preds = get_batch_preds(*acts, anchors, box_size)\n",
    "    pred_idxs = torch.cat([torch.tensor([i]*len(o)) for i,o in enumerate(preds[0])])\n",
    "    pred_idxs = pred_idxs.numpy().tolist()\n",
    "    gt_idxs   = torch.cat([torch.tensor([i]*len(o)) for i,o in enumerate(batch[1])])\n",
    "    gt_idxs   = gt_idxs.numpy().tolist()\n",
    "    batch_preds = list(zip(pred_idxs, *[torch.cat(o) for o in preds]))\n",
    "    batch_gts   = list(zip(gt_idxs,   *[o.flatten(end_dim=1) for o in batch[1:]]))\n",
    "    return batch_preds, batch_gts\n",
    "def _flatten_list(l, ret_L=False):\n",
    "    \"\"\"Flatten a list-of-lists; lists can be python `list`s or a fastai `L`s.\"\"\"\n",
    "    def _recur(l,res):\n",
    "        for o in l:\n",
    "            if   isinstance(o,list): _recur(o,res)\n",
    "            elif isinstance(o,L)   : _recur(o,res)\n",
    "            else: res.append(o)\n",
    "        return res\n",
    "    res = _recur(l, [])\n",
    "    return res if not ret_L else L(res)\n",
    "def _get_tp_bbs(preds_tp):\n",
    "    \"\"\"Output list of tp bbs per im in batch. Not used to calculate mAP; only\n",
    "       used to grab true positive bb preds for visualizing in ResultShower.\"\"\"\n",
    "    \n",
    "    # Each row in preds_tps is a formatted pred (see format_ap_inputs) and a list of\n",
    "    # 1s and 0s (signifying tps and fps) for a cls. No preds for a cls â†’ empty lists.\n",
    "    \n",
    "    batch_idxs, pred_bbs, tpfps = [], [], []\n",
    "    for preds,tp in preds_tp:\n",
    "        batch_idxs.append([o[0] for o in preds])\n",
    "        pred_bbs.append([o[1] for o in preds])\n",
    "        tpfps.append(tp)\n",
    "    flat_idxs  = _flatten_list(batch_idxs)\n",
    "    flat_bbs   = _flatten_list(pred_bbs)\n",
    "    flat_tpfps = torch.cat(tpfps)\n",
    "\n",
    "    scored_preds = list(zip(flat_idxs, flat_bbs, flat_tpfps))\n",
    "    true_bbs = [(int(o[0]), o[1]) for o in scored_preds if o[2]==True]\n",
    "\n",
    "    true_preds = [torch.zeros(4).view(1,4) for i in range(0,bs)]\n",
    "    for i,bb in true_bbs:\n",
    "        if true_preds[i].sum()==0: true_preds[i] = bb.view(1,4)\n",
    "        else: true_preds[i] = torch.cat([true_preds[int(i)], bb.view(1,4)], dim=0)\n",
    "    return true_preds\n",
    "def ap_per_cls(acts, batch, n_cls, ancs, anc_sz, iou_thresh=.5, device='cuda'):\n",
    "    \"\"\"Calculate AP score per class. Returns avg AP over all classes.\"\"\"\n",
    "    batch_preds, batch_gts = _format_inps(acts, batch, ancs.to(device), anc_sz.to(device))\n",
    "    # avg_precs holds ap per cls; other accumulators only used for vizing results\n",
    "    avg_precs, preds_tp, n_objs_accum = [],[],[] \n",
    "    for c in range(1,n_cls): # start at 1 to ignore gt\n",
    "        # store preds and gts for current cls\n",
    "        preds = [b for b in batch_preds if b[2]==c]\n",
    "        gts   = [b for b in batch_gts   if b[2]==c]\n",
    "                \n",
    "        # sort preds by conf desc\n",
    "        preds.sort(key=lambda x: x[3], reverse=True)\n",
    "        \n",
    "        # make dict of im_idx:zeros(n_objs)\n",
    "        n_objs = Counter([gt[0] for gt in gts])\n",
    "        for k,v in n_objs.items(): n_objs[k] = torch.zeros(v)\n",
    "        \n",
    "        # init tp: a bool tensor for each im s.t. 1s indicate a pred is a tp\n",
    "        tp = torch.zeros((len(preds))).bool()\n",
    "        total_gt_objs = len(gts)\n",
    "        \n",
    "        for pred_idx, pred in enumerate(preds):\n",
    "            gt_objs = [o for o in gts if o[0] == pred[0]]\n",
    "            n_gt_objs = len(gt_objs)\n",
    "            max_iou = 0\n",
    "            \n",
    "            for idx, gt in enumerate(gt_objs):\n",
    "                iou = get_iou(pred[1].view(1,4), gt[1].view(1,4))\n",
    "                if iou > max_iou: max_iou, idx_of_max = iou, idx\n",
    "                    \n",
    "            # update idx of gt_obj to indicate it's been used\n",
    "            if max_iou > iou_thresh:\n",
    "                if n_objs[pred[0]][idx_of_max]==0:\n",
    "                    tp[pred_idx] = 1\n",
    "                    n_objs[pred[0]][idx_of_max] = 1\n",
    "                    \n",
    "        # store tp_bbs: use tp ask mask on preds and take 1th item (the bb) from each\n",
    "        preds_tp.append(([preds,tp]))\n",
    "        n_objs_accum.append(n_objs)\n",
    "        \n",
    "        # calc avg_prec and store\n",
    "        tps = torch.cumsum(tp, dim=0)               # 1. tp csum: [0,1,1,0,0] â†’ [0,1,2,2,2]\n",
    "        fps = torch.cumsum(~tp, dim=0)              # (basically same steps for fp)\n",
    "        prec = torch.div(tps, (tps + fps + 1e-6))   # 2. divide each tps item by n_preds\n",
    "        prec = torch.cat((torch.tensor([1]), prec)) # 3. slap on a 1 at the beginning\n",
    "        rec = tps / (total_gt_objs + 1e-6)\n",
    "        rec = torch.cat((torch.tensor([0]), rec))\n",
    "        avg_prec = torch.trapz(prec, rec) # calc AP w/ trap rule\n",
    "        avg_precs.append(avg_prec)        # store AP of this cls in accum\n",
    "    return avg_precs, _get_tp_bbs(preds_tp), n_objs_accum\n",
    "class MeanAveragePrecision(Metric):\n",
    "    \n",
    "    def __init__(self, func, n_cls): self.func,self.n_cls = func,n_cls\n",
    "        \n",
    "    def reset(self): self.res = []\n",
    "        \n",
    "    def accumulate(self, learn):\n",
    "        # store n_iter for use in self.value\n",
    "        self.n_iter = learn.n_iter\n",
    "        # calc cls_aps and append result to self.res\n",
    "        cls_aps,_,_ = self.func(learn.pred, (*learn.xb,*learn.yb), self.n_cls, anchors, box_size, device='cuda')\n",
    "        self.res.append(cls_aps)\n",
    "        \n",
    "    @property\n",
    "    def value(self):\n",
    "        return sum(torch.stack([tensor(o) for o in self.res]).sum(axis=0)/self.n_iter)/(self.n_cls-1)\n",
    "    \n",
    "    @property\n",
    "    def name(self): return \"mAP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### show_bb and ResultShower ###\n",
    "def show_bb(im, bb=None, lbl=[''], sz=im_sz, figsz=5, color='white', ctx=None, title=None):\n",
    "    # process empties and nones\n",
    "    if bb.shape[-1]==0 or bb==None: bb  = tensor([[0.,0,0,0]])\n",
    "    if lbl==['']:                   lbl = ['']*bb.shape[0]\n",
    "        \n",
    "    # process tensors to take advantage of fastai show methods\n",
    "    bbox = TensorBBox((bb+1)*sz//2)\n",
    "    labeledbbox = LabeledBBox(bbox,lbl)\n",
    "    \n",
    "    if ctx:     show_image(im, figsize=[figsz,figsz], title=title, ctx=ctx)\n",
    "    else: ctx = show_image(im, figsize=[figsz,figsz], title=title)\n",
    "    \n",
    "    labeledbbox.show(ctx=ctx)       # first, draw white lbl bbs...\n",
    "    bbox.show(ctx=ctx, color=color) # ... then overlay color bbs.\n",
    "    return ctx\n",
    "class ResultShower():\n",
    "    def __init__(self, dls, lrn, ancs, anc_sz):\n",
    "        # store init's args\n",
    "        self.dls    = dls\n",
    "        self.mod    = lrn.model.eval()\n",
    "        self.ancs   = ancs.cpu()\n",
    "        self.anc_sz = anc_sz.cpu()\n",
    "        # compute attributes\n",
    "        self.batch    = next(iter(self.dls.cpu().valid))\n",
    "        self.acts     = [a.data for a in self.mod.cpu()(self.batch[0])]\n",
    "        self.preds    = get_batch_preds(*self.acts, self.ancs, self.anc_sz, device='cpu')\n",
    "        self.dec_ims  = self.dls.decode(self.batch)[0]\n",
    "        self.im_sz    = self.batch[0].shape[-1]\n",
    "        self.v        = self.dls.vocab\n",
    "        self.last_res = 0\n",
    "        self.figsize  = [12,12]\n",
    "        # compute metrics\n",
    "        self.ap_scores,self.tp_bbs,self.nobj = ap_per_cls(self.acts,self.batch,len(self.v),self.ancs,self.anc_sz,device='cpu')\n",
    "        # clean up\n",
    "        self.dls.cuda(); self.mod.cuda()\n",
    "        \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.show_next(*args, **kwargs)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        # get everything to draw\n",
    "        _,bbs,lbls      = self.batch\n",
    "        pbbs,plbls,conf = self.preds\n",
    "        tbbs            = self.tp_bbs\n",
    "        ims             = self.dec_ims\n",
    "        # draw\n",
    "        ctx = get_grid(2, figsize=self.figsize)\n",
    "        show_bb(ims[i], bbs[i],   self.v[lbls[i]],  sz=self.im_sz, ctx=ctx[0]);\n",
    "        show_bb(ims[i], pbbs[i],  self.v[plbls[i]], sz=self.im_sz, ctx=ctx[1], color='magenta');\n",
    "        show_bb(ims[i], tbbs[i],                    sz=self.im_sz, ctx=ctx[1], color='lime')\n",
    "                 \n",
    "    def show_next(self, n=1):\n",
    "        for i in range(n): self[i + self.last_res]\n",
    "        self.last_res += n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just coded the above; next step is to test it out in training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init anchors\n",
    "subdivs = [4, 2, 1]\n",
    "zooms   = [0.75, 1.0, 1.3]\n",
    "ratios  = [(1.,1.), (1.,.5), (.5,1)]\n",
    "k = len(zooms) * len(ratios)\n",
    "anchors, box_size = create_anchors(subdivs, zooms, ratios)\n",
    "anchor_boxes = create_anchor_boxes(anchors[:,:2], anchors[:,2:])\n",
    "\n",
    "# loss, metric\n",
    "bb_loss  = mae\n",
    "lbl_loss = FocalLoss(n_cls)\n",
    "metric = MeanAveragePrecision(ap_per_cls, n_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CustomModule' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7c39240ee800>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_fresh_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mCustomModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreate_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresnet34\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSSDHead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_fresh_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-7c39240ee800>\u001b[0m in \u001b[0;36mget_fresh_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_fresh_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mCustomModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreate_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresnet34\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSSDHead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_fresh_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CustomModule' is not defined"
     ]
    }
   ],
   "source": [
    "def get_fresh_model():\n",
    "    return CustomModule(create_body(resnet34,pretrained=True), SSDHead(k,-4.,drop=.4))\n",
    "model = get_fresh_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What worked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intentional overfitting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = Learner(dls, model, loss_func=ssd_loss)\n",
    "learner.freeze()\n",
    "lr_min, lr_steep = learner.lr_find()\n",
    "lr = (lr_min+lr_steep)/2; print(\"lr:\",lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit_one_cycle(10, lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result:\n",
    "- The model learned for five epochs and started overfitting on the sixth.\n",
    "- Min valid loss: 477 at 5th epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stop before overfitting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- I have to reset the model to try again. I was incorrectly just resetting the learner.\n",
    "- I added to_fp16() to the learner to speed it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_fresh_model()\n",
    "learner = Learner(dls, model, loss_func=ssd_loss).to_fp16()\n",
    "learner.freeze()\n",
    "learner.fit_one_cycle(5, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save('s1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learner.export('models/20201209_cocosample.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What didn't work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More Fitting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.load('s1')\n",
    "lr_min, lr_steep = learner.lr_find()\n",
    "lr = (lr_min+lr_steep)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overfit fine tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit_one_cycle(10, lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save('s2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result: overfitting started immediately. In fastbook.05_pet_breeds, Jeremy says this, which I think may explain this result:\n",
    "\n",
    "> Before the days of 1cycle training it was very common to save the model at the end of each epoch, and then select whichever model had the best accuracy out of all of the models saved in each epoch. This is known as early stopping. However, this is very unlikely to give you the best answer, because those epochs in the middle occur before the learning rate has had a chance to reach the small values, where it can really find the best result. Therefore, if you find that you have overfit, what you should actually do is retrain your model from scratch, and this time select a total number of epochs based on where your previous best results were found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test fine_tune method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_fresh_model()\n",
    "learner = Learner(dls, model, loss_func=ssd_loss).to_fp16()\n",
    "learner.freeze()\n",
    "learner.fine_tune(5, lr=3e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result: almost achieved accuracy of foc(5,lr).\n",
    "\n",
    "Notes:\n",
    "- base_lr is the correct \"normal\" lr to set with fine_tune\n",
    "- discriminitive lrs are applied during deep training (min is lr/100)\n",
    "- Next test: remove to_fp16(), correctly set base_lr, more epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_fresh_model()\n",
    "learner = Learner(dls, model, loss_func=ssd_loss)\n",
    "learner.freeze()\n",
    "learner.fine_tune(8, base_lr=3e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result: didn't achieve accuracy of foc(5,lr).\n",
    "\n",
    "Overall results so far: foc(5,lr) is still winning.\n",
    "\n",
    "Notes:\n",
    "- I don't have to call learner.freeze() when using fine_tune â€“ it already does that.\n",
    "- Next test: 3 frozen epochs and 8 unfrozen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_fresh_model()\n",
    "learner = Learner(dls, model, loss_func=ssd_loss)\n",
    "learner.fine_tune(8, freeze_epochs=3, base_lr=3e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result: same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learner.save('s1')\n",
    "# learner.export('models/cocosample_...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(learner.d5ls.valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acts = learner.model.eval()(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = get_batch_preds(*acts, anchors, box_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_per_cls(acts, batch, len(voc), anchors, box_size, device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2020-11-17\n",
    "- **Baseline: resnet34 w/ 10% of data**\n",
    "    - valid_loss = 100\n",
    "    - fit_one_cycle(10, lr=2e-3)\n",
    "    - 48s per epoch\n",
    "    - resnet34\n",
    "    - dls.n_cls = 36\n",
    "    - data params\n",
    "        - im_size    = 224\n",
    "        - batch_size = 128\n",
    "        - valid_pct  = .10\n",
    "        - subset_pct = .1\n",
    "    - Exported to `models/20201117_coco_ssdfocal_0.pkl`\n",
    "    - Note: my attempts to unfreeze and train further have been unsuccessful\n",
    "    - vl/bs = .781\n",
    "- **xresnet34 w/ 10% of data**\n",
    "    - Summary\n",
    "        - Testing **xresnet34** vs the baseline\n",
    "        - Result: similar or possibly worse\n",
    "        - valid_loss = 113\n",
    "    - Training\n",
    "        - fit_one_cycle(10, lr=3e-3)\n",
    "        - 52s per epoch\n",
    "    - Data\n",
    "        - im_size    = 224\n",
    "        - batch_size = 128\n",
    "        - valid_pct  = .10\n",
    "        - subset_pct = .1\n",
    "        - n_cls = 38\n",
    "- **xresnet34 w/ 20% of data**\n",
    "    - valid_loss = 111\n",
    "    - fit_one_cycle(5, lr=3e-3)\n",
    "    - 1:40 per epoch (wasn't it taking 2:40 earlier? is this due to batch size?)\n",
    "    - data params\n",
    "        - im_size    = 224\n",
    "        - batch_size = 128\n",
    "        - valid_pct  = .10\n",
    "        - subset_pct = **.2**\n",
    "- **resnet34 w/ 10% of data & bs=64**\n",
    "    - I'm running this to see if I get better results than the baseline with a lower bs\n",
    "        - Oh baby, I'm getting a lower loss, but I see now that *loss sums over the batch!* Lower bs will almost always have lower loss.\n",
    "        - For example, the starting valid loss is 80, which is exactly half of the starting loss from the sessions with a larger bs.\n",
    "        - I need a metric! I suppose I can use valid_loss/bs for now?\n",
    "    - valid_loss = 53\n",
    "    - fit_one_cycle(5, lr=3e-3) \n",
    "    - data params\n",
    "        - im_sz   = 224\n",
    "        - bs      = **64**\n",
    "        - val_pct = .10\n",
    "        - sub_pct = .10\n",
    "- **resnet34 fine_tune**\n",
    "    - Testing fine_tune vs fit_one_cycle. I expect fine_tune to outperform.\n",
    "    - Result: fine_tune did not improve the result and required an additional epoch to match the above model's loss.\n",
    "    - valid_loss = 54\n",
    "    - **fine_tune**(3, freeze_epochs=3, lr=slice(lr/100,lr))\n",
    "    - data params\n",
    "        - im_sz   = 224\n",
    "        - bs      = 64\n",
    "        - val_pct = .10\n",
    "        - sub_pct = .10\n",
    "- **resnet34 w/ 50% of data**\n",
    "    - Let's see how much more data helps the result.\n",
    "    - Result: 88\n",
    "        - best valid_loss on 7th epoch (vl = 86)\n",
    "        - started overfitting on 8th epoch (vl = 87)\n",
    "    - vl/bs = .685\n",
    "    - fit_one_cycle(10, lr=(lr_min+lr_max)/2)\n",
    "    - 227s per epoch\n",
    "    - lr = 3.6e-3\n",
    "    - data params\n",
    "        - im_sz   = 224\n",
    "        - bs      = 128\n",
    "        - val_pct = .10\n",
    "        - sub_pct = **.50**\n",
    "    - Exported to `models/20201117_coco_ssd_resnet34_50pct.pkl`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results and Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viz results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrn = load_learner('models/20201209_cocosample.pkl')\n",
    "res = ResultShower(dls, lrn, anchors, box_size)\n",
    "res(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are not good. Something must be off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating low performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think something is off because I was able to get OK results by training the same architecture on 50% of the entire COCO dataset (55k training ims). My intuition is that it would be harder to train a model to detect 80 different classess of objects on 55k ims than training a model to det 6 cls of objs on 22k ims."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What I'm noticing about the visualized results:\n",
    "- Almost zero predictions for remotes\n",
    "- Possibly fewer predictions in general (fewer FPs and FNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deeper architecture?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't think this is the problem, but I'm almost done solving it so I'll go ahead and do it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last layer of rn34 has an output as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body34 = create_body(resnet34, pretrained=True)\n",
    "just_body34 = Learner(dls, body34, loss_func=ssd_loss)\n",
    "just_body34.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last layer of rn50 has this output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body50 = create_body(resnet50, pretrained=True)\n",
    "just_body50 = Learner(dls, body50, loss_func=ssd_loss)\n",
    "just_body50.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What layer will take the [64 x 2048 x 7 x 7] to a [64 x 512 x 7 x 7]?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dls' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-95c5bc8aaf92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblbls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dls' is not defined"
     ]
    }
   ],
   "source": [
    "bims, bbbs, blbls = next(iter(dls.train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = body50(bims)\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will a conv2d work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fconv = nn.Conv2d(2048, 512, 3, padding=1)\n",
    "u = fconv(t.cpu())\n",
    "u.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes!!! I can use my StdConv w/ stride=1 to get the same result and reuse code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StdConv(2048, 512, stride=1)(t.cpu()).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I made two changes to SSDHead to make it fit the resnet50:\n",
    "1. I added the extra StdConv (seen above)\n",
    "2. I removed the first ReLU in the forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSDHead50(Module):\n",
    "    \"\"\"Wraps StdConv and OutConv into a head module.\"\"\"\n",
    "    def __init__(self, k, bias, drop):\n",
    "        super().__init__()\n",
    "        self.drop  = nn.Dropout(drop)\n",
    "        self.conv_rn50 = StdConv(2048, 512, stride=1) # ADDED\n",
    "        \n",
    "        self.conv0 = StdConv(512,256, drop=drop) # most grid cells (4x4 for sz=224)\n",
    "        self.out0  = OutConv(k,  256, bias)\n",
    "        self.conv1 = StdConv(256,256, drop=drop)\n",
    "        self.out1  = OutConv(k,  256, bias)\n",
    "        self.conv2 = StdConv(256,256, drop=drop) # fewest grid cells (1x1 for sz=224)\n",
    "        self.out2  = OutConv(k,  256, bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.drop(x)      # REMOVED ReLU(x)\n",
    "        x = self.conv_rn50(x) # ADDED layer\n",
    "        x = self.conv0(x)\n",
    "        bb0,lbl0 = self.out0(x)\n",
    "        x = self.conv1(x)\n",
    "        bb1,lbl1 = self.out1(x)\n",
    "        x = self.conv2(x)\n",
    "        bb2,lbl2 = self.out2(x)\n",
    "        return [torch.cat([ bb0, bb1, bb2], dim=1),\n",
    "                torch.cat([lbl0,lbl1,lbl2], dim=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomModule(create_body(resnet50, pretrained=True),\n",
    "                     SSDHead50(k, -4., drop=.4))\n",
    "learner = Learner(dls, model, loss_func=ssd_loss).to_fp16()\n",
    "learner.freeze()\n",
    "learner.lr_find()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I'm using a batch size of 64 instead of 128, I'll have to multiply the valid_loss by 2 to get comparable results. That means I'm looking for an ending valid_loss of 225 or better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit_one_cycle(5, lr=2e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying again with only 20% of the data, and for 10 cycles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomModule(create_body(resnet50, pretrained=True),\n",
    "                     SSDHead50(k, -4., drop=.4))\n",
    "learner = Learner(dls, model, loss_func=ssd_loss).to_fp16()\n",
    "learner.freeze()\n",
    "learner.fit_one_cycle(10, lr=2e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll try fine tune:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomModule(create_body(resnet50, pretrained=True),\n",
    "                     SSDHead50(k, -4., drop=.4))\n",
    "learner = Learner(dls, model, loss_func=ssd_loss).to_fp16()\n",
    "learner.fine_tune(8, base_lr=2e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll now try using a lower dropout (.1 instead of .4). I actually don't really know what effect this is having on my training â€“ according to JH, it will prevent overfitting while decreasing min possible loss, but the tradeoff is worth it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomModule(create_body(resnet50, pretrained=True),\n",
    "                     SSDHead50(k, -4., drop=.1))\n",
    "learner = Learner(dls, model, loss_func=ssd_loss).to_fp16()\n",
    "learner.freeze()\n",
    "learner.fit_one_cycle(8, lr=2e-3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

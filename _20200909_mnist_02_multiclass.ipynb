{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Improvements-pursued-in-this-notebook\" data-toc-modified-id=\"Improvements-pursued-in-this-notebook-0.1\"><span class=\"toc-item-num\">0.1&nbsp;&nbsp;</span>Improvements pursued in this notebook</a></span></li></ul></li><li><span><a href=\"#Multiclass-classifier-&amp;-train_n_sessions\" data-toc-modified-id=\"Multiclass-classifier-&amp;-train_n_sessions-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Multiclass classifier &amp; train_n_sessions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Tools-for-data,-model,-and-training\" data-toc-modified-id=\"Tools-for-data,-model,-and-training-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Tools for data, model, and training</a></span></li><li><span><a href=\"#Train-model-once-w/-train_n_epochs\" data-toc-modified-id=\"Train-model-once-w/-train_n_epochs-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Train model once w/ <code>train_n_epochs</code></a></span></li><li><span><a href=\"#Train-model-many-times-w/-train_n_sessions\" data-toc-modified-id=\"Train-model-many-times-w/-train_n_sessions-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Train model many times w/ <code>train_n_sessions</code></a></span><ul class=\"toc-item\"><li><span><a href=\"#Diving-into-results-of-a-single-session\" data-toc-modified-id=\"Diving-into-results-of-a-single-session-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Diving into results of a single session</a></span></li></ul></li><li><span><a href=\"#Train-model-w/-Learner\" data-toc-modified-id=\"Train-model-w/-Learner-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Train model w/ <code>Learner</code></a></span><ul class=\"toc-item\"><li><span><a href=\"#data\" data-toc-modified-id=\"data-1.4.1\"><span class=\"toc-item-num\">1.4.1&nbsp;&nbsp;</span>data</a></span></li><li><span><a href=\"#dataloaders\" data-toc-modified-id=\"dataloaders-1.4.2\"><span class=\"toc-item-num\">1.4.2&nbsp;&nbsp;</span>dataloaders</a></span></li><li><span><a href=\"#model\" data-toc-modified-id=\"model-1.4.3\"><span class=\"toc-item-num\">1.4.3&nbsp;&nbsp;</span>model</a></span></li><li><span><a href=\"#Learner\" data-toc-modified-id=\"Learner-1.4.4\"><span class=\"toc-item-num\">1.4.4&nbsp;&nbsp;</span>Learner</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvements pursued in this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Change from binary classifier to multi category classifier:\n",
    "    - add ims 0-9\n",
    "    - add change loss fxn to cross entropy loss w/ softmax\n",
    "    - change shape of final activation from 1 to 10\n",
    "    - change label to 1HE\n",
    "2. Add RGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Super-short version with all of the helpers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass classifier & train_n_sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools for data, model, and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "\n",
    "### Data ###\n",
    "def init_data(path, im_size, n_cls, batch_size):\n",
    "    ## Train\n",
    "    # ims\n",
    "    for i in range(n_cls):\n",
    "        new_ims = torch.stack(\n",
    "            [tensor(Image.open(fn)) for fn in (path/'training'/f'{i}').ls()]\n",
    "        ).float()/255\n",
    "        if i == 0: ims = new_ims\n",
    "        else: ims = torch.cat([ims,new_ims])\n",
    "    train_ims = ims.view(-1,im_size)\n",
    "    # lbls\n",
    "    train_lbls = []\n",
    "    for i in range(n_cls):\n",
    "        l = L([0]*n_cls)\n",
    "        l[i] = 1\n",
    "        train_lbls += [l] * len((path/'training'/f'{i}').ls())    \n",
    "    train_lbls = tensor(train_lbls)\n",
    "    ## Valid\n",
    "    # ims\n",
    "    for i in range(n_cls):\n",
    "        new_ims = torch.stack(\n",
    "            [tensor(Image.open(fn)) for fn in (path/'testing'/f'{i}').ls()]\n",
    "        ).float()/255\n",
    "        if i == 0: ims = new_ims\n",
    "        else: ims = torch.cat([ims,new_ims])\n",
    "    valid_ims = ims.view(-1,im_size)\n",
    "    # lbls\n",
    "    valid_lbls = []\n",
    "    for i in range(n_cls):\n",
    "        l = L([0]*n_cls)\n",
    "        l[i] = 1\n",
    "        valid_lbls += [l] * len((path/'testing'/f'{i}').ls())    \n",
    "    valid_lbls = tensor(valid_lbls)\n",
    "    ## DataLoaders\n",
    "    train_ds = L(zip(train_ims, train_lbls))\n",
    "    valid_ds = L(zip(valid_ims, valid_lbls))\n",
    "    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    valid_dl = DataLoader(valid_ds, batch_size=batch_size, shuffle=True)\n",
    "    return train_dl\n",
    "\n",
    "### Model ###\n",
    "def init_mod(im_size, n_cls, hidden_params):\n",
    "    mod = nn.Sequential(\n",
    "        nn.Linear(im_size,hidden_params),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_params,n_cls)\n",
    "    )\n",
    "    return mod\n",
    "\n",
    "### Loss ###\n",
    "def softmax(t):\n",
    "    if len(t.shape) == 1: return torch.exp(t) / torch.exp(t).sum()\n",
    "    else:                 return torch.exp(t) / torch.exp(t).sum(dim=1, keepdim=True)\n",
    "def loss(yp, y):\n",
    "    return (1 - (y*softmax(yp)).sum(dim=1, keepdim=True)).mean()  # softmax cross entropy loss\n",
    "\n",
    "### Calculate gradients for use in train_once ###\n",
    "def calc_grad(x,y,mod):\n",
    "    yp = mod(x)     # get predictions\n",
    "    ls = loss(yp,y) # calculate loss\n",
    "    ls.backward()   # take gradient w.r.t. loss\n",
    "\n",
    "### Create SGD Stepper; args = (mod.parameters(), lr) ###\n",
    "class ParamStepper:\n",
    "    def __init__(self, p, lr): self.p,self.lr = list(p),lr # initialize w/ mod.params & lr\n",
    "        \n",
    "    def step(self, *args, **kwargs):                       # take step\n",
    "        for o in self.p: o.data -= o.grad.data * self.lr\n",
    "            \n",
    "    def zero_grad(self, *args, **kwargs):                  # reset grad\n",
    "        for o in self.p: o.grad = None\n",
    "\n",
    "### Train parameters by performing SGD on each mini-batch in the dl ###\n",
    "def train_one_epoch(dl, mod, stepper):\n",
    "    for xb,yb in dl:           # for every minibatch (xb,yb) in the dataloader:\n",
    "        calc_grad(xb, yb, mod) # calc grad(loss(mod(xb),yb))\n",
    "        stepper.step()         # take step\n",
    "        stepper.zero_grad()    # reset grad\n",
    "\n",
    "### Get accuracy of mod on a mini-batch ###\n",
    "def mb_acc(yp,y):\n",
    "    yp_max,yp_i = torch.max(yp, dim=1, keepdim=True)\n",
    "    y_max, y_i  = torch.max(y,  dim=1, keepdim=True)\n",
    "    return (yp_i==y_i).float().mean()\n",
    "        \n",
    "### Get accuracy of mod on a dataloader (takes avg of all mbs in dl) ###\n",
    "def epoch_acc(dl, mod):\n",
    "    a = [mb_acc(mod(xb), yb) for xb,yb in dl]\n",
    "    return round(torch.stack(a).mean().item(), 5)          # avg acc over all mini-batches\n",
    "\n",
    "### Run `train_once` `epochs` times given data `dl`, model `mod`, and stepper `stepper`\n",
    "def train_n_epochs(dl, mod, stepper, nepochs):\n",
    "    accs = L()\n",
    "    for i in range(nepochs):\n",
    "        print('.',end='')\n",
    "        train_one_epoch(dl, mod, stepper)\n",
    "        accs += epoch_acc(dl, mod)\n",
    "    print('',end='\\t')\n",
    "    return accs\n",
    "\n",
    "### Perform n training sessions ###\n",
    "def train_n_sessions(dl, im_size, n_cls, hidden_params, nepochs, lr, nsessions):\n",
    "    # accuracies and trained models are returned by train_n_sessions for access after training.\n",
    "    accs = L()\n",
    "    mods = L()\n",
    "    # train model and record accs, mods\n",
    "    print('Progress:',end='\\n')\n",
    "    for i in range(nsessions):\n",
    "        print(i,end='')\n",
    "        mod = init_mod(im_size, n_cls, hidden_params)\n",
    "        stepper = ParamStepper(mod.parameters(), lr)\n",
    "        accs += train_n_epochs(dl, mod, stepper, nepochs)\n",
    "        mods += mod\n",
    "    # restructure mods into nn.Sequential form for end-user convenience\n",
    "    nmods = len(mods)\n",
    "    trained_models = L()\n",
    "    for n in range(nsessions):\n",
    "        layers = L()\n",
    "        for j in range(nmods//nsessions): layers.append(mods.pop())\n",
    "        layers.reverse()\n",
    "        seqmod = nn.Sequential(*layers.items)\n",
    "        trained_models += [seqmod]\n",
    "    print('Done')\n",
    "    return accs, trained_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model once w/ `train_n_epochs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................\t"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(#20) [0.2564,0.40156,0.60688,0.71052,0.72864,0.73829,0.78658,0.8024,0.80999,0.81523...]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# params\n",
    "path          = untar_data(URLs.MNIST)\n",
    "n_cls         = 10\n",
    "im_size       = 28*28\n",
    "batch_size    = 64*2*2*2\n",
    "hidden_params = 30\n",
    "lr            = .1\n",
    "nepochs       = 20\n",
    "\n",
    "# inits\n",
    "dl            = init_data(path, im_size, n_cls, batch_size)\n",
    "mod           = init_mod(im_size, n_cls, hidden_params)\n",
    "stepper       = ParamStepper(mod.parameters(), lr)\n",
    "\n",
    "# train\n",
    "train_n_epochs(dl, mod, stepper, nepochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model many times w/ `train_n_sessions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:\n",
      "0..............................\t1..............................\t2..............................\t3..............................\tDone\n",
      "Max accuracy: 0.918\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.20321</td>\n",
       "      <td>0.36224</td>\n",
       "      <td>0.64941</td>\n",
       "      <td>0.72600</td>\n",
       "      <td>0.78859</td>\n",
       "      <td>0.79937</td>\n",
       "      <td>0.80784</td>\n",
       "      <td>0.87557</td>\n",
       "      <td>0.88299</td>\n",
       "      <td>0.88825</td>\n",
       "      <td>...</td>\n",
       "      <td>0.91078</td>\n",
       "      <td>0.91226</td>\n",
       "      <td>0.91305</td>\n",
       "      <td>0.91460</td>\n",
       "      <td>0.91491</td>\n",
       "      <td>0.91579</td>\n",
       "      <td>0.91647</td>\n",
       "      <td>0.91694</td>\n",
       "      <td>0.91773</td>\n",
       "      <td>0.91803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.28229</td>\n",
       "      <td>0.41783</td>\n",
       "      <td>0.55677</td>\n",
       "      <td>0.56697</td>\n",
       "      <td>0.72080</td>\n",
       "      <td>0.73564</td>\n",
       "      <td>0.74246</td>\n",
       "      <td>0.74530</td>\n",
       "      <td>0.74886</td>\n",
       "      <td>0.75083</td>\n",
       "      <td>...</td>\n",
       "      <td>0.76214</td>\n",
       "      <td>0.76183</td>\n",
       "      <td>0.76275</td>\n",
       "      <td>0.76358</td>\n",
       "      <td>0.76409</td>\n",
       "      <td>0.76411</td>\n",
       "      <td>0.76448</td>\n",
       "      <td>0.76473</td>\n",
       "      <td>0.76548</td>\n",
       "      <td>0.76562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.29635</td>\n",
       "      <td>0.52273</td>\n",
       "      <td>0.59737</td>\n",
       "      <td>0.77249</td>\n",
       "      <td>0.80325</td>\n",
       "      <td>0.81428</td>\n",
       "      <td>0.82119</td>\n",
       "      <td>0.82421</td>\n",
       "      <td>0.82798</td>\n",
       "      <td>0.82920</td>\n",
       "      <td>...</td>\n",
       "      <td>0.84383</td>\n",
       "      <td>0.84389</td>\n",
       "      <td>0.84510</td>\n",
       "      <td>0.84530</td>\n",
       "      <td>0.84617</td>\n",
       "      <td>0.84669</td>\n",
       "      <td>0.84775</td>\n",
       "      <td>0.84849</td>\n",
       "      <td>0.84820</td>\n",
       "      <td>0.87624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.39058</td>\n",
       "      <td>0.42651</td>\n",
       "      <td>0.64370</td>\n",
       "      <td>0.65673</td>\n",
       "      <td>0.66317</td>\n",
       "      <td>0.66713</td>\n",
       "      <td>0.73243</td>\n",
       "      <td>0.74230</td>\n",
       "      <td>0.74811</td>\n",
       "      <td>0.75052</td>\n",
       "      <td>...</td>\n",
       "      <td>0.76288</td>\n",
       "      <td>0.76335</td>\n",
       "      <td>0.76415</td>\n",
       "      <td>0.76439</td>\n",
       "      <td>0.76494</td>\n",
       "      <td>0.76504</td>\n",
       "      <td>0.76519</td>\n",
       "      <td>0.76547</td>\n",
       "      <td>0.76642</td>\n",
       "      <td>0.77723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0        1        2        3        4        5        6        7   \\\n",
       "0  0.20321  0.36224  0.64941  0.72600  0.78859  0.79937  0.80784  0.87557   \n",
       "1  0.28229  0.41783  0.55677  0.56697  0.72080  0.73564  0.74246  0.74530   \n",
       "2  0.29635  0.52273  0.59737  0.77249  0.80325  0.81428  0.82119  0.82421   \n",
       "3  0.39058  0.42651  0.64370  0.65673  0.66317  0.66713  0.73243  0.74230   \n",
       "\n",
       "        8        9   ...       20       21       22       23       24  \\\n",
       "0  0.88299  0.88825  ...  0.91078  0.91226  0.91305  0.91460  0.91491   \n",
       "1  0.74886  0.75083  ...  0.76214  0.76183  0.76275  0.76358  0.76409   \n",
       "2  0.82798  0.82920  ...  0.84383  0.84389  0.84510  0.84530  0.84617   \n",
       "3  0.74811  0.75052  ...  0.76288  0.76335  0.76415  0.76439  0.76494   \n",
       "\n",
       "        25       26       27       28       29  \n",
       "0  0.91579  0.91647  0.91694  0.91773  0.91803  \n",
       "1  0.76411  0.76448  0.76473  0.76548  0.76562  \n",
       "2  0.84669  0.84775  0.84849  0.84820  0.87624  \n",
       "3  0.76504  0.76519  0.76547  0.76642  0.77723  \n",
       "\n",
       "[4 rows x 30 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# params\n",
    "path          = untar_data(URLs.MNIST)\n",
    "n_cls         = 10\n",
    "im_size       = 28*28\n",
    "batch_size    = 64*2*2*2\n",
    "hidden_params = 30\n",
    "nepochs       = 30\n",
    "lr            = .1\n",
    "nsessions     = 4\n",
    "\n",
    "# inits\n",
    "# dl = init_data(path, im_size, n_cls, batch_size)\n",
    "\n",
    "# train\n",
    "accs,mods = train_n_sessions(dl, im_size, n_cls, hidden_params, nepochs, lr, nsessions)\n",
    "accs_t = tensor(accs).reshape(nsessions,nepochs)\n",
    "\n",
    "# print max acc\n",
    "print(\"Max accuracy:\",round(accs_t.max().item(),4))\n",
    "\n",
    "# print training sessions\n",
    "pd.DataFrame(accs_t.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diving into results of a single session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xb,yb = list(dl)[0]\n",
    "show_image(xb[0].view(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypb = mods[0](xb)\n",
    "softmax(ypb[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It thought the five was an 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "\n",
    "### Data ###\n",
    "def init_data(path, im_size, n_cls, batch_size):\n",
    "    ## Train\n",
    "    # ims\n",
    "    for i in range(n_cls):\n",
    "        new_ims = torch.stack(\n",
    "            [tensor(Image.open(fn)) for fn in (path/'training'/f'{i}').ls()]\n",
    "        ).float()/255\n",
    "        if i == 0: ims = new_ims # do this on first iter\n",
    "        else:      ims = torch.cat([ims,new_ims]) # do this on all others\n",
    "    train_ims = ims.view(-1,im_size)\n",
    "    # lbls\n",
    "    train_lbls = []\n",
    "    for i in range(n_cls): train_lbls += [i] * len((path/'training'/f'{i}').ls())    \n",
    "    train_lbls = tensor(train_lbls)\n",
    "    ## Valid\n",
    "    # ims\n",
    "    for i in range(n_cls):\n",
    "        new_ims = torch.stack(\n",
    "            [tensor(Image.open(fn)) for fn in (path/'testing'/f'{i}').ls()]\n",
    "        ).float()/255\n",
    "        if i == 0: ims = new_ims\n",
    "        else:      ims = torch.cat([ims,new_ims])\n",
    "    valid_ims = ims.view(-1,im_size)\n",
    "    # lbls\n",
    "    valid_lbls = []\n",
    "    for i in range(n_cls): valid_lbls += [i] * len((path/'testing'/f'{i}').ls())    \n",
    "    valid_lbls = tensor(valid_lbls)\n",
    "    ## DataLoaders\n",
    "    train_ds = L(zip(train_ims, train_lbls))\n",
    "    valid_ds = L(zip(valid_ims, valid_lbls))\n",
    "    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    valid_dl = DataLoader(valid_ds, batch_size=batch_size, shuffle=True)\n",
    "    return train_dl\n",
    "\n",
    "### Model ###\n",
    "def init_mod(im_size, n_cls, hidden_params):\n",
    "    mod = nn.Sequential(\n",
    "        nn.Linear(im_size,hidden_params),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_params,n_cls)\n",
    "    )\n",
    "    return mod\n",
    "\n",
    "### Loss ###\n",
    "loss = F.cross_entropy\n",
    "# def softmax(t):\n",
    "#     if len(t.shape) == 1: return torch.exp(t) / torch.exp(t).sum()\n",
    "#     else:                 return torch.exp(t) / torch.exp(t).sum(dim=1, keepdim=True)\n",
    "# def loss(yp, y):\n",
    "#     return (1 - (y*softmax(yp)).sum(dim=1, keepdim=True)).mean()\n",
    "\n",
    "### Calculate gradients for use in train_once ###\n",
    "def calc_grad(x,y,mod):\n",
    "    yp = mod(x)     # get predictions\n",
    "    ls = loss(yp,y) # calculate loss\n",
    "    ls.backward()   # take gradient w.r.t. loss\n",
    "\n",
    "### Create SGD Stepper; args = (mod.parameters(), lr) ###\n",
    "class ParamStepper:\n",
    "    def __init__(self, p, lr): self.p,self.lr = list(p),lr # initialize w/ mod.params & lr\n",
    "        \n",
    "    def step(self, *args, **kwargs):                       # take step\n",
    "        for o in self.p: o.data -= o.grad.data * self.lr\n",
    "            \n",
    "    def zero_grad(self, *args, **kwargs):                  # reset grad\n",
    "        for o in self.p: o.grad = None\n",
    "\n",
    "### Train parameters by performing SGD on each mini-batch in the dl ###\n",
    "def train_one_epoch(dl, mod, stepper):\n",
    "    for xb,yb in dl:           # for every minibatch (xb,yb) in the dataloader:\n",
    "        calc_grad(xb, yb, mod) # calc grad(loss(mod(xb),yb))\n",
    "        stepper.step()         # take step\n",
    "        stepper.zero_grad()    # reset grad\n",
    "\n",
    "### Get accuracy of mod on a mini-batch ###\n",
    "def mb_acc(yp,y):\n",
    "    yp_max,yp_i = torch.max(yp, dim=1, keepdim=True)\n",
    "    y_max, y_i  = torch.max(y,  dim=1, keepdim=True)\n",
    "    return (yp_i==y_i).float().mean()\n",
    "        \n",
    "### Get accuracy of mod on a dataloader (takes avg of all mbs in dl) ###\n",
    "def epoch_acc(dl, mod):\n",
    "    a = [mb_acc(mod(xb), yb) for xb,yb in dl]\n",
    "    return round(torch.stack(a).mean().item(), 5)          # avg acc over all mini-batches\n",
    "\n",
    "### Run `train_once` `epochs` times given data `dl`, model `mod`, and stepper `stepper`\n",
    "def train_n_epochs(dl, mod, stepper, nepochs):\n",
    "    accs = L()\n",
    "    for i in range(nepochs):\n",
    "        print('.',end='')\n",
    "        train_one_epoch(dl, mod, stepper)\n",
    "        accs += epoch_acc(dl, mod)\n",
    "    print('',end='\\t')\n",
    "    return accs\n",
    "\n",
    "### Perform n training sessions ###\n",
    "def train_n_sessions(dl, im_size, n_cls, hidden_params, nepochs, lr, nsessions):\n",
    "    accs = L()\n",
    "    mods = L()\n",
    "    print('Progress:',end='\\n')\n",
    "    for i in range(nsessions):\n",
    "        print(i,end='')\n",
    "        mod = init_mod(im_size, n_cls, hidden_params)\n",
    "        stepper = ParamStepper(mod.parameters(), lr)\n",
    "        accs += train_n_epochs(dl, mod, stepper, nepochs)\n",
    "        mods += mod\n",
    "        \n",
    "    # repack mods into nn.Sequential form\n",
    "    nmods = len(mods)\n",
    "    trained_models = L()\n",
    "    for n in range(nsessions):\n",
    "        layers = L()\n",
    "        for j in range(nmods//nsessions): layers.append(mods.pop())\n",
    "        layers.reverse()\n",
    "        seqmod = nn.Sequential(*layers.items)\n",
    "        trained_models += [seqmod]\n",
    "        \n",
    "    print('Done')\n",
    "    return accs, trained_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "path          = untar_data(URLs.MNIST)\n",
    "n_cls         = 10\n",
    "im_size       = 28*28\n",
    "batch_size    = 64*2*2*2\n",
    "hidden_params = 30\n",
    "lr            = .1\n",
    "nepochs       = 20\n",
    "\n",
    "# inits\n",
    "dl            = init_data(path, im_size, n_cls, batch_size)\n",
    "mod           = init_mod(im_size, n_cls, hidden_params)\n",
    "stepper       = ParamStepper(mod.parameters(), lr)\n",
    "\n",
    "# train\n",
    "train_n_epochs(dl, mod, stepper, nepochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,yb = list(dl)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod(xb).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = -softmax(mod(xb))[range(batch_size),yb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.cross_entropy(preds,yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod(xb).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
